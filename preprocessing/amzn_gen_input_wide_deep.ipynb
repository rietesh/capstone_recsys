{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44944d2f",
   "metadata": {},
   "source": [
    "# Prepare for wide_deep\n",
    "\n",
    "Prepare Electronics, Home and Games datasets for wide & deep model.\n",
    "\n",
    "#### Input\n",
    "- all_reviews_20.csv, all_meta_20.csv\n",
    "\n",
    "#### Output (includes positive & negative samples for NDCG, Hit Rate calculation)\n",
    "- users_e_20.pkl, items_e_20.pkl\n",
    "- wide_deep_amzn_e_20.csv, wide_deep_amzn_e_20_train.csv, wide_deep_amzn_e_20_test.csv\n",
    "- amzn_e_tst_w_neg[0-5].txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e8bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../recommenders') # if needed, adjust the path to Microsoft Recommenders clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a9ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/'\n",
    "DATA_DIR_WIDE_DEEP = '/home/shiv/Documents/DataScience/Capstone/Data/wide_deep/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3539b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from recommenders.datasets.python_splitters import python_chrono_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6396a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_REVIEWS = 20\n",
    "NUM_NEG_SAMPLES = 50\n",
    "\n",
    "# read the ratings dataframe\n",
    "ratings_df = pd.read_csv(DATA_DIR + f'all_reviews_{MIN_REVIEWS}.csv', header=None)\n",
    "ratings_df.columns = ['reviewerID', 'asin', 'rating', 'unixTimeStamp']     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7628de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the items dataframe\n",
    "items_df = pd.read_csv(DATA_DIR + f'all_meta_{MIN_REVIEWS}.csv', header=None)\n",
    "# items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12391d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.columns=['asin','price','title','main_cat','category']\n",
    "items_df['category'].fillna('', inplace=True)\n",
    "items_df['price'].fillna('$$$', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01aa035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample to show how to drop a main category\n",
    "\n",
    "# mag_subs = items_df[items_df['main_cat']=='Magazine Subscriptions']['asin'].values\n",
    "# ratings_df=ratings_df[~ratings_df['asin'].isin(mag_subs)]\n",
    "# items_df=items_df[~items_df['asin'].isin(mag_subs)]\n",
    "# ratings_df.reset_index(inplace=True,drop=True)\n",
    "# items_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0cf3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(items_df['main_cat'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f03340",
   "metadata": {},
   "source": [
    "<a id=\"electronics\"></a>\n",
    "# Prepare Electronics, Home, Games datasets\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Filter the items dataframe to remove all items whose main_cat is not in the list of main_categories for each umbrella category (e.g. Electronics)\n",
    "2. Filter the rating dataframe to remove all reviews that do not belong to the list of item IDs left in the items dataframe after step 1.\n",
    "3. Convert hexadecimal reviewerID in the ratings dataframe to userID (0 based)\n",
    "4. Save the reviewerID dict in a pkl file\n",
    "5. Left merge ratings and items filtered dataframe on asin (hexadecimal)\n",
    "6. Convert hexadecimal asin to itemID (0 based)\n",
    "7. Save the itemID dict (for UI) in a pkl file\n",
    "8. Add genre; currently, consider the main categories or if an item in category matches the sub category list\n",
    "9. Save the prepared dataset for use by modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "857e83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, dataset):\n",
    "    data_dir = DATA_DIR_WIDE_DEEP + dataset + '/'\n",
    "    \n",
    "    # Step 1: filter the items dataframe\n",
    "    items_filtered_df = items_df[items_df['main_cat'].isin(lst_main_cat)].copy()\n",
    "    items_filtered_ids = items_filtered_df['asin'].values\n",
    "\n",
    "    # Step 2: filter the ratings dataframe\n",
    "    ratings_filtered_df = ratings_df[ratings_df['asin'].isin(items_filtered_ids)].copy()\n",
    "    \n",
    "    print(dataset)\n",
    "    print(\"Num ratings:\", ratings_filtered_df.shape[0])\n",
    "    print(\"users\", ratings_filtered_df['reviewerID'].nunique(), \"items\", ratings_filtered_df['asin'].nunique())\n",
    "    \n",
    "    # Step 3: convert reviewerID to userID (0 based)\n",
    "    reviewers_dict = {}\n",
    "    reviewer_id = 0\n",
    "    for _, row in ratings_filtered_df.iterrows():\n",
    "        if row['reviewerID'] not in reviewers_dict:\n",
    "            reviewers_dict[row['reviewerID']] = reviewer_id\n",
    "            reviewer_id += 1\n",
    "            \n",
    "    ratings_filtered_df['userID'] = ratings_filtered_df['reviewerID'].apply(lambda x: reviewers_dict[x])\n",
    "\n",
    "    # Step 4: save the reviewerID dict (perhaps for UI)\n",
    "    users_pkl_path = data_dir + f'users_{dataset[0].lower()}_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "    with open(users_pkl_path, 'wb') as handle:\n",
    "        pickle.dump(reviewers_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    ratings_filtered_df.drop(columns=['reviewerID'], inplace=True) # we will henceforth use userID\n",
    "\n",
    "    # Step 5: left merge the ratings with items on asin\n",
    "    data = ratings_filtered_df.merge(items_filtered_df, on=['asin'], how='left')\n",
    "    print(\"merged dataframe\", data.shape)\n",
    "    data['category'] = data['category'].astype('string')\n",
    "    \n",
    "    # Step 6: convert asin to itemID (0 based)\n",
    "    items_dict = {}\n",
    "    item_id = 0\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        if row['asin'] not in items_dict:\n",
    "            items_dict[row['asin']] = item_id\n",
    "            item_id += 1\n",
    "    data['itemID'] = data['asin'].apply(lambda x: items_dict[x])\n",
    "    \n",
    "    # Step 7: save the itemID dict (for UI)\n",
    "    items_pkl_path = data_dir + f'items_{dataset[0].lower()}_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "    with open(items_pkl_path, 'wb') as handle:\n",
    "        pickle.dump(items_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)        \n",
    "    data.drop(columns=['asin'], inplace=True) # we will henceforth use itemID\n",
    "    \n",
    "    # Step 8: add genre; \n",
    "    # currently, consider the main categories or if an item in category matches the sub category list\n",
    "    data['genre'] = ''\n",
    "\n",
    "    def update_category(row):\n",
    "        categories = row['category']\n",
    "        new_cat = []\n",
    "        new_cat.append(row['main_cat'])\n",
    "        # print(categories)\n",
    "        for cat in categories.split('|'):\n",
    "            if cat.strip() != '' and ((cat in lst_main_cat) or (cat in lst_sub_cat)):\n",
    "                new_cat.append(cat)\n",
    "        row['genre'] = '|'.join(new_cat)\n",
    "        return row\n",
    "    data = data.apply(update_category, axis=1)\n",
    "    \n",
    "    # Step 9: save the prepared dataset to be used by wide_deep model\n",
    "    # data.drop(columns=['category'], inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    data = data[['userID','itemID', 'rating','genre','unixTimeStamp','title','price','main_cat','category']]\n",
    "    data.to_csv(data_dir + f'wide_deep_amzn_{dataset[0].lower()}_{MIN_REVIEWS}.csv', \n",
    "                 header=False, index=False)\n",
    "    \n",
    "    del items_filtered_df   # Save on RAM memory!\n",
    "    del ratings_filtered_df\n",
    "    del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e2f0e",
   "metadata": {},
   "source": [
    "# Code to discover relevant sub categories to build the lst_sub_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98301b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ratings_g_df = pd.read_csv(DATA_DIR + 'wide_deep_amzn_g_20.csv', header=None, low_memory=False)\n",
    "ratings_g_df.columns=['userID','itemID', 'rating','genre','unixTimeStamp','title','price','main_cat','category']\n",
    "ratings_g_df['category'].fillna('', inplace=True)\n",
    "ratings_g_df['price'].fillna('$$$', inplace=True)\n",
    "    \n",
    "lst = list(ratings_g_df['category'].unique())\n",
    "word_count = defaultdict(int)\n",
    "for l in lst:\n",
    "    for w in l.split('|'):\n",
    "        word_count[w] += 1\n",
    "dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4597070",
   "metadata": {},
   "source": [
    "### Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cefa68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n",
      "Num ratings: 5613183\n",
      "users 830668 items 63725\n",
      "merged dataframe (5613183, 8)\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Electronics'\n",
    "lst_main_cat = ['All Electronics', 'Amazon Devices', 'Apple Products', \n",
    "                 'Camera & Photo', 'Car Electronics', 'Cell Phones & Accessories', 'Computers',\n",
    "                 'Electronics', 'GPS & Navigation', 'Home Audio & Theater', 'Industrial & Scientific',\n",
    "                 'Portable Audio & Accessories']\n",
    "\n",
    "lst_sub_cat = ['Accessories','Computers & Accessories','Office Products','Video Games',\n",
    "               'Accessories & Supplies','Tools & Home Improvement','Computer Accessories & Peripherals',\n",
    "               'Audio & Video Accessories', 'Automotive', 'Office & School Supplies',\n",
    "               'Car & Vehicle Electronics', 'Industrial & Scientific','Sports & Outdoors','Office Electronics',\n",
    "               'Home & Kitchen','Musical Instruments','Portable Audio & Video','Electrical',\n",
    "               'Clothing, Shoes & Jewelry','Toys & Games','Laptop Accessories','Home Audio',\n",
    "               'Controllers','Computer Components','Sports & Fitness']\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, \"Electronics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f8b96",
   "metadata": {},
   "source": [
    "### Train, test split using python_chrono_split\n",
    "\n",
    "**Run it once! It takes a very long time!!**\n",
    "If you have already saved the two csv files, go to the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009330e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv(DATA_DIR_WIDE_DEEP + 'wide_deep_amzn_e_20.csv', header=None, low_memory=False)\n",
    "\n",
    "train, test = python_chrono_split(ratings_df, ratio=0.9, col_timestamp='unixTimeStamp')\n",
    "\n",
    "print(\"{} train samples and {} test samples\".format(len(train), len(test)))\n",
    "\n",
    "train.sort_values('unixTimeStamp', inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "train.to_csv(DATA_DIR_WIDE_DEEP + 'wide_deep_amzn_e_20_train.csv', header=None, index=False)\n",
    "test.to_csv(DATA_DIR_WIDE_DEEP + 'wide_deep_amzn_e_20_test.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR_WIDE_DEEP + 'wide_deep_amzn_e_20_train.csv', header=None,\n",
    "                    converters={3: lambda x: x.strip(\"[]\").split(\", \")})\n",
    "train.columns = ['userID','itemID','rating','genre','unixTimeStamp']\n",
    "\n",
    "test = pd.read_csv(DATA_DIR_WIDE_DEEP + 'wide_deep_amzn_e_20_test.csv', header=None,\n",
    "                   converters={3: lambda x: x.strip(\"[]\").split(\", \")})\n",
    "test.columns = ['userID','itemID','rating','genre','unixTimeStamp']\n",
    "\n",
    "print(\"{} train samples and {} test samples\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bcf357",
   "metadata": {},
   "source": [
    "### Prepare to get ndcg@10, hit@10 for wide_n_deep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['unixTimeStamp'], inplace=True)\n",
    "test.drop(columns=['unixTimeStamp'], inplace=True)\n",
    "\n",
    "users_grp = train[[USER_COL, ITEM_COL]].groupby([USER_COL]).agg(list)\n",
    "\n",
    "items_df = train.drop_duplicates([ITEM_COL]).copy()\n",
    "items_df.drop(columns=[USER_COL, RATING_COL], inplace=True)\n",
    "items_df.set_index(ITEM_COL, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f524353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from multiprocessing import Process, Queue\n",
    "import random\n",
    "\n",
    "items_set = set(train[ITEM_COL].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2141c1d",
   "metadata": {},
   "source": [
    "### Process for generating the test data positive and negative samples\n",
    "\n",
    "Note that the entire process takes time even when using multiprocessing module.\n",
    "\n",
    "- Each worker takes a part of the test dataframe, so if there are 5 workers, each get 1/5th of the dataframe; worker # 5 gets to work a bit more to handle the remaining rows in the end\n",
    "- Once a review is selected, that becomes the positive sample. NUM_NEG_SAMPLES are then found for this user.\n",
    "- Each of the negative sample is unique and not seen by the user that wrote the review. Both positive and negative samples are written using the required csv format (userID, itemID, rating, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc579f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORKERS = 5\n",
    "\n",
    "def sample_function(test, users_grp, items_df, i, num_workers, seed, data_dir):\n",
    "    nrows = test.shape[0]\n",
    "    each = nrows // num_workers\n",
    "    start = i*each\n",
    "    end = nrows if i == num_workers - 1 else (i+1)*each\n",
    "    sym = ['!','@','#','$','%']\n",
    "    \n",
    "    # print(i, start, end, test.iloc[start:end].shape)\n",
    "    random.seed(seed)\n",
    "    tst_w_neg_samples_path = data_dir + f'amzn_e_tst_w_neg{i}.txt'\n",
    "    \n",
    "    user_col = []\n",
    "    item_col = []\n",
    "    rating_col = []\n",
    "    feat_col = []\n",
    "    for j, row in test.iloc[start:end].iterrows():\n",
    "        u = row[USER_COL]\n",
    "        positive_item = row[ITEM_COL]\n",
    "        tmp_df = users_grp.loc[u]\n",
    "        assert(tmp_df.shape[0] != 0)\n",
    "        \n",
    "        items_seen_set = set(tmp_df[ITEM_COL])\n",
    "        items_not_seen_set = list(items_set - items_seen_set)\n",
    "        user_col.append(u)\n",
    "        item_col.append(positive_item)\n",
    "        rating_col.append(row[RATING_COL])\n",
    "        feat_col.append([int(f) for f in row[ITEM_FEAT_COL]])\n",
    "        \n",
    "        cnt = 0\n",
    "        neg_items = set()\n",
    "        while cnt < NUM_NEG_SAMPLES:\n",
    "            neg_item = random.choice(list(items_not_seen_set))\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "                \n",
    "            cnt += 1\n",
    "            tmp_df = items_df.loc[neg_item]\n",
    "            assert(tmp_df.shape[0] != 0)\n",
    "            \n",
    "            user_col.append(u)\n",
    "            item_col.append(neg_item)\n",
    "            rating_col.append(5.0) # unused\n",
    "            feat_col.append([int(f) for f in tmp_df[ITEM_FEAT_COL]])   \n",
    "            \n",
    "        if j % 10_000 == 0:\n",
    "            print(sym[i], end='')\n",
    "\n",
    "    X_test = pd.DataFrame({USER_COL: user_col, ITEM_COL: item_col, \n",
    "                           RATING_COL: rating_col, ITEM_FEAT_COL: feat_col})\n",
    "    X_test.to_csv(tst_w_neg_samples_path, header=False, index=False)    \n",
    "\n",
    "processors = []\n",
    "for i in range(N_WORKERS):\n",
    "    processors.append(\n",
    "        Process(\n",
    "            target = sample_function,\n",
    "            args = (test, users_grp, items_df, i, N_WORKERS, RANDOM_SEED, DATA_DIR_WIDE_DEEP)\n",
    "        ))\n",
    "    # processors[-1].daemon = True\n",
    "    processors[-1].start()\n",
    "\n",
    "for i in range(N_WORKERS):\n",
    "    processors[i].join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c15c47",
   "metadata": {},
   "source": [
    "### Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca0f2129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home\n",
      "Num ratings: 7575829\n",
      "users 882963 items 92562\n",
      "merged dataframe (7575829, 8)\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Home'\n",
    "lst_main_cat = ['Amazon Home', 'Appliances', 'Home & Kitchen', \n",
    "                'Patio, Lawn & Garden', 'Tools & Home Improvement']\n",
    "\n",
    "lst_sub_cat = ['Kitchen & Dining', 'Industrial & Scientific', 'Power & Hand Tools',\n",
    "               'Automotive', 'Arts, Crafts & Sewing', 'Office Products',\n",
    "               'Electronics', 'Sports & Outdoors', 'Home Dcor', 'Accessories', 'Hand Tools',\n",
    "               'Office & School Supplies', 'Gardening & Lawn Care',\n",
    "               'Hardware', 'Storage & Organization', 'Lighting & Ceiling Fans',\n",
    "               'Kitchen Utensils & Gadgets', 'Electrical', 'Furniture', 'Pet Supplies',\n",
    "               'Building Supplies', 'Bedding', 'Sports & Fitness', 'Safety & Security',\n",
    "               'Outdoor Recreation', 'Power Tool Parts & Accessories', 'Kitchen & Bath Fixtures',\n",
    "               'Parts & Accessories', 'Small Appliances', 'Replacement Parts',\n",
    "               'Crafting', 'Sewing', 'Tools & Equipment', 'Outdoor Dcor', 'Patio Furniture & Accessories',\n",
    "               'Grills & Outdoor Cooking', 'Power Tools', 'Rough Plumbing', 'Bath',\n",
    "               'Bakeware', 'Accessories & Supplies', 'Heating, Cooling & Air Quality',\n",
    "               'Outdoor Power Tools', 'Outdoor Lighting', 'Paint, Wall Treatments & Supplies',\n",
    "               'Home Dcor Accents', 'Pools, Hot Tubs & Supplies', 'Bathroom Fixtures',\n",
    "               'Test, Measure & Inspect', 'Bathroom Accessories', 'Personal Protective Equipment',\n",
    "               'Fasteners', 'Vacuums & Floor Care', 'Clothing & Closet Storage', 'Exterior Accessories',\n",
    "               'Replacement Parts & Accessories', 'Desk Accessories & Workspace Organizers',\n",
    "               'Outdoor Cooking Tools & Accessories', 'HVAC']\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, \"Home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afde184",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f8e2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games\n",
      "Num ratings: 3735507\n",
      "users 748773 items 56219\n",
      "merged dataframe (3735507, 8)\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Games'\n",
    "lst_main_cat = ['Sports & Outdoors', 'Toys & Games', 'Video Games']\n",
    "lst_sub_cat = ['Sports & Fitness', 'Accessories', 'Clothing, Shoes & Jewelry', 'Home & Kitchen',\n",
    "               'Outdoor Recreation', 'Clothing', 'Electronics', 'Games', \n",
    "               'Sports & Outdoor Play', 'Cycling', 'Exercise & Fitness', 'Camping & Hiking',\n",
    "               'Leisure Sports & Game Room', 'Hunting & Fishing', 'Retro Gaming & Microconsoles',\n",
    "               'Costumes & Accessories', 'Dress Up & Pretend Play', 'Shoes', 'Golf', 'Hobbies',\n",
    "               'Water Sports', 'Controllers', 'Xbox One', 'PlayStation 3', 'Motorcycle & Powersports',\n",
    "               'Arts & Crafts', 'Replacement Parts', 'Xbox 360', 'Other Sports', 'Crafting',\n",
    "               'Wii', 'Learning & Education', 'Sports', 'Active', 'Consoles',\n",
    "               'Painting, Drawing & Art Supplies', 'Audio & Video Accessories', 'Nintendo 3DS & 2DS',\n",
    "               'Athletic', 'Skates, Skateboards & Scooters', 'Building Toys', 'Building Sets']\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, \"Games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579af3e1",
   "metadata": {},
   "source": [
    "# Additional datasets: Books, Health & Personal Care, Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d70047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_cat = 'Books'\n",
    "lst_main_cat = ['Books', 'Kindle', 'Audible audiobooks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e726516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_cat = 'Health & Personal Care'\n",
    "lst_main_cat = ['All Beauty', 'Health & Personal Care', 'Luxury Beauty']\n",
    "\n",
    "# main_cat = 'Fashion'\n",
    "lst_main_cat = ['Amazon Fashion', 'Clothing, Shoes & Jewelry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fb7b4",
   "metadata": {},
   "source": [
    "<a id=\"all_main_cat\"></a>\n",
    "## Use all the main categories; if only specific main categories are needed, jump to [Electronics](#electronics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a11847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting reviewerID to userID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 38007219/38007219 [07:52<00:00, 80368.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged dataframe (38007219, 8)\n",
      "Converting asin to itemID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 484062/484062 [00:07<00:00, 62080.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: convert reviewerID to userID (0 based)\n",
    "print(\"Converting reviewerID to userID\")\n",
    "reviewers_dict = {}\n",
    "reviewer_id = 0\n",
    "with tqdm(total=ratings_df.shape[0]) as pbar:\n",
    "    for _, row in ratings_df.iterrows():\n",
    "        if row['reviewerID'] not in reviewers_dict:\n",
    "            reviewers_dict[row['reviewerID']] = reviewer_id\n",
    "            reviewer_id += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "ratings_df['userID'] = ratings_df['reviewerID'].apply(lambda x: reviewers_dict[x])\n",
    "\n",
    "# Step 4: save the reviewerID dict (perhaps for UI)\n",
    "users_pkl_path = DATA_DIR + f'users_all_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "with open(users_pkl_path, 'wb') as handle:\n",
    "    pickle.dump(reviewers_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "del reviewers_dict\n",
    "ratings_df.drop(columns=['reviewerID'], inplace=True) # we will henceforth use userID\n",
    "\n",
    "# Step 5: left merge the ratings with items on asin\n",
    "data = ratings_df.merge(items_df, on=['asin'], how='left')\n",
    "print(\"merged dataframe\", data.shape)\n",
    "data['category'] = data['category'].astype('string')\n",
    "\n",
    "del ratings_df\n",
    "\n",
    "# Step 6: convert asin to itemID (0 based)\n",
    "print(\"Converting asin to itemID\")\n",
    "items_dict = {}\n",
    "item_id = 0\n",
    "\n",
    "with tqdm(total=items_df.shape[0]) as pbar:\n",
    "    for _, row in items_df.iterrows():\n",
    "        if row['asin'] not in items_dict:\n",
    "            items_dict[row['asin']] = item_id\n",
    "            item_id += 1\n",
    "        pbar.update(1)\n",
    "data['itemID'] = data['asin'].apply(lambda x: items_dict[x])\n",
    "\n",
    "# Step 7: save the itemID dict (for UI)\n",
    "items_pkl_path = DATA_DIR + f'items_all_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "with open(items_pkl_path, 'wb') as handle:\n",
    "    pickle.dump(items_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "del items_dict\n",
    "data.drop(columns=['asin'], inplace=True) # we will henceforth use itemID\n",
    "\n",
    "tqdm.pandas()\n",
    "lst_main_cat = items_df.main_cat.unique()\n",
    "\n",
    "# Step 8: add genre; \n",
    "# currently, consider the main categories or if an item in category matches the main category list\n",
    "data['genre'] = ''\n",
    "\n",
    "def update_category(row):\n",
    "    categories = row['category']\n",
    "    new_cat = []\n",
    "    new_cat.append(row['main_cat'])\n",
    "    # print(categories)\n",
    "    for cat in categories.split('|'):\n",
    "        if cat.strip() != '' and (cat in lst_main_cat):\n",
    "            new_cat.append(cat)\n",
    "    row['genre'] = '|'.join(new_cat)\n",
    "    return row\n",
    "data = data.progress_apply(update_category, axis=1)\n",
    "\n",
    "# Step 9: save the prepared dataset to be used by wide_deep model\n",
    "data.drop(columns=['category'], inplace=True)\n",
    "data.sort_values('unixTimeStamp', inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data = data[['userID','itemID', 'rating','genre','unixTimeStamp','title','price','main_cat','category']]\n",
    "data.to_csv(DATA_DIR + f'wide_deep_amzn_all_{MIN_REVIEWS}.csv', \n",
    "             header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ac402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
