{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4214a04a",
   "metadata": {},
   "source": [
    "# Prepare for SLiRec, SASRec\n",
    "\n",
    "#### Inputs\n",
    "- all_reviews_20.csv, all_meta_20.csv\n",
    "\n",
    "#### Outputs\n",
    "- train_data, valid_data, test_data\n",
    "- valid_data, test_data includes negative samples (4 in valid_data, 49 in test_data)\n",
    "- user_vocab.pkl, item_vocab.pkl, category_vocab.pkl\n",
    "- reviews.out (also used in the Jupyter notebook for SASRec model which does NOT have any other preprocessing requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fbc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/'\n",
    "DATA_DIR_SLIREC = '/home/shiv/Documents/DataScience/Capstone/Data/slirec/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0ad9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e355aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_REVIEWS = 20   # constants\n",
    "TEST_NUM_NGS = 49\n",
    "VALID_NUM_NGS = 4\n",
    "\n",
    "ratings_df = pd.read_csv(DATA_DIR + f'all_reviews_{MIN_REVIEWS}.csv', header=None)\n",
    "ratings_df.columns = ['reviewerID', 'asin', 'rating', 'unixTimeStamp']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f848697",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.read_csv(DATA_DIR + f'all_meta_{MIN_REVIEWS}.csv', header=None)\n",
    "items_df.columns=['asin','price','title','main_cat','category']\n",
    "items_df['category'].fillna('', inplace=True)\n",
    "items_df['price'].fillna('$$$', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d2018c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(items_df['main_cat'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b673df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def write_reviews(data_dir, ratings_filtered_df):\n",
    "    reviews_writefile = data_dir + '/reviews.output'\n",
    "    reviews_w = open(reviews_writefile, 'w')\n",
    "    for _, row in ratings_filtered_df.iterrows():\n",
    "        reviews_w.write(\n",
    "            row[\"reviewerID\"]\n",
    "            + \"\\t\"\n",
    "            + row[\"asin\"]\n",
    "            + \"\\t\"\n",
    "            + str(row[\"unixTimeStamp\"])\n",
    "            + \"\\n\"\n",
    "        )\n",
    "    reviews_w.close()\n",
    "    return reviews_writefile\n",
    "\n",
    "def write_meta(data_dir, items_df):\n",
    "    meta_writefile = data_dir + \"/meta.output\"\n",
    "    meta_w = open(meta_writefile, \"w\")\n",
    "\n",
    "    for _, row in items_df.iterrows():\n",
    "        meta_w.write(row[\"asin\"] + \"\\t\" + row[\"main_cat\"] + \"\\n\")\n",
    "    meta_w.close()\n",
    "    return meta_writefile\n",
    "\n",
    "def write_instance_output(reviews_file, meta_file):\n",
    "    # For every user, create a list of items reviews sorted by ascending timestamp\n",
    "    print(\"start create instances...\")\n",
    "    dirs, _ = os.path.split(reviews_file)\n",
    "    output_file = os.path.join(dirs, \"instance.output\")\n",
    "\n",
    "    f_reviews = open(reviews_file, \"r\")\n",
    "    user_dict = {}\n",
    "    item_list = []\n",
    "    \n",
    "    for line in f_reviews:\n",
    "        line = line.strip()\n",
    "        reviews_things = line.split(\"\\t\") # user_id asin unix_ts\n",
    "        if reviews_things[0] not in user_dict:\n",
    "            user_dict[reviews_things[0]] = []\n",
    "        user_dict[reviews_things[0]].append((line, float(reviews_things[-1]))) # note append whole line, ts\n",
    "        item_list.append(reviews_things[1])\n",
    "\n",
    "    f_meta = open(meta_file, \"r\")\n",
    "    meta_dict = {}\n",
    "    for line in f_meta:\n",
    "        line = line.strip()\n",
    "        meta_things = line.split(\"\\t\") # asin category\n",
    "        if meta_things[0] not in meta_dict:\n",
    "            meta_dict[meta_things[0]] = meta_things[1]\n",
    "\n",
    "    f_output = open(output_file, \"w\")\n",
    "    num_default_cat = 0\n",
    "    for user_id in user_dict:\n",
    "        sorted_user_behavior = sorted(user_dict[user_id], key=lambda x: x[1]) # x[1]: tuple (line, ts)\n",
    "        for line, _ in sorted_user_behavior:\n",
    "            user_things = line.split(\"\\t\") # user_id, asin, ts\n",
    "            asin = user_things[1]\n",
    "            if asin in meta_dict:\n",
    "                f_output.write(\"1\" + \"\\t\" + line + \"\\t\" + meta_dict[asin] + \"\\n\") # positive\n",
    "            else:\n",
    "                num_default_cat += 1\n",
    "                f_output.write(\"1\" + \"\\t\" + line + \"\\t\" + \"default_cat\" + \"\\n\") # positive\n",
    "\n",
    "    f_reviews.close()\n",
    "    f_meta.close()\n",
    "    f_output.close()\n",
    "    print(\"Num default categories:\", num_default_cat)\n",
    "    assert(num_default_cat == 0)\n",
    "    return output_file\n",
    "\n",
    "def write_preprocessed_output(sampled_instance_file):\n",
    "    print(\"start data processing...\")\n",
    "    dirs, _ = os.path.split(sampled_instance_file)\n",
    "    output_file = os.path.join(dirs, \"preprocessed.output\")\n",
    "\n",
    "    f_input = open(sampled_instance_file, \"r\")\n",
    "    f_output = open(output_file, \"w\")\n",
    "    user_count = {}\n",
    "\n",
    "    for line in f_input:\n",
    "        line = line.strip()\n",
    "        user = line.split(\"\\t\")[1] # [\"label\", \"user_id\", \"item_id\", \"timestamp\", \"cate_id\"]\n",
    "        if user not in user_count:\n",
    "            user_count[user] = 0\n",
    "        user_count[user] += 1\n",
    "\n",
    "    f_input.seek(0)\n",
    "    i = 0\n",
    "    last_user = None\n",
    "    for line in f_input:\n",
    "        line = line.strip()\n",
    "        user = line.split(\"\\t\")[1]\n",
    "        if user == last_user:\n",
    "            if i < user_count[user] - 2:\n",
    "                f_output.write(\"train\" + \"\\t\" + line + \"\\n\")\n",
    "            elif i < user_count[user] - 1:\n",
    "                f_output.write(\"valid\" + \"\\t\" + line + \"\\n\")\n",
    "            else:\n",
    "                f_output.write(\"test\" + \"\\t\" + line + \"\\n\")\n",
    "        else:\n",
    "            last_user = user\n",
    "            i = 0\n",
    "            if i < user_count[user] - 2:\n",
    "                f_output.write(\"train\" + \"\\t\" + line + \"\\n\")\n",
    "            elif i < user_count[user] - 1:\n",
    "                f_output.write(\"valid\" + \"\\t\" + line + \"\\n\")\n",
    "            else:\n",
    "                f_output.write(\"test\" + \"\\t\" + line + \"\\n\")\n",
    "        i += 1\n",
    "    return output_file\n",
    "\n",
    "def write_train_valid_test_data(data_dir, output_file):\n",
    "    train_file = os.path.join(data_dir, r'train_data')\n",
    "    valid_file = os.path.join(data_dir, r'valid_data')\n",
    "    test_file = os.path.join(data_dir, r'test_data')\n",
    "    \n",
    "    f_input = open(output_file, \"r\") # preprocessed.output\n",
    "    f_train = open(train_file, \"w\")\n",
    "    f_valid = open(valid_file, \"w\")\n",
    "    f_test = open(test_file, \"w\")\n",
    "    min_sequence = 1\n",
    "\n",
    "    print(\"train, valid, test positive data generating...\")\n",
    "    last_user_id = None\n",
    "    for line in f_input:\n",
    "        line_split = line.strip().split(\"\\t\")\n",
    "        tfile = line_split[0]        # train/valid/test\n",
    "        label = int(line_split[1])   # label 1\n",
    "        user_id = line_split[2]      # user\n",
    "        item_id = line_split[3]      # asin\n",
    "        date_time = line_split[4]    # ts\n",
    "        category = line_split[5]     # category\n",
    "\n",
    "        if tfile == \"train\":\n",
    "            fo = f_train\n",
    "        elif tfile == \"valid\":\n",
    "            fo = f_valid\n",
    "        elif tfile == \"test\":\n",
    "            fo = f_test\n",
    "        if user_id != last_user_id:\n",
    "            item_id_list = [] # collect all items\n",
    "            cate_list = []\n",
    "            dt_list = []\n",
    "        else:\n",
    "            history_clk_num = len(item_id_list)\n",
    "            cat_str = \"\"\n",
    "            iid_str = \"\"\n",
    "            dt_str = \"\"\n",
    "            for c1 in cate_list:\n",
    "                cat_str += c1 + \",\"\n",
    "            for iid in item_id_list:\n",
    "                iid_str += iid + \",\"\n",
    "            for dt_time in dt_list:\n",
    "                dt_str += dt_time + \",\"\n",
    "            if len(cat_str) > 0:\n",
    "                cat_str = cat_str[:-1]\n",
    "            if len(iid_str) > 0:\n",
    "                iid_str = iid_str[:-1]\n",
    "            if len(dt_str) > 0:\n",
    "                dt_str = dt_str[:-1]\n",
    "            if history_clk_num >= min_sequence:\n",
    "                fo.write(\n",
    "                    line_split[1]\n",
    "                    + \"\\t\"\n",
    "                    + user_id\n",
    "                    + \"\\t\"\n",
    "                    + item_id\n",
    "                    + \"\\t\"\n",
    "                    + category\n",
    "                    + \"\\t\"\n",
    "                    + date_time\n",
    "                    + \"\\t\"\n",
    "                    + iid_str\n",
    "                    + \"\\t\"\n",
    "                    + cat_str\n",
    "                    + \"\\t\"\n",
    "                    + dt_str\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "        last_user_id = user_id\n",
    "        if label:\n",
    "            item_id_list.append(item_id)\n",
    "            cate_list.append(category)\n",
    "            dt_list.append(date_time)\n",
    "    return train_file, valid_file, test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73cc68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocab_pkl_files(data_dir, train_file):\n",
    "    user_vocab = os.path.join(data_dir, r'user_vocab.pkl')\n",
    "    item_vocab = os.path.join(data_dir, r'item_vocab.pkl')\n",
    "    cate_vocab = os.path.join(data_dir, r'category_vocab.pkl')\n",
    "\n",
    "    f_train = open(train_file, \"r\") # NOTE: only train_file\n",
    "\n",
    "    user_dict = {}\n",
    "    item_dict = {}\n",
    "    cat_dict = {}\n",
    "\n",
    "    print(\"vocab generating...\")\n",
    "    for line in f_train:\n",
    "        arr = line.strip(\"\\n\").split(\"\\t\") # label uid asin category ts asin_list, cat_list, ts_list\n",
    "        uid = arr[1]\n",
    "        iid = arr[2]\n",
    "        cat = arr[3]\n",
    "        iid_list = arr[5]\n",
    "        cat_list = arr[6]\n",
    "\n",
    "        if uid not in user_dict:\n",
    "            user_dict[uid] = 0\n",
    "        user_dict[uid] += 1\n",
    "        if iid not in item_dict:\n",
    "            item_dict[iid] = 0\n",
    "        item_dict[iid] += 1\n",
    "        if cat not in cat_dict:\n",
    "            cat_dict[cat] = 0\n",
    "        cat_dict[cat] += 1\n",
    "        if len(iid_list) == 0:\n",
    "            print(\"No history\", uid)\n",
    "            continue\n",
    "        for m in iid_list.split(\",\"):\n",
    "            if m not in item_dict:\n",
    "                item_dict[m] = 0\n",
    "            item_dict[m] += 1\n",
    "        for c in cat_list.split(\",\"):\n",
    "            if c not in cat_dict:\n",
    "                cat_dict[c] = 0\n",
    "            cat_dict[c] += 1\n",
    "\n",
    "    sorted_user_dict = sorted(user_dict.items(), key=lambda x: x[1], reverse=True) # sorted on popularity\n",
    "    sorted_item_dict = sorted(item_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_cat_dict = sorted(cat_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    uid_voc = {}\n",
    "    index = 0\n",
    "    for key, value in sorted_user_dict:\n",
    "        uid_voc[key] = index\n",
    "        index += 1\n",
    "\n",
    "    iid_voc = {}\n",
    "    iid_voc[\"default_mid\"] = 0\n",
    "    index = 1\n",
    "    for key, value in sorted_item_dict:\n",
    "        iid_voc[key] = index\n",
    "        index += 1\n",
    "\n",
    "    cat_voc = {}\n",
    "    cat_voc[\"default_cat\"] = 0\n",
    "    index = 1\n",
    "    for key, value in sorted_cat_dict:\n",
    "        cat_voc[key] = index\n",
    "        index += 1\n",
    "\n",
    "    cPickle.dump(uid_voc, open(user_vocab, \"wb\"))\n",
    "    cPickle.dump(iid_voc, open(item_vocab, \"wb\"))\n",
    "    cPickle.dump(cat_voc, open(cate_vocab, \"wb\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d425a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_neg_samples(sampled_instance_file, item2cate, valid_file, test_file):\n",
    "    columns = [\"label\", \"user_id\", \"item_id\", \"timestamp\", \"cate_id\"]\n",
    "    ns_df = pd.read_csv(sampled_instance_file, sep=\"\\t\", names=columns)\n",
    "    items_with_popular = list(ns_df[\"item_id\"])\n",
    "\n",
    "    # valid negative sampling\n",
    "    print(\"start valid negative sampling\")\n",
    "    with open(valid_file, \"r\") as f:\n",
    "        valid_lines = f.readlines()\n",
    "    write_valid = open(valid_file, \"w\")\n",
    "    for line in valid_lines:\n",
    "        write_valid.write(line)\n",
    "        words = line.strip().split(\"\\t\")\n",
    "        positive_item = words[2]\n",
    "        count = 0\n",
    "        neg_items = set()\n",
    "        while count < VALID_NUM_NGS:\n",
    "            neg_item = random.choice(items_with_popular)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "            count += 1\n",
    "            neg_items.add(neg_item)\n",
    "            words[0] = \"0\"\n",
    "            words[2] = neg_item\n",
    "            words[3] = item2cate[neg_item]\n",
    "            write_valid.write(\"\\t\".join(words) + \"\\n\")\n",
    "\n",
    "    # test negative sampling\n",
    "    print(\"start test negative sampling\")\n",
    "    with open(test_file, \"r\") as f:\n",
    "        test_lines = f.readlines()\n",
    "    write_test = open(test_file, \"w\")\n",
    "    for line in test_lines:\n",
    "        write_test.write(line)\n",
    "        words = line.strip().split(\"\\t\")\n",
    "        positive_item = words[2]\n",
    "        count = 0\n",
    "        neg_items = set()\n",
    "        while count < TEST_NUM_NGS:\n",
    "            neg_item = random.choice(items_with_popular)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "            count += 1\n",
    "            neg_items.add(neg_item)\n",
    "            words[0] = \"0\"\n",
    "            words[2] = neg_item\n",
    "            words[3] = item2cate[neg_item]\n",
    "            write_test.write(\"\\t\".join(words) + \"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8d9e1",
   "metadata": {},
   "source": [
    "### Steps for preparing the outputs required by the SLi-Rec model\n",
    "\n",
    "1. filter the items dataframe for the main categories under \"Electronics\"\n",
    "2. filter the ratings dataframe to only include reviews for the main categories\n",
    "3. write the reviews output file (reviews preprocessing)\n",
    "4. write the meta data output file (meta preprocessing)\n",
    "5. create the instance.output\n",
    "6. create item2category dictionary\n",
    "7. split into train (all records for a user except the last 2), validation (second last), test (last), write preprocessed.output\n",
    "8. generate vocabulary files\n",
    "9. negative sampling (validation, test only); train is done inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5684599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, dataset):\n",
    "    data_dir = DATA_DIR_SLIREC + dataset + '/'\n",
    "    \n",
    "    # Step 1: filter the items dataframe for the main categories under \"Electronics\"\n",
    "    items_filtered_df = items_df[items_df['main_cat'].isin(lst_main_cat)].copy()\n",
    "    items_filtered_ids = items_filtered_df['asin'].values\n",
    "\n",
    "    # Step 2: filter the ratings dataframe\n",
    "    ratings_filtered_df = ratings_df[ratings_df['asin'].isin(items_filtered_ids)].copy()\n",
    "    \n",
    "    print(dataset)\n",
    "    print(\"Num ratings:\", ratings_filtered_df.shape[0])\n",
    "    print(\"users\", ratings_filtered_df['reviewerID'].nunique(), \"items\", ratings_filtered_df['asin'].nunique())\n",
    "    \n",
    "    # Step 3: write the reviews output file (reviews preprocessing)\n",
    "    # reviews_writefile = data_dir + 'reviews.output'\n",
    "    reviews_writefile = write_reviews(data_dir, ratings_filtered_df)\n",
    "    \n",
    "    # Step 4: write the meta output file (meta preprocessing)\n",
    "    # meta_writefile = data_dir + 'meta.output'\n",
    "    meta_writefile = write_meta(data_dir, items_df)\n",
    "    \n",
    "    # Step 5: create the instance.output\n",
    "    # instance_output = data_dir + 'instance.output'\n",
    "    instance_output = write_instance_output(reviews_writefile, meta_writefile)\n",
    "    \n",
    "    # Step 6: create item2category dictionary\n",
    "    instance_df = pd.read_csv(\n",
    "        instance_output,\n",
    "        sep=\"\\t\",\n",
    "        names=[\"label\", \"user_id\", \"item_id\", \"timestamp\", \"cate_id\"],\n",
    "    )\n",
    "    \n",
    "    # tmp_df = instance_df[[\"item_id\",\"cate_id\"]].drop_duplicates()\n",
    "    # print(tmp_df.shape)\n",
    "    item2cate = instance_df.set_index(\"item_id\")[\"cate_id\"].to_dict() # item_id/asin: category\n",
    "    print(instance_df.shape, instance_df['item_id'].nunique(), len(item2cate))\n",
    "    del instance_df\n",
    "    \n",
    "    # everything in the instance.output is either train, valid or test\n",
    "    sampled_instance_file = instance_output\n",
    "    \n",
    "    # Step 7: split into train (all records for a user except the last 2), \n",
    "    # validation (second last), test (last)\n",
    "    # output_file = data_dir + 'preprocessed.output'\n",
    "    output_file = write_preprocessed_output(sampled_instance_file)\n",
    "    train_file, valid_file, test_file = write_train_valid_test_data(data_dir, output_file)\n",
    "\n",
    "    # Step 8: generate vocabulary files        \n",
    "    gen_vocab_pkl_files(data_dir, train_file)\n",
    "    \n",
    "    # Step 9: negative sampling (validation, test only); train is done inline\n",
    "    gen_neg_samples(sampled_instance_file, item2cate, valid_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309922a",
   "metadata": {},
   "source": [
    "## Electronics\n",
    "\n",
    "- Refer to the amzn_gen_input_wide_deep notebook on how the sub categories are picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd5b5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n",
      "Num ratings: 5613183\n",
      "users 830668 items 63725\n",
      "start create instances...\n",
      "Num default categories: 0\n",
      "(5613183, 5) 63725 63725\n",
      "start data processing...\n",
      "train, valid, test positive data generating...\n",
      "vocab generating...\n",
      "start valid negative sampling\n",
      "start test negative sampling\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Electronics'\n",
    "lst_main_cat = ['All Electronics', 'Amazon Devices', 'Apple Products', \n",
    "                 'Camera & Photo', 'Car Electronics', 'Cell Phones & Accessories', 'Computers',\n",
    "                 'Electronics', 'GPS & Navigation', 'Home Audio & Theater', 'Industrial & Scientific',\n",
    "                 'Portable Audio & Accessories']\n",
    "\n",
    "lst_sub_cat = ['Accessories','Computers & Accessories','Office Products','Video Games',\n",
    "               'Accessories & Supplies','Tools & Home Improvement','Computer Accessories & Peripherals',\n",
    "               'Audio & Video Accessories', 'Automotive', 'Office & School Supplies',\n",
    "               'Car & Vehicle Electronics', 'Industrial & Scientific','Sports & Outdoors','Office Electronics',\n",
    "               'Home & Kitchen','Musical Instruments','Portable Audio & Video','Electrical',\n",
    "               'Clothing, Shoes & Jewelry','Toys & Games','Laptop Accessories','Home Audio',\n",
    "               'Controllers','Computer Components','Sports & Fitness']\n",
    "\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, 'Electronics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ac402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
