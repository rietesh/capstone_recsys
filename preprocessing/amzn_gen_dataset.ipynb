{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd37576f",
   "metadata": {},
   "source": [
    "# Read all item IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d98476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!\n",
      "Total items 15023059\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "items_w_meta = set()\n",
    "# skip_mag_sub = 0\n",
    "\n",
    "with open('/home/shiv/Downloads/All_Amazon_Meta.json', 'r') as f: # 15,023,059 lines!\n",
    "    line_cnt = 0\n",
    "    for line in f:\n",
    "        line_new = eval(line)\n",
    "        \n",
    "        items_w_meta.add(line_new['asin']) # collect all the item IDs\n",
    "        line_cnt += 1\n",
    "        if line_cnt % 100_000 == 0:\n",
    "            if line_cnt % 1_000_000 == 0:\n",
    "                print('!', end = '')\n",
    "            else:\n",
    "                print('*', end = '')  \n",
    "print()\n",
    "print(\"Total items\", line_cnt)\n",
    "\n",
    "# NOTES: lots of duplicate asins in meta file: only 14741571 items from 15023059 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a9a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "items_pkl_path = f'/home/shiv/Documents/DataScience/Capstone/Data/items_w_meta.pkl'\n",
    "\n",
    "with open(items_pkl_path, 'wb') as handle:\n",
    "    pickle.dump(items_w_meta, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb59ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14741571\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "items_pkl_path = f'/home/shiv/Documents/DataScience/Capstone/Data/items_w_meta.pkl'\n",
    "with open(items_pkl_path, 'rb') as handle:\n",
    "    items_w_meta = pickle.load(handle)\n",
    "print(len(items_w_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc060ee2",
   "metadata": {},
   "source": [
    "# Read all reviews (157M + )\n",
    "\n",
    "1. <s>save only reviews on or after the CUTOFF_DATE</s>\n",
    "2. save only verified reviews\n",
    "3. only for those products for which we have the meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9918501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!**\n",
      "157260920\n",
      "133256497\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# CUTOFF_DATE = pd.to_datetime(\"2017-06-01\")\n",
    "\n",
    "ratings_items = []\n",
    "line_cnt = 0\n",
    "with open('/home/shiv/Documents/DataScience/Capstone/Data/All_Amazon_Review_5.json', 'r') as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line) # overall, verified, reviewerID, asin, unixReviewTime, reviewText, summary\n",
    "#         ts = pd.to_datetime(review['unixReviewTime'], unit='s')\n",
    "#         if ts >= CUTOFF_DATE and review['verified'] and review['asin'] in items_w_meta:\n",
    "        if review['verified'] and review['asin'] in items_w_meta:\n",
    "            ratings_items.append([review['reviewerID'], review['asin'],\n",
    "                                  review['overall'], review['unixReviewTime']])\n",
    "      \n",
    "        line_cnt += 1\n",
    "        if line_cnt % 100_000 == 0:\n",
    "            if line_cnt % 1_000_000 == 0:\n",
    "                print('!', end = '')\n",
    "            else:\n",
    "                print('*', end = '')\n",
    "print()\n",
    "print(line_cnt) # 157260920\n",
    "print(len(ratings_items))\n",
    "\n",
    "# NOTES: even though it is curated, there aren't 5 reviews per product nor 5 reviews per user\n",
    "# So, choosing to only look at verified reviews and for products that have the meta information.\n",
    "# Down to 133M+ reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256a88ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEUZVK0XRMI40</td>\n",
       "      <td>B000HA3CEY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1363824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1U4R8DP7MLOYP</td>\n",
       "      <td>B000HA3CEY</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1354060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1JIXKFDTEC6DZ</td>\n",
       "      <td>B000HA3CEY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1225411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A11AP8Z9IF3NH2</td>\n",
       "      <td>B001EJZNXK</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1361836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2YJWVLJ21CUFG</td>\n",
       "      <td>B001EJZNXK</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1357171200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  rating  unixReviewTime\n",
       "0   AEUZVK0XRMI40  B000HA3CEY     5.0      1363824000\n",
       "1  A1U4R8DP7MLOYP  B000HA3CEY     2.0      1354060800\n",
       "2  A1JIXKFDTEC6DZ  B000HA3CEY     5.0      1225411200\n",
       "3  A11AP8Z9IF3NH2  B001EJZNXK     5.0      1361836800\n",
       "4  A2YJWVLJ21CUFG  B001EJZNXK     4.0      1357171200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = pd.DataFrame(ratings_items, columns = ['reviewerID', 'asin', \n",
    "                                                    'rating', 'unixReviewTime'])\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19a65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_csv_path = f'/home/shiv/Documents/DataScience/Capstone/Data/all_reviews.csv'\n",
    "ratings_df.to_csv(ratings_csv_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43730e04",
   "metadata": {},
   "source": [
    "# Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251ace5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ratings_csv_path = f'/home/shiv/Documents/DataScience/Capstone/Data/all_reviews_5.csv'\n",
    "ratings_df = pd.read_csv(ratings_csv_path, header=None)\n",
    "ratings_df.columns = ['reviewerID', 'asin', 'rating', 'unixReviewTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f78ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min reviews per item 5\n",
      "min reviews per user 5\n"
     ]
    }
   ],
   "source": [
    "def get_min_reviews_prod_per_user(ratings_df, ret_min_prod, ret_min_user):\n",
    "    prod_df = ratings_df[['reviewerID','asin']].groupby(['asin']).count()\n",
    "    min_prod = -1\n",
    "    if ret_min_prod:\n",
    "        min_prod = prod_df['reviewerID'].min()\n",
    "        print(\"min reviews per item\", min_prod) #, prod_df['reviewerID'].max())\n",
    "    user_df = ratings_df[['reviewerID','asin']].groupby(['reviewerID']).count()\n",
    "    min_user = -1\n",
    "    if ret_min_user:\n",
    "        min_user = user_df['asin'].min()\n",
    "        print(\"min reviews per user\", min_user) #, user_df['asin'].max())\n",
    "    return min_prod, prod_df, min_user, user_df\n",
    "\n",
    "min_prod, prod_df, min_user, user_df = get_min_reviews_prod_per_user(ratings_df, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab59ac",
   "metadata": {},
   "source": [
    "# Remove items with less than MIN_REVIEWS reviews\n",
    "\n",
    "This is a multi-round process. In each round, we do the following:\n",
    "\n",
    "1. Filter the ratings dataframe by removing products that have less than MIN_REVIEWS reviews\n",
    "2. Next, check if the remaining users have MIN_REVIEWS_PER_USER reviews\n",
    "3. If both MIN_REVIEWS and MIN_REVIEWS_PER_USER conditions are satisfied, we are done!\n",
    "4. Otherwise filter the ratings dataframe by removing users that have less than MIN_REVIEWS_PER_USER reviews\n",
    "5. Next, check if the remaining items have MIN_REVIEWS reviews\n",
    "6. If both MIN_REVIEWS and MIN_REVIEWS_PER_USER conditions are satisfied, we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c2c228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users 8534835 #items 3214887\n"
     ]
    }
   ],
   "source": [
    "MIN_REVIEWS = 20\n",
    "MIN_REVIEWS_PER_USER = 20\n",
    "\n",
    "print(\"# users\", ratings_df['reviewerID'].nunique(), \"#items\", ratings_df['asin'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20be7686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round # 1\n",
      "# items: 1070664\n",
      "(108501158, 4)\n",
      "min reviews per user 1\n",
      "# users 1284496\n",
      "(50858773, 4)\n",
      "min reviews per item 1\n",
      "Round # 2\n",
      "# items: 552846\n",
      "(44457426, 4)\n",
      "min reviews per user 1\n",
      "# users 1040451\n",
      "(40210494, 4)\n",
      "min reviews per item 2\n",
      "Round # 3\n",
      "# items: 497273\n",
      "(39222851, 4)\n",
      "min reviews per user 3\n",
      "# users 999330\n",
      "(38455791, 4)\n",
      "min reviews per item 5\n",
      "Round # 4\n",
      "# items: 486864\n",
      "(38261068, 4)\n",
      "min reviews per user 14\n",
      "# users 990998\n",
      "(38103482, 4)\n",
      "min reviews per item 15\n",
      "Round # 5\n",
      "# items: 484648\n",
      "(38061565, 4)\n",
      "min reviews per user 17\n",
      "# users 989235\n",
      "(38028150, 4)\n",
      "min reviews per item 16\n",
      "Round # 6\n",
      "# items: 484197\n",
      "(38019593, 4)\n",
      "min reviews per user 18\n",
      "# users 988826\n",
      "(38011832, 4)\n",
      "min reviews per item 16\n",
      "Round # 7\n",
      "# items: 484088\n",
      "(38009765, 4)\n",
      "min reviews per user 19\n",
      "# users 988743\n",
      "(38008188, 4)\n",
      "min reviews per item 19\n",
      "Round # 8\n",
      "# items: 484066\n",
      "(38007770, 4)\n",
      "min reviews per user 19\n",
      "# users 988722\n",
      "(38007371, 4)\n",
      "min reviews per item 19\n",
      "Round # 9\n",
      "# items: 484062\n",
      "(38007295, 4)\n",
      "min reviews per user 19\n",
      "# users 988718\n",
      "(38007219, 4)\n",
      "min reviews per item 20\n"
     ]
    }
   ],
   "source": [
    "round = 1\n",
    "while True:\n",
    "    print(\"Round #\", round)\n",
    "    \n",
    "    prod_list = list(prod_df[prod_df['reviewerID'] >= MIN_REVIEWS].index)\n",
    "    print(\"# items:\", len(prod_list))\n",
    "    \n",
    "    ratings_df = ratings_df[ratings_df['asin'].isin(prod_list)]\n",
    "    print(ratings_df.shape)\n",
    "    \n",
    "    min_prod, prod_df, min_user, user_df = get_min_reviews_prod_per_user(ratings_df, False, True)\n",
    "    min_prod = MIN_REVIEWS\n",
    "    \n",
    "    if min_prod >= MIN_REVIEWS and min_user >= MIN_REVIEWS_PER_USER:\n",
    "        break    \n",
    "\n",
    "    user_list = list(user_df[user_df['asin'] >= MIN_REVIEWS_PER_USER].index)\n",
    "    print(\"# users\", len(user_list))\n",
    "    \n",
    "    ratings_df = ratings_df[ratings_df['reviewerID'].isin(user_list)]\n",
    "    print(ratings_df.shape)\n",
    "    \n",
    "    min_prod, prod_df, min_user, user_df = get_min_reviews_prod_per_user(ratings_df, True, False)\n",
    "    min_user = MIN_REVIEWS_PER_USER\n",
    "    \n",
    "    if min_prod >= MIN_REVIEWS and min_user >= MIN_REVIEWS_PER_USER:\n",
    "        break    \n",
    "    round += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692962de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.to_csv(f'/home/shiv/Documents/DataScience/Capstone/Data/all_reviews_{MIN_REVIEWS}.csv', \n",
    "                  header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def1d35",
   "metadata": {},
   "source": [
    "# Get the meta information for the products in all_reviews_{MIN_REVIEWS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e0856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38007219, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A14E6RS86U3IWJ</td>\n",
       "      <td>B001EJY2IC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1363737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A279VRO0YVM43V</td>\n",
       "      <td>B001EJY2IC</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1362355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A1HJ4BHRNFBGSX</td>\n",
       "      <td>B001EJY2IC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1473033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A139FFBSZTFUAT</td>\n",
       "      <td>B001EJY2IC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1468281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A2MBPLWN8MKY93</td>\n",
       "      <td>B001EJY2IC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1466208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128135433</th>\n",
       "      <td>A1WAJWOFBE4PPR</td>\n",
       "      <td>B01HB9Q7CW</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1537747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128135642</th>\n",
       "      <td>A1MCN1E5GNFNXJ</td>\n",
       "      <td>B01HI9W5HQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1538006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128135643</th>\n",
       "      <td>A3IAV917ZH50SE</td>\n",
       "      <td>B01HI9W5HQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1535068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128135644</th>\n",
       "      <td>A2RPMPBKX1MWR7</td>\n",
       "      <td>B01HI9W5HQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1534550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128135645</th>\n",
       "      <td>A1FMG24RFTCLT</td>\n",
       "      <td>B01HI9W5HQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1529193600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38007219 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               reviewerID        asin  rating  unixReviewTime\n",
       "5          A14E6RS86U3IWJ  B001EJY2IC     5.0      1363737600\n",
       "6          A279VRO0YVM43V  B001EJY2IC     4.0      1362355200\n",
       "9          A1HJ4BHRNFBGSX  B001EJY2IC     5.0      1473033600\n",
       "11         A139FFBSZTFUAT  B001EJY2IC     5.0      1468281600\n",
       "12         A2MBPLWN8MKY93  B001EJY2IC     5.0      1466208000\n",
       "...                   ...         ...     ...             ...\n",
       "128135433  A1WAJWOFBE4PPR  B01HB9Q7CW     5.0      1537747200\n",
       "128135642  A1MCN1E5GNFNXJ  B01HI9W5HQ     1.0      1538006400\n",
       "128135643  A3IAV917ZH50SE  B01HI9W5HQ     4.0      1535068800\n",
       "128135644  A2RPMPBKX1MWR7  B01HI9W5HQ     5.0      1534550400\n",
       "128135645   A1FMG24RFTCLT  B01HI9W5HQ     3.0      1529193600\n",
       "\n",
       "[38007219 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# ratings_df = pd.read_csv(f'/home/shiv/Documents/DataScience/Capstone/Data/all_reviews_{MIN_REVIEWS}.csv',\n",
    "#                          header=None)\n",
    "# ratings_df.columns = ['reviewerID', 'asin', 'rating', 'unixReviewTime']\n",
    "print(ratings_df.shape)\n",
    "ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add8a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users 988718 # items 484062\n"
     ]
    }
   ],
   "source": [
    "ratings_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "set_items = set(ratings_df['asin'].unique()) # all unique items in the ratings dataframe\n",
    "print(\"# users\", ratings_df['reviewerID'].nunique(), \"# items\", len(set_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8281ab",
   "metadata": {},
   "source": [
    "## Clean up meta information\n",
    "\n",
    "Meta information consists of the following:\n",
    "\n",
    "1. main category\n",
    "2. categories\n",
    "3. price\n",
    "4. title\n",
    "\n",
    "- Each of the columns can come with HTML escape sequences that need to be removed.\n",
    "- Each of the columns can be empty!\n",
    "- price and title columns are very noisy!\n",
    "- There are some items that have too many categories. Only the first 5 are saved to conserve space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891ff0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!*********!\n",
      "498489 duplicates 14427\n",
      "main category 1269\n",
      "lst category 19596\n",
      "title 0 104\n",
      "price 0 129512\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "items = []\n",
    "meta_items = set()\n",
    "\n",
    "with open('/home/shiv/Downloads/All_Amazon_Meta.json', 'r') as f: # 15,023,059 lines!\n",
    "    line_cnt = 0\n",
    "    empty_main_cat = 0\n",
    "    empty_lst_cat = 0\n",
    "    empty_price = 0\n",
    "    empty_title = 0\n",
    "    missing_price_cnt = 0\n",
    "    missing_title_cnt = 0\n",
    "    dups_cnt = 0\n",
    "    \n",
    "    for line in f:\n",
    "        line_new = eval(line)\n",
    "\n",
    "        asin = line_new['asin']\n",
    "        if asin in set_items:\n",
    "            lst_vals = []\n",
    "            # asin\n",
    "            lst_vals.append(line_new['asin'])\n",
    "\n",
    "            # main_cat\n",
    "            main_cat = line_new['main_cat']\n",
    "            main_cat = html.unescape(main_cat)\n",
    "            lst_vals.append(main_cat)\n",
    "            if line_new['main_cat'] == '':\n",
    "                empty_main_cat += 1\n",
    "            \n",
    "            # categories\n",
    "            lst_categories = line_new['category']\n",
    "            if len(lst_categories) > 5:\n",
    "                lst_categories = lst_categories[:5]\n",
    "            cats = []\n",
    "            for cat in lst_categories:\n",
    "                cats.append(html.unescape(cat))\n",
    "            if len(cats) == 0:\n",
    "                empty_lst_cat += 1\n",
    "            lst_vals.append('|'.join(cats))\n",
    "            \n",
    "            # price\n",
    "            if 'price' in line_new:\n",
    "                lst_vals.append(line_new['price'])\n",
    "                if line_new['price'].strip() == '':\n",
    "                    empty_price += 1\n",
    "            else:\n",
    "                missing_price_cnt += 1\n",
    "                lst_vals.append('')\n",
    "\n",
    "            # title    \n",
    "            if 'title' in line_new:\n",
    "                title = line_new['title'].strip()\n",
    "                title = html.unescape(title)\n",
    "                if title == '':\n",
    "                    empty_title += 1\n",
    "                    title = 'Unknown'\n",
    "                lst_vals.append(title)                \n",
    "            else:\n",
    "                missing_title_cnt += 1\n",
    "                lst_vals.append('')\n",
    "\n",
    "            if asin in meta_items:\n",
    "                lst_vals.append(1)\n",
    "                dups_cnt += 1\n",
    "            else:\n",
    "                lst_vals.append(0)\n",
    "                meta_items.add(asin)\n",
    "                \n",
    "            items.append(lst_vals)\n",
    "\n",
    "        line_cnt += 1\n",
    "        if line_cnt % 100_000 == 0:\n",
    "            if line_cnt % 1_000_000 == 0:\n",
    "                print('!', end = '')\n",
    "            else:\n",
    "                print('*', end = '')        \n",
    "\n",
    "print()\n",
    "print(len(items), \"duplicates\", dups_cnt)\n",
    "print(\"main category\", empty_main_cat)\n",
    "print(\"lst category\", empty_lst_cat)\n",
    "print(\"title\", missing_title_cnt, empty_title)\n",
    "print(\"price\", missing_price_cnt, empty_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ec6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert if we did not get the meta data for all unique items\n",
    "lst_no_meta_items = list(set_items - meta_items)\n",
    "assert(len(lst_no_meta_items) == 0)\n",
    "# ratings_df = ratings_df[~ratings_df['asin'].isin(lst_no_meta_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97d7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.DataFrame(items, columns = ['asin', 'main_cat', 'category', 'price', 'title', 'duplicate'])\n",
    "# sorted(items_df['main_cat'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5fe12a",
   "metadata": {},
   "source": [
    "## Fix html tags appearing in main_cat, use alt instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1119a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.found_tag = False\n",
    "        self.recorded_alt = False\n",
    "        self.alt_cat = ''\n",
    "        \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        # print(\"Encountered a start tag:\", attrs)\n",
    "        if tag == 'img':\n",
    "            self.found_tag = True\n",
    "            for a in attrs:\n",
    "                if a[0] == 'alt':\n",
    "                    self.recorded_alt = True\n",
    "                    self.alt_cat = a[1].title()\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        # print(\"Encountered an end tag :\", tag)\n",
    "        self.found_tag = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        # print(\"Encountered some data  :\", data)\n",
    "        pass\n",
    "\n",
    "main_cat_dict = {}\n",
    "parser = MyHTMLParser()\n",
    "lst_main_cat = items_df.main_cat.unique()\n",
    "\n",
    "for cat in lst_main_cat:\n",
    "    parser.recorded_alt = False\n",
    "    parser.feed(cat)\n",
    "    if parser.recorded_alt:\n",
    "        # print(parser.alt_cat)\n",
    "        main_cat_dict[cat] = parser.alt_cat\n",
    "    else:\n",
    "        main_cat_dict[cat] = cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd020ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df['main_cat_tr'] = items_df['main_cat'].apply(lambda x: main_cat_dict[x])\n",
    "# sorted(items_df['main_cat_tr'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe0bf22",
   "metadata": {},
   "source": [
    "## Pick first category if main_cat_tr is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17b69478",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df['main_cat_tr'] = items_df[['main_cat_tr','category']].apply(lambda x: x['category'].split('|')[0] if x['main_cat_tr'] == '' else x['main_cat_tr'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0de9c",
   "metadata": {},
   "source": [
    "## Consolidate main category\n",
    "\n",
    "- Some of the main categories have very few items. We fold them into an overarching category by looking at the categories list (if not empty) and/or the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585a1c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>main_cat_tr</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>All Beauty</th>\n",
       "      <td>1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Electronics</th>\n",
       "      <td>10417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon Devices</th>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon Fashion</th>\n",
       "      <td>55759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon Fire TV</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon Home</th>\n",
       "      <td>68701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple Products</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Appliances</th>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arts, Crafts &amp; Sewing</th>\n",
       "      <td>7018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Audible Audiobooks</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Automotive</th>\n",
       "      <td>17695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baby</th>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Books</th>\n",
       "      <td>94614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buy a Kindle</th>\n",
       "      <td>17765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camera &amp; Photo</th>\n",
       "      <td>7035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car Electronics</th>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell Phones &amp; Accessories</th>\n",
       "      <td>18200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clothing, Shoes &amp; Jewelry</th>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Collectible Coins</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Collectibles &amp; Fine Art</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computers</th>\n",
       "      <td>14736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Digital Music</th>\n",
       "      <td>6634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electronics</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fire Phone</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPS &amp; Navigation</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gift Cards</th>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grocery</th>\n",
       "      <td>14320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grocery &amp; Gourmet Food</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Handmade</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health &amp; Personal Care</th>\n",
       "      <td>3316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home &amp; Business Services</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home &amp; Kitchen</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home Audio &amp; Theater</th>\n",
       "      <td>7618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Industrial &amp; Scientific</th>\n",
       "      <td>5101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kindle Store</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luxury Beauty</th>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEMBERSHIPS &amp; SUBSCRIPTIONS</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magazine Subscriptions</th>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies &amp; TV</th>\n",
       "      <td>19170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Musical Instruments</th>\n",
       "      <td>4172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Office Products</th>\n",
       "      <td>12874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patio, Lawn &amp; Garden</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pet Supplies</th>\n",
       "      <td>16481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Portable Audio &amp; Accessories</th>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prime Pantry</th>\n",
       "      <td>2436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software</th>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports &amp; Outdoors</th>\n",
       "      <td>31970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports Collectibles</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tools &amp; Home Improvement</th>\n",
       "      <td>27507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toys &amp; Games</th>\n",
       "      <td>20749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video Games</th>\n",
       "      <td>4655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              main_cat\n",
       "main_cat_tr                           \n",
       "All Beauty                        1513\n",
       "All Electronics                  10417\n",
       "Amazon Devices                     668\n",
       "Amazon Fashion                   55759\n",
       "Amazon Fire TV                       5\n",
       "Amazon Home                      68701\n",
       "Apple Products                      43\n",
       "Appliances                         249\n",
       "Arts, Crafts & Sewing             7018\n",
       "Audible Audiobooks                 173\n",
       "Automotive                       17695\n",
       "Baby                              1271\n",
       "Books                            94614\n",
       "Buy a Kindle                     17765\n",
       "Camera & Photo                    7035\n",
       "Car Electronics                   1290\n",
       "Cell Phones & Accessories        18200\n",
       "Clothing, Shoes & Jewelry          876\n",
       "Collectible Coins                   22\n",
       "Collectibles & Fine Art              4\n",
       "Computers                        14736\n",
       "Digital Music                     6634\n",
       "Electronics                         80\n",
       "Fire Phone                           8\n",
       "GPS & Navigation                   145\n",
       "Gift Cards                         332\n",
       "Grocery                          14320\n",
       "Grocery & Gourmet Food              13\n",
       "Handmade                            12\n",
       "Health & Personal Care            3316\n",
       "Home & Business Services            28\n",
       "Home & Kitchen                      78\n",
       "Home Audio & Theater              7618\n",
       "Industrial & Scientific           5101\n",
       "Kindle Store                         1\n",
       "Luxury Beauty                     1368\n",
       "MEMBERSHIPS & SUBSCRIPTIONS          1\n",
       "Magazine Subscriptions             329\n",
       "Movies & TV                      19170\n",
       "Musical Instruments               4172\n",
       "Office Products                  12874\n",
       "Patio, Lawn & Garden                21\n",
       "Pet Supplies                     16481\n",
       "Portable Audio & Accessories       221\n",
       "Prime Pantry                      2436\n",
       "Software                           793\n",
       "Sports & Outdoors                31970\n",
       "Sports Collectibles                  2\n",
       "Tools & Home Improvement         27507\n",
       "Toys & Games                     20749\n",
       "Video Games                       4655"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sorted(items_df['main_cat_tr'].unique())))\n",
    "items_df[['main_cat_tr', 'main_cat']].groupby(['main_cat_tr']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7156ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cat_map = { # Transform the main_cat if the count for a main category is low\n",
    "    'MEMBERSHIPS & SUBSCRIPTIONS': 'Home & Kitchen',\n",
    "    'Entertainment': 'Collectibles',\n",
    "    'Amazon Fire TV': 'Amazon Devices',\n",
    "    'Fire Phone' : 'Amazon Devices',\n",
    "    'Sports Collectibles': 'Collectibles',\n",
    "    'Collectibles & Fine Art' : 'Collectibles',\n",
    "    'Collectible Coins': 'Collectibles',\n",
    "    'Buy a Kindle': 'Kindle',\n",
    "    'Kindle Store': 'Kindle'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce12b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df['main_cat_tr'] = items_df['main_cat_tr'].apply(lambda x: main_cat_map[x] if x in main_cat_map else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81e87ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(items_df['main_cat_tr'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b963d",
   "metadata": {},
   "source": [
    "## Fix category lists:\n",
    "1. remove less frequently occuring categories\n",
    "2. remove main_category from this list of (sub)categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1bdf1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 498489/498489 [00:00<00:00, 1533226.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "cat_count = defaultdict(lambda: 0)\n",
    "with tqdm(total=items_df.shape[0]) as pbar:\n",
    "    categories = items_df['category'].values\n",
    "    for row in categories:\n",
    "        for cat in row.split('|'):\n",
    "            cat_count[cat] += 1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24658261",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df['category_tr'] = ''\n",
    "def update_category(row):\n",
    "    categories = row['category']\n",
    "    new_cat = []\n",
    "    # print(categories)\n",
    "    for cat in categories.split('|'):\n",
    "        if cat_count[cat] >= 5 and cat != row['main_cat_tr']:\n",
    "            new_cat.append(cat)\n",
    "    if len(new_cat) > 0:\n",
    "        row['category_tr'] = '|'.join(new_cat)\n",
    "    else:\n",
    "        row['category_tr'] = ''\n",
    "    return row\n",
    "items_df = items_df.apply(update_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e0cb44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 14427/14427 [08:23<00:00, 28.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the duplicate rows have no new information\n",
    "tmp_df = items_df[items_df['duplicate']==1]\n",
    "with tqdm(total=tmp_df.shape[0]) as pbar:\n",
    "    for _, row in tmp_df.iterrows():\n",
    "        dup_rows = items_df[items_df['asin']==row['asin']]\n",
    "        assert(dup_rows.shape[0]==2) # only 2 rows share an asin\n",
    "        assert(dup_rows.iloc[0]['category'] == dup_rows.iloc[1]['category'])\n",
    "        assert(dup_rows.iloc[0]['main_cat_tr'] == dup_rows.iloc[1]['main_cat_tr'])\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed8629",
   "metadata": {},
   "source": [
    "## Drop the unnecessary columns; save all_meta_{MIN_REVIEWS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a44b166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498489, 5)\n",
      "(484062, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "      <th>main_cat_tr</th>\n",
       "      <th>category_tr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7709260373</td>\n",
       "      <td>$23.99 - $29.99</td>\n",
       "      <td>LJYH Children's Collar Motorcycle Faux Leather...</td>\n",
       "      <td>Amazon Fashion</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry|Boys|Clothing|Jacket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9792252916</td>\n",
       "      <td></td>\n",
       "      <td>Casio Men's Quartz Resin Casual Watch, Color:B...</td>\n",
       "      <td>Amazon Fashion</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry|Men|Watches|Wrist Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00001QHXX</td>\n",
       "      <td></td>\n",
       "      <td>Politically Incorrect Nixon Vinyl Mask</td>\n",
       "      <td>Amazon Fashion</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry|Costumes &amp; Accessori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00001TOXD</td>\n",
       "      <td>$4.99</td>\n",
       "      <td>Adult Witch Broom</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry|Costumes &amp; Accessori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00001T38Y</td>\n",
       "      <td>$24.98</td>\n",
       "      <td>Star Wars Stormtrooper Deluxe Adult Mask</td>\n",
       "      <td>Amazon Fashion</td>\n",
       "      <td>Clothing, Shoes &amp; Jewelry|Costumes &amp; Accessori...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin            price  \\\n",
       "0  7709260373  $23.99 - $29.99   \n",
       "1  9792252916                    \n",
       "2  B00001QHXX                    \n",
       "3  B00001TOXD            $4.99   \n",
       "4  B00001T38Y           $24.98   \n",
       "\n",
       "                                               title     main_cat_tr  \\\n",
       "0  LJYH Children's Collar Motorcycle Faux Leather...  Amazon Fashion   \n",
       "1  Casio Men's Quartz Resin Casual Watch, Color:B...  Amazon Fashion   \n",
       "2             Politically Incorrect Nixon Vinyl Mask  Amazon Fashion   \n",
       "3                                  Adult Witch Broom    Toys & Games   \n",
       "4           Star Wars Stormtrooper Deluxe Adult Mask  Amazon Fashion   \n",
       "\n",
       "                                         category_tr  \n",
       "0  Clothing, Shoes & Jewelry|Boys|Clothing|Jacket...  \n",
       "1  Clothing, Shoes & Jewelry|Men|Watches|Wrist Wa...  \n",
       "2  Clothing, Shoes & Jewelry|Costumes & Accessori...  \n",
       "3  Clothing, Shoes & Jewelry|Costumes & Accessori...  \n",
       "4  Clothing, Shoes & Jewelry|Costumes & Accessori...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df.drop(columns=['main_cat','category','duplicate'], inplace=True)\n",
    "print(items_df.shape)\n",
    "items_df.drop_duplicates(inplace=True)\n",
    "print(items_df.shape)\n",
    "items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd66c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.rename(columns={'main_cat_tr': 'main_cat', 'category_tr': 'category'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43a91524",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.to_csv(f'/home/shiv/Documents/DataScience/Capstone/Data/all_meta_{MIN_REVIEWS}.csv', \n",
    "                header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44944d2f",
   "metadata": {},
   "source": [
    "# Prepare for wide_deep\n",
    "\n",
    "1. Restart the kernel\n",
    "2. Read the ratings and items dataframes\n",
    "3. Prepare Electronics, Home and Games datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6396a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "MIN_REVIEWS = 20\n",
    "\n",
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/wide_deep/'\n",
    "ratings_df = pd.read_csv(DATA_DIR + f'all_reviews_{MIN_REVIEWS}.csv', header=None)\n",
    "ratings_df.columns = ['reviewerID', 'asin', 'rating', 'unixTimeStamp']     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7628de",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.read_csv(DATA_DIR + f'all_meta_{MIN_REVIEWS}.csv', header=None)\n",
    "# items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12391d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.columns=['asin','price','title','main_cat','category']\n",
    "items_df['category'].fillna('', inplace=True)\n",
    "items_df['price'].fillna('$$$', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01aa035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample to show how to drop a main category\n",
    "\n",
    "# mag_subs = items_df[items_df['main_cat']=='Magazine Subscriptions']['asin'].values\n",
    "# ratings_df=ratings_df[~ratings_df['asin'].isin(mag_subs)]\n",
    "# items_df=items_df[~items_df['asin'].isin(mag_subs)]\n",
    "# ratings_df.reset_index(inplace=True,drop=True)\n",
    "# items_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0cf3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(items_df['main_cat'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fb7b4",
   "metadata": {},
   "source": [
    "## Use all the main categories; if only specific main categories are needed, skip to [Electronics](#electronics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a11847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting reviewerID to userID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 38007219/38007219 [07:52<00:00, 80368.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged dataframe (38007219, 8)\n",
      "Converting asin to itemID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 484062/484062 [00:07<00:00, 62080.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Step 3: convert reviewerID to userID (0 based)\n",
    "print(\"Converting reviewerID to userID\")\n",
    "reviewers_dict = {}\n",
    "reviewer_id = 0\n",
    "with tqdm(total=ratings_df.shape[0]) as pbar:\n",
    "    for _, row in ratings_df.iterrows():\n",
    "        if row['reviewerID'] not in reviewers_dict:\n",
    "            reviewers_dict[row['reviewerID']] = reviewer_id\n",
    "            reviewer_id += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "ratings_df['userID'] = ratings_df['reviewerID'].apply(lambda x: reviewers_dict[x])\n",
    "\n",
    "# Step 4: save the reviewerID dict (perhaps for UI)\n",
    "users_pkl_path = DATA_DIR + f'users_all_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "with open(users_pkl_path, 'wb') as handle:\n",
    "    pickle.dump(reviewers_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "del reviewers_dict\n",
    "ratings_df.drop(columns=['reviewerID'], inplace=True) # we will henceforth use userID\n",
    "\n",
    "# Step 5: left merge the ratings with items on asin\n",
    "data = ratings_df.merge(items_df, on=['asin'], how='left')\n",
    "print(\"merged dataframe\", data.shape)\n",
    "data['category'] = data['category'].astype('string')\n",
    "\n",
    "del ratings_df\n",
    "\n",
    "# Step 6: convert asin to itemID (0 based)\n",
    "print(\"Converting asin to itemID\")\n",
    "items_dict = {}\n",
    "item_id = 0\n",
    "\n",
    "with tqdm(total=items_df.shape[0]) as pbar:\n",
    "    for _, row in items_df.iterrows():\n",
    "        if row['asin'] not in items_dict:\n",
    "            items_dict[row['asin']] = item_id\n",
    "            item_id += 1\n",
    "        pbar.update(1)\n",
    "data['itemID'] = data['asin'].apply(lambda x: items_dict[x])\n",
    "\n",
    "# Step 7: save the itemID dict (for UI)\n",
    "items_pkl_path = DATA_DIR + f'items_all_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "with open(items_pkl_path, 'wb') as handle:\n",
    "    pickle.dump(items_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "del items_dict\n",
    "data.drop(columns=['asin'], inplace=True) # we will henceforth use itemID\n",
    "\n",
    "tqdm.pandas()\n",
    "lst_main_cat = items_df.main_cat.unique()\n",
    "\n",
    "# Step 8: add genre; \n",
    "# currently, consider the main categories or if an item in category matches the main category list\n",
    "data['genre'] = ''\n",
    "\n",
    "def update_category(row):\n",
    "    categories = row['category']\n",
    "    new_cat = []\n",
    "    new_cat.append(row['main_cat'])\n",
    "    # print(categories)\n",
    "    for cat in categories.split('|'):\n",
    "        if cat.strip() != '' and (cat in lst_main_cat):\n",
    "            new_cat.append(cat)\n",
    "    row['genre'] = '|'.join(new_cat)\n",
    "    return row\n",
    "data = data.progress_apply(update_category, axis=1)\n",
    "\n",
    "# Step 9: save the prepared dataset to be used by wide_deep model\n",
    "data.drop(columns=['category'], inplace=True)\n",
    "data.sort_values('unixTimeStamp', inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data = data[['userID','itemID', 'rating','genre','unixTimeStamp','title','price','main_cat','category']]\n",
    "data.to_csv(DATA_DIR + f'wide_deep_amzn_all_{MIN_REVIEWS}.csv', \n",
    "             header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f03340",
   "metadata": {},
   "source": [
    "<a id=\"electronics\"></a>\n",
    "# Prepare Electronics, Home, Games datasets\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Filter the items dataframe to remove all items whose main_cat is not in the list of main_categories for each umbrella category (e.g. Electronics)\n",
    "2. Filter the rating dataframe to remove all reviews that do not belong to the list of item IDs left in the items dataframe after step 1.\n",
    "3. Convert hexadecimal reviewerID in the ratings dataframe to userID (0 based)\n",
    "4. Save the reviewerID dict in a pkl file\n",
    "5. Left merge ratings and items filtered dataframe on asin (hexadecimal)\n",
    "6. Convert hexadecimal asin to itemID (0 based)\n",
    "7. Save the itemID dict (for UI) in a pkl file\n",
    "8. Add genre; currently, consider the main categories or if an item in category matches the sub category list\n",
    "9. Save the prepared dataset for use by modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "857e83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/wide_deep/'\n",
    "\n",
    "def prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, dataset):\n",
    "    data_dir = DATA_DIR + dataset + '/'\n",
    "    \n",
    "    # Step 1: filter the items dataframe\n",
    "    items_filtered_df = items_df[items_df['main_cat'].isin(lst_main_cat)].copy()\n",
    "    items_filtered_ids = items_filtered_df['asin'].values\n",
    "\n",
    "    # Step 2: filter the ratings dataframe\n",
    "    ratings_filtered_df = ratings_df[ratings_df['asin'].isin(items_filtered_ids)].copy()\n",
    "    \n",
    "    print(dataset)\n",
    "    print(\"Num ratings:\", ratings_filtered_df.shape[0])\n",
    "    print(\"users\", ratings_filtered_df['reviewerID'].nunique(), \"items\", ratings_filtered_df['asin'].nunique())\n",
    "    \n",
    "    # Step 3: convert reviewerID to userID (0 based)\n",
    "    reviewers_dict = {}\n",
    "    reviewer_id = 0\n",
    "    for _, row in ratings_filtered_df.iterrows():\n",
    "        if row['reviewerID'] not in reviewers_dict:\n",
    "            reviewers_dict[row['reviewerID']] = reviewer_id\n",
    "            reviewer_id += 1\n",
    "            \n",
    "    ratings_filtered_df['userID'] = ratings_filtered_df['reviewerID'].apply(lambda x: reviewers_dict[x])\n",
    "\n",
    "    # Step 4: save the reviewerID dict (perhaps for UI)\n",
    "    users_pkl_path = data_dir + f'users_{dataset[0].lower()}_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "    with open(users_pkl_path, 'wb') as handle:\n",
    "        pickle.dump(reviewers_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    ratings_filtered_df.drop(columns=['reviewerID'], inplace=True) # we will henceforth use userID\n",
    "\n",
    "    # Step 5: left merge the ratings with items on asin\n",
    "    data = ratings_filtered_df.merge(items_filtered_df, on=['asin'], how='left')\n",
    "    print(\"merged dataframe\", data.shape)\n",
    "    data['category'] = data['category'].astype('string')\n",
    "    \n",
    "    # Step 6: convert asin to itemID (0 based)\n",
    "    items_dict = {}\n",
    "    item_id = 0\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        if row['asin'] not in items_dict:\n",
    "            items_dict[row['asin']] = item_id\n",
    "            item_id += 1\n",
    "    data['itemID'] = data['asin'].apply(lambda x: items_dict[x])\n",
    "    \n",
    "    # Step 7: save the itemID dict (for UI)\n",
    "    items_pkl_path = data_dir + f'items_{dataset[0].lower()}_{MIN_REVIEWS}.pkl'\n",
    "\n",
    "    with open(items_pkl_path, 'wb') as handle:\n",
    "        pickle.dump(items_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)        \n",
    "    data.drop(columns=['asin'], inplace=True) # we will henceforth use itemID\n",
    "    \n",
    "    # Step 8: add genre; \n",
    "    # currently, consider the main categories or if an item in category matches the sub category list\n",
    "    data['genre'] = ''\n",
    "\n",
    "    def update_category(row):\n",
    "        categories = row['category']\n",
    "        new_cat = []\n",
    "        new_cat.append(row['main_cat'])\n",
    "        # print(categories)\n",
    "        for cat in categories.split('|'):\n",
    "            if cat.strip() != '' and ((cat in lst_main_cat) or (cat in lst_sub_cat)):\n",
    "                new_cat.append(cat)\n",
    "        row['genre'] = '|'.join(new_cat)\n",
    "        return row\n",
    "    data = data.apply(update_category, axis=1)\n",
    "    \n",
    "    # Step 9: save the prepared dataset to be used by wide_deep model\n",
    "    # data.drop(columns=['category'], inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    data = data[['userID','itemID', 'rating','genre','unixTimeStamp','title','price','main_cat','category']]\n",
    "    data.to_csv(data_dir + f'wide_deep_amzn_{dataset[0].lower()}_{MIN_REVIEWS}.csv', \n",
    "                 header=False, index=False)\n",
    "    \n",
    "    del items_filtered_df\n",
    "    del ratings_filtered_df\n",
    "    del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4597070",
   "metadata": {},
   "source": [
    "### Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cefa68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n",
      "Num ratings: 5613183\n",
      "users 830668 items 63725\n",
      "merged dataframe (5613183, 8)\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Electronics'\n",
    "lst_main_cat = ['All Electronics', 'Amazon Devices', 'Apple Products', \n",
    "                 'Camera & Photo', 'Car Electronics', 'Cell Phones & Accessories', 'Computers',\n",
    "                 'Electronics', 'GPS & Navigation', 'Home Audio & Theater', 'Industrial & Scientific',\n",
    "                 'Portable Audio & Accessories']\n",
    "\n",
    "lst_sub_cat = ['Accessories','Computers & Accessories','Office Products','Video Games',\n",
    "               'Accessories & Supplies','Tools & Home Improvement','Computer Accessories & Peripherals',\n",
    "               'Audio & Video Accessories', 'Automotive', 'Office & School Supplies',\n",
    "               'Car & Vehicle Electronics', 'Industrial & Scientific','Sports & Outdoors','Office Electronics',\n",
    "               'Home & Kitchen','Musical Instruments','Portable Audio & Video','Electrical',\n",
    "               'Clothing, Shoes & Jewelry','Toys & Games','Laptop Accessories','Home Audio',\n",
    "               'Controllers','Computer Components','Sports & Fitness']\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, \"Electronics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c15c47",
   "metadata": {},
   "source": [
    "### Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca0f2129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home\n",
      "Num ratings: 7575829\n",
      "users 882963 items 92562\n",
      "merged dataframe (7575829, 8)\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Home'\n",
    "lst_main_cat = ['Amazon Home', 'Appliances', 'Home & Kitchen', \n",
    "                'Patio, Lawn & Garden', 'Tools & Home Improvement']\n",
    "\n",
    "lst_sub_cat = ['Kitchen & Dining', 'Industrial & Scientific', 'Power & Hand Tools',\n",
    "               'Automotive', 'Arts, Crafts & Sewing', 'Office Products',\n",
    "               'Electronics', 'Sports & Outdoors', 'Home Dcor', 'Accessories', 'Hand Tools',\n",
    "               'Office & School Supplies', 'Gardening & Lawn Care',\n",
    "               'Hardware', 'Storage & Organization', 'Lighting & Ceiling Fans',\n",
    "               'Kitchen Utensils & Gadgets', 'Electrical', 'Furniture', 'Pet Supplies',\n",
    "               'Building Supplies', 'Bedding', 'Sports & Fitness', 'Safety & Security',\n",
    "               'Outdoor Recreation', 'Power Tool Parts & Accessories', 'Kitchen & Bath Fixtures',\n",
    "               'Parts & Accessories', 'Small Appliances', 'Replacement Parts',\n",
    "               'Crafting', 'Sewing', 'Tools & Equipment', 'Outdoor Dcor', 'Patio Furniture & Accessories',\n",
    "               'Grills & Outdoor Cooking', 'Power Tools', 'Rough Plumbing', 'Bath',\n",
    "               'Bakeware', 'Accessories & Supplies', 'Heating, Cooling & Air Quality',\n",
    "               'Outdoor Power Tools', 'Outdoor Lighting', 'Paint, Wall Treatments & Supplies',\n",
    "               'Home Dcor Accents', 'Pools, Hot Tubs & Supplies', 'Bathroom Fixtures',\n",
    "               'Test, Measure & Inspect', 'Bathroom Accessories', 'Personal Protective Equipment',\n",
    "               'Fasteners', 'Vacuums & Floor Care', 'Clothing & Closet Storage', 'Exterior Accessories',\n",
    "               'Replacement Parts & Accessories', 'Desk Accessories & Workspace Organizers',\n",
    "               'Outdoor Cooking Tools & Accessories', 'HVAC']\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, \"Home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afde184",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f8e2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games\n",
      "Num ratings: 3735507\n",
      "users 748773 items 56219\n",
      "merged dataframe (3735507, 8)\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Games'\n",
    "lst_main_cat = ['Sports & Outdoors', 'Toys & Games', 'Video Games']\n",
    "lst_sub_cat = ['Sports & Fitness', 'Accessories', 'Clothing, Shoes & Jewelry', 'Home & Kitchen',\n",
    "               'Outdoor Recreation', 'Clothing', 'Electronics', 'Games', \n",
    "               'Sports & Outdoor Play', 'Cycling', 'Exercise & Fitness', 'Camping & Hiking',\n",
    "               'Leisure Sports & Game Room', 'Hunting & Fishing', 'Retro Gaming & Microconsoles',\n",
    "               'Costumes & Accessories', 'Dress Up & Pretend Play', 'Shoes', 'Golf', 'Hobbies',\n",
    "               'Water Sports', 'Controllers', 'Xbox One', 'PlayStation 3', 'Motorcycle & Powersports',\n",
    "               'Arts & Crafts', 'Replacement Parts', 'Xbox 360', 'Other Sports', 'Crafting',\n",
    "               'Wii', 'Learning & Education', 'Sports', 'Active', 'Consoles',\n",
    "               'Painting, Drawing & Art Supplies', 'Audio & Video Accessories', 'Nintendo 3DS & 2DS',\n",
    "               'Athletic', 'Skates, Skateboards & Scooters', 'Building Toys', 'Building Sets']\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, \"Games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e2f0e",
   "metadata": {},
   "source": [
    "# Code to discover relevant sub categories to build the lst_sub_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98301b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ratings_g_df = pd.read_csv(DATA_DIR + 'wide_deep_amzn_g_20.csv', header=None, low_memory=False)\n",
    "ratings_g_df.columns=['userID','itemID', 'rating','genre','unixTimeStamp','title','price','main_cat','category']\n",
    "ratings_g_df['category'].fillna('', inplace=True)\n",
    "ratings_g_df['price'].fillna('$$$', inplace=True)\n",
    "    \n",
    "lst = list(ratings_g_df['category'].unique())\n",
    "word_count = defaultdict(int)\n",
    "for l in lst:\n",
    "    for w in l.split('|'):\n",
    "        word_count[w] += 1\n",
    "dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579af3e1",
   "metadata": {},
   "source": [
    "# Additional datasets: Books, Health & Personal Care, Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d70047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_cat = 'Books'\n",
    "lst_main_cat = ['Books', 'Kindle', 'Audible audiobooks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e726516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_cat = 'Health & Personal Care'\n",
    "lst_main_cat = ['All Beauty', 'Health & Personal Care', 'Luxury Beauty']\n",
    "\n",
    "# main_cat = 'Fashion'\n",
    "lst_main_cat = ['Amazon Fashion', 'Clothing, Shoes & Jewelry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214a04a",
   "metadata": {},
   "source": [
    "# Prepare for SLiRec\n",
    "\n",
    "1. Restart the kernel\n",
    "2. Read the ratings and items dataframes\n",
    "3. Prepare Electronics, Home and Games datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e355aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "\n",
    "MIN_REVIEWS = 20\n",
    "\n",
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/wide_deep/'\n",
    "ratings_df = pd.read_csv(DATA_DIR + f'all_reviews_{MIN_REVIEWS}.csv', header=None)\n",
    "ratings_df.columns = ['reviewerID', 'asin', 'rating', 'unixTimeStamp']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f848697",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.read_csv(DATA_DIR + f'all_meta_{MIN_REVIEWS}.csv', header=None)\n",
    "items_df.columns=['asin','price','title','main_cat','category']\n",
    "items_df['category'].fillna('', inplace=True)\n",
    "items_df['price'].fillna('$$$', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d2018c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(items_df['main_cat'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a11440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/slirec/'\n",
    "TEST_NUM_NGS = 49\n",
    "VALID_NUM_NGS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b673df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def write_reviews(data_dir, ratings_filtered_df):\n",
    "    reviews_writefile = data_dir + '/reviews.output'\n",
    "    reviews_w = open(reviews_writefile, 'w')\n",
    "    for _, row in ratings_filtered_df.iterrows():\n",
    "        reviews_w.write(\n",
    "            row[\"reviewerID\"]\n",
    "            + \"\\t\"\n",
    "            + row[\"asin\"]\n",
    "            + \"\\t\"\n",
    "            + str(row[\"unixTimeStamp\"])\n",
    "            + \"\\n\"\n",
    "        )\n",
    "    reviews_w.close()\n",
    "    return reviews_writefile\n",
    "\n",
    "def write_meta(data_dir, items_df):\n",
    "    meta_writefile = data_dir + \"/meta.output\"\n",
    "    meta_w = open(meta_writefile, \"w\")\n",
    "\n",
    "    for _, row in items_df.iterrows():\n",
    "        meta_w.write(row[\"asin\"] + \"\\t\" + row[\"main_cat\"] + \"\\n\")\n",
    "    meta_w.close()\n",
    "    return meta_writefile\n",
    "\n",
    "def write_instance_output(reviews_file, meta_file):\n",
    "    # For every user, create a list of items reviews sorted by ascending timestamp\n",
    "    print(\"start create instances...\")\n",
    "    dirs, _ = os.path.split(reviews_file)\n",
    "    output_file = os.path.join(dirs, \"instance.output\")\n",
    "\n",
    "    f_reviews = open(reviews_file, \"r\")\n",
    "    user_dict = {}\n",
    "    item_list = []\n",
    "    \n",
    "    for line in f_reviews:\n",
    "        line = line.strip()\n",
    "        reviews_things = line.split(\"\\t\") # user_id asin unix_ts\n",
    "        if reviews_things[0] not in user_dict:\n",
    "            user_dict[reviews_things[0]] = []\n",
    "        user_dict[reviews_things[0]].append((line, float(reviews_things[-1]))) # note append whole line, ts\n",
    "        item_list.append(reviews_things[1])\n",
    "\n",
    "    f_meta = open(meta_file, \"r\")\n",
    "    meta_dict = {}\n",
    "    for line in f_meta:\n",
    "        line = line.strip()\n",
    "        meta_things = line.split(\"\\t\") # asin category\n",
    "        if meta_things[0] not in meta_dict:\n",
    "            meta_dict[meta_things[0]] = meta_things[1]\n",
    "\n",
    "    f_output = open(output_file, \"w\")\n",
    "    num_default_cat = 0\n",
    "    for user_id in user_dict:\n",
    "        sorted_user_behavior = sorted(user_dict[user_id], key=lambda x: x[1]) # x[1]: tuple (line, ts)\n",
    "        for line, _ in sorted_user_behavior:\n",
    "            user_things = line.split(\"\\t\") # user_id, asin, ts\n",
    "            asin = user_things[1]\n",
    "            if asin in meta_dict:\n",
    "                f_output.write(\"1\" + \"\\t\" + line + \"\\t\" + meta_dict[asin] + \"\\n\") # positive\n",
    "            else:\n",
    "                num_default_cat += 1\n",
    "                f_output.write(\"1\" + \"\\t\" + line + \"\\t\" + \"default_cat\" + \"\\n\") # positive\n",
    "\n",
    "    f_reviews.close()\n",
    "    f_meta.close()\n",
    "    f_output.close()\n",
    "    print(\"Num default categories:\", num_default_cat)\n",
    "    assert(num_default_cat == 0)\n",
    "    return output_file\n",
    "\n",
    "def write_preprocessed_output(sampled_instance_file):\n",
    "    print(\"start data processing...\")\n",
    "    dirs, _ = os.path.split(sampled_instance_file)\n",
    "    output_file = os.path.join(dirs, \"preprocessed.output\")\n",
    "\n",
    "    f_input = open(sampled_instance_file, \"r\")\n",
    "    f_output = open(output_file, \"w\")\n",
    "    user_count = {}\n",
    "\n",
    "    for line in f_input:\n",
    "        line = line.strip()\n",
    "        user = line.split(\"\\t\")[1] # [\"label\", \"user_id\", \"item_id\", \"timestamp\", \"cate_id\"]\n",
    "        if user not in user_count:\n",
    "            user_count[user] = 0\n",
    "        user_count[user] += 1\n",
    "\n",
    "    f_input.seek(0)\n",
    "    i = 0\n",
    "    last_user = None\n",
    "    for line in f_input:\n",
    "        line = line.strip()\n",
    "        user = line.split(\"\\t\")[1]\n",
    "        if user == last_user:\n",
    "            if i < user_count[user] - 2:\n",
    "                f_output.write(\"train\" + \"\\t\" + line + \"\\n\")\n",
    "            elif i < user_count[user] - 1:\n",
    "                f_output.write(\"valid\" + \"\\t\" + line + \"\\n\")\n",
    "            else:\n",
    "                f_output.write(\"test\" + \"\\t\" + line + \"\\n\")\n",
    "        else:\n",
    "            last_user = user\n",
    "            i = 0\n",
    "            if i < user_count[user] - 2:\n",
    "                f_output.write(\"train\" + \"\\t\" + line + \"\\n\")\n",
    "            elif i < user_count[user] - 1:\n",
    "                f_output.write(\"valid\" + \"\\t\" + line + \"\\n\")\n",
    "            else:\n",
    "                f_output.write(\"test\" + \"\\t\" + line + \"\\n\")\n",
    "        i += 1\n",
    "    return output_file\n",
    "\n",
    "def write_train_valid_test_data(data_dir, output_file):\n",
    "    train_file = os.path.join(data_dir, r'train_data')\n",
    "    valid_file = os.path.join(data_dir, r'valid_data')\n",
    "    test_file = os.path.join(data_dir, r'test_data')\n",
    "    \n",
    "    f_input = open(output_file, \"r\") # preprocessed.output\n",
    "    f_train = open(train_file, \"w\")\n",
    "    f_valid = open(valid_file, \"w\")\n",
    "    f_test = open(test_file, \"w\")\n",
    "    min_sequence = 1\n",
    "\n",
    "    print(\"train, valid, test positive data generating...\")\n",
    "    last_user_id = None\n",
    "    for line in f_input:\n",
    "        line_split = line.strip().split(\"\\t\")\n",
    "        tfile = line_split[0]        # train/valid/test\n",
    "        label = int(line_split[1])   # label 1\n",
    "        user_id = line_split[2]      # user\n",
    "        item_id = line_split[3]      # asin\n",
    "        date_time = line_split[4]    # ts\n",
    "        category = line_split[5]     # category\n",
    "\n",
    "        if tfile == \"train\":\n",
    "            fo = f_train\n",
    "        elif tfile == \"valid\":\n",
    "            fo = f_valid\n",
    "        elif tfile == \"test\":\n",
    "            fo = f_test\n",
    "        if user_id != last_user_id:\n",
    "            item_id_list = [] # collect all items\n",
    "            cate_list = []\n",
    "            dt_list = []\n",
    "        else:\n",
    "            history_clk_num = len(item_id_list)\n",
    "            cat_str = \"\"\n",
    "            iid_str = \"\"\n",
    "            dt_str = \"\"\n",
    "            for c1 in cate_list:\n",
    "                cat_str += c1 + \",\"\n",
    "            for iid in item_id_list:\n",
    "                iid_str += iid + \",\"\n",
    "            for dt_time in dt_list:\n",
    "                dt_str += dt_time + \",\"\n",
    "            if len(cat_str) > 0:\n",
    "                cat_str = cat_str[:-1]\n",
    "            if len(iid_str) > 0:\n",
    "                iid_str = iid_str[:-1]\n",
    "            if len(dt_str) > 0:\n",
    "                dt_str = dt_str[:-1]\n",
    "            if history_clk_num >= min_sequence:\n",
    "                fo.write(\n",
    "                    line_split[1]\n",
    "                    + \"\\t\"\n",
    "                    + user_id\n",
    "                    + \"\\t\"\n",
    "                    + item_id\n",
    "                    + \"\\t\"\n",
    "                    + category\n",
    "                    + \"\\t\"\n",
    "                    + date_time\n",
    "                    + \"\\t\"\n",
    "                    + iid_str\n",
    "                    + \"\\t\"\n",
    "                    + cat_str\n",
    "                    + \"\\t\"\n",
    "                    + dt_str\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "        last_user_id = user_id\n",
    "        if label:\n",
    "            item_id_list.append(item_id)\n",
    "            cate_list.append(category)\n",
    "            dt_list.append(date_time)\n",
    "    return train_file, valid_file, test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73cc68be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocab_pkl_files(data_dir, train_file):\n",
    "    user_vocab = os.path.join(data_dir, r'user_vocab.pkl')\n",
    "    item_vocab = os.path.join(data_dir, r'item_vocab.pkl')\n",
    "    cate_vocab = os.path.join(data_dir, r'category_vocab.pkl')\n",
    "\n",
    "    f_train = open(train_file, \"r\") # NOTE: only train_file\n",
    "\n",
    "    user_dict = {}\n",
    "    item_dict = {}\n",
    "    cat_dict = {}\n",
    "\n",
    "    print(\"vocab generating...\")\n",
    "    for line in f_train:\n",
    "        arr = line.strip(\"\\n\").split(\"\\t\") # label uid asin category ts asin_list, cat_list, ts_list\n",
    "        uid = arr[1]\n",
    "        iid = arr[2]\n",
    "        cat = arr[3]\n",
    "        iid_list = arr[5]\n",
    "        cat_list = arr[6]\n",
    "\n",
    "        if uid not in user_dict:\n",
    "            user_dict[uid] = 0\n",
    "        user_dict[uid] += 1\n",
    "        if iid not in item_dict:\n",
    "            item_dict[iid] = 0\n",
    "        item_dict[iid] += 1\n",
    "        if cat not in cat_dict:\n",
    "            cat_dict[cat] = 0\n",
    "        cat_dict[cat] += 1\n",
    "        if len(iid_list) == 0:\n",
    "            print(\"No history\", uid)\n",
    "            continue\n",
    "        for m in iid_list.split(\",\"):\n",
    "            if m not in item_dict:\n",
    "                item_dict[m] = 0\n",
    "            item_dict[m] += 1\n",
    "        for c in cat_list.split(\",\"):\n",
    "            if c not in cat_dict:\n",
    "                cat_dict[c] = 0\n",
    "            cat_dict[c] += 1\n",
    "\n",
    "    sorted_user_dict = sorted(user_dict.items(), key=lambda x: x[1], reverse=True) # sorted on popularity\n",
    "    sorted_item_dict = sorted(item_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_cat_dict = sorted(cat_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    uid_voc = {}\n",
    "    index = 0\n",
    "    for key, value in sorted_user_dict:\n",
    "        uid_voc[key] = index\n",
    "        index += 1\n",
    "\n",
    "    iid_voc = {}\n",
    "    iid_voc[\"default_mid\"] = 0\n",
    "    index = 1\n",
    "    for key, value in sorted_item_dict:\n",
    "        iid_voc[key] = index\n",
    "        index += 1\n",
    "\n",
    "    cat_voc = {}\n",
    "    cat_voc[\"default_cat\"] = 0\n",
    "    index = 1\n",
    "    for key, value in sorted_cat_dict:\n",
    "        cat_voc[key] = index\n",
    "        index += 1\n",
    "\n",
    "    cPickle.dump(uid_voc, open(user_vocab, \"wb\"))\n",
    "    cPickle.dump(iid_voc, open(item_vocab, \"wb\"))\n",
    "    cPickle.dump(cat_voc, open(cate_vocab, \"wb\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d425a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_neg_samples(sampled_instance_file, item2cate, valid_file, test_file):\n",
    "    columns = [\"label\", \"user_id\", \"item_id\", \"timestamp\", \"cate_id\"]\n",
    "    ns_df = pd.read_csv(sampled_instance_file, sep=\"\\t\", names=columns)\n",
    "    items_with_popular = list(ns_df[\"item_id\"])\n",
    "\n",
    "    # valid negative sampling\n",
    "    print(\"start valid negative sampling\")\n",
    "    with open(valid_file, \"r\") as f:\n",
    "        valid_lines = f.readlines()\n",
    "    write_valid = open(valid_file, \"w\")\n",
    "    for line in valid_lines:\n",
    "        write_valid.write(line)\n",
    "        words = line.strip().split(\"\\t\")\n",
    "        positive_item = words[2]\n",
    "        count = 0\n",
    "        neg_items = set()\n",
    "        while count < VALID_NUM_NGS:\n",
    "            neg_item = random.choice(items_with_popular)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "            count += 1\n",
    "            neg_items.add(neg_item)\n",
    "            words[0] = \"0\"\n",
    "            words[2] = neg_item\n",
    "            words[3] = item2cate[neg_item]\n",
    "            write_valid.write(\"\\t\".join(words) + \"\\n\")\n",
    "\n",
    "    # test negative sampling\n",
    "    print(\"start test negative sampling\")\n",
    "    with open(test_file, \"r\") as f:\n",
    "        test_lines = f.readlines()\n",
    "    write_test = open(test_file, \"w\")\n",
    "    for line in test_lines:\n",
    "        write_test.write(line)\n",
    "        words = line.strip().split(\"\\t\")\n",
    "        positive_item = words[2]\n",
    "        count = 0\n",
    "        neg_items = set()\n",
    "        while count < TEST_NUM_NGS:\n",
    "            neg_item = random.choice(items_with_popular)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "            count += 1\n",
    "            neg_items.add(neg_item)\n",
    "            words[0] = \"0\"\n",
    "            words[2] = neg_item\n",
    "            words[3] = item2cate[neg_item]\n",
    "            write_test.write(\"\\t\".join(words) + \"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5684599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, dataset):\n",
    "    data_dir = DATA_DIR + dataset + '/'\n",
    "    \n",
    "    # Step 1: filter the items dataframe\n",
    "    items_filtered_df = items_df[items_df['main_cat'].isin(lst_main_cat)].copy()\n",
    "    items_filtered_ids = items_filtered_df['asin'].values\n",
    "\n",
    "    # Step 2: filter the ratings dataframe\n",
    "    ratings_filtered_df = ratings_df[ratings_df['asin'].isin(items_filtered_ids)].copy()\n",
    "    \n",
    "    print(dataset)\n",
    "    print(\"Num ratings:\", ratings_filtered_df.shape[0])\n",
    "    print(\"users\", ratings_filtered_df['reviewerID'].nunique(), \"items\", ratings_filtered_df['asin'].nunique())\n",
    "    \n",
    "    # Step 3: write the reviews output file (reviews preprocessing)\n",
    "    # reviews_writefile = data_dir + 'reviews.output'\n",
    "    reviews_writefile = write_reviews(data_dir, ratings_filtered_df)\n",
    "    \n",
    "    # Step 4: write the meta output file (meta preprocessing)\n",
    "    # meta_writefile = data_dir + 'meta.output'\n",
    "    meta_writefile = write_meta(data_dir, items_df)\n",
    "    \n",
    "    # Step 5: create the instance.output\n",
    "    # instance_output = data_dir + 'instance.output'\n",
    "    instance_output = write_instance_output(reviews_writefile, meta_writefile)\n",
    "    \n",
    "    # Step 6: create item2category dictionary\n",
    "    instance_df = pd.read_csv(\n",
    "        instance_output,\n",
    "        sep=\"\\t\",\n",
    "        names=[\"label\", \"user_id\", \"item_id\", \"timestamp\", \"cate_id\"],\n",
    "    )\n",
    "    \n",
    "    # tmp_df = instance_df[[\"item_id\",\"cate_id\"]].drop_duplicates()\n",
    "    # print(tmp_df.shape)\n",
    "    item2cate = instance_df.set_index(\"item_id\")[\"cate_id\"].to_dict() # item_id/asin: category\n",
    "    print(instance_df.shape, instance_df['item_id'].nunique(), len(item2cate))\n",
    "    del instance_df\n",
    "    \n",
    "    # everything in the instance.output is either train, valid or test\n",
    "    sampled_instance_file = instance_output\n",
    "    \n",
    "    # Step 7: split into train (all records for a user except the last 2), \n",
    "    # validation (second last), test (last)\n",
    "    # output_file = data_dir + 'preprocessed.output'\n",
    "    output_file = write_preprocessed_output(sampled_instance_file)\n",
    "    train_file, valid_file, test_file = write_train_valid_test_data(data_dir, output_file)\n",
    "\n",
    "    # Step 8: generate vocabulary files        \n",
    "    gen_vocab_pkl_files(data_dir, train_file)\n",
    "    \n",
    "    # Step 9: negative sampling (validation, test only); train is done inline\n",
    "    gen_neg_samples(sampled_instance_file, item2cate, valid_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309922a",
   "metadata": {},
   "source": [
    "## Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd5b5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n",
      "Num ratings: 5613183\n",
      "users 830668 items 63725\n",
      "start create instances...\n",
      "Num default categories: 0\n",
      "(5613183, 5) 63725 63725\n",
      "start data processing...\n",
      "train, valid, test positive data generating...\n",
      "vocab generating...\n",
      "start valid negative sampling\n",
      "start test negative sampling\n"
     ]
    }
   ],
   "source": [
    "# main_cat = 'Electronics'\n",
    "lst_main_cat = ['All Electronics', 'Amazon Devices', 'Apple Products', \n",
    "                 'Camera & Photo', 'Car Electronics', 'Cell Phones & Accessories', 'Computers',\n",
    "                 'Electronics', 'GPS & Navigation', 'Home Audio & Theater', 'Industrial & Scientific',\n",
    "                 'Portable Audio & Accessories']\n",
    "\n",
    "lst_sub_cat = ['Accessories','Computers & Accessories','Office Products','Video Games',\n",
    "               'Accessories & Supplies','Tools & Home Improvement','Computer Accessories & Peripherals',\n",
    "               'Audio & Video Accessories', 'Automotive', 'Office & School Supplies',\n",
    "               'Car & Vehicle Electronics', 'Industrial & Scientific','Sports & Outdoors','Office Electronics',\n",
    "               'Home & Kitchen','Musical Instruments','Portable Audio & Video','Electrical',\n",
    "               'Clothing, Shoes & Jewelry','Toys & Games','Laptop Accessories','Home Audio',\n",
    "               'Controllers','Computer Components','Sports & Fitness']\n",
    "\n",
    "prepare_dataset(lst_main_cat, lst_sub_cat, items_df, ratings_df, 'Electronics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ac402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
