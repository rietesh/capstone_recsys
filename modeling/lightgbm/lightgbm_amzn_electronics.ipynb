{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n",
    "This notebook will give you an example of how to train a LightGBM model to estimate click-through rates on an e-commerce advertisement. We will train a LightGBM based model on the Criteo dataset.\n",
    "\n",
    "[LightGBM](https://github.com/Microsoft/LightGBM) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "* Fast training speed and high efficiency.\n",
    "* Low memory usage.\n",
    "* Great accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/shiv/Documents/DataScience/Kaggle/recommenders/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.12 (main, Apr  5 2022, 06:56:58) \n",
      "[GCC 7.5.0]\n",
      "LightGBM version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import category_encoders as ce\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import recommenders.models.lightgbm.lightgbm_utils as lgb_utils\n",
    "import recommenders.evaluation.python_evaluation as evaluator\n",
    "from recommenders.utils import plot\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"LightGBM version: {}\".format(lgb.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.utils.constants import (\n",
    "    DEFAULT_USER_COL as USER_COL,\n",
    "    DEFAULT_ITEM_COL as ITEM_COL,\n",
    "    DEFAULT_RATING_COL as RATING_COL,\n",
    "    DEFAULT_GENRE_COL as ITEM_FEAT_COL,\n",
    "    DEFAULT_PREDICTION_COL as PREDICT_COL,\n",
    "    DEFAULT_K,\n",
    "    DEFAULT_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting\n",
    "Let's set the main related parameters for LightGBM now. Basically, the task is regression (predicting rating of an item), so the objective function is set to loss and RMSE and MAE are used as metrics.\n",
    "\n",
    "Generally, we can adjust the number of leaves (MAX_LEAF), the minimum number of data in each leaf (MIN_DATA), maximum number of trees (NUM_OF_TREES), the learning rate of trees (TREE_LEARNING_RATE) and EARLY_STOPPING_ROUNDS (to avoid overfitting) in the model to get better performance.\n",
    "\n",
    "Besides, we can also adjust some other listed parameters to optimize the results. [In this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst), a list of all the parameters is shown. Also, some advice on how to tune these parameters can be found [in this url](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "MAX_LEAF = 64\n",
    "MIN_DATA = 20\n",
    "NUM_OF_TREES = 100\n",
    "TREE_LEARNING_RATE = 0.15\n",
    "EARLY_STOPPING_ROUNDS = 20\n",
    "METRIC = [\"rmse\", \"mae\"]\n",
    "# Generate negative samples for ndcg, hit\n",
    "NUM_NEG_SAMPLES = 50\n",
    "N_WORKERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 1,\n",
    "    'objective': \"regression\",\n",
    "    'metric': METRIC,\n",
    "    'num_leaves': MAX_LEAF,\n",
    "    'min_data': MIN_DATA,\n",
    "    'boost_from_average': True,\n",
    "    # set it according to your cpu cores.\n",
    "    'num_threads': 20,\n",
    "    'feature_fraction': 0.8,\n",
    "    'learning_rate': TREE_LEARNING_RATE,\n",
    "    'num_iterations': 750\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(DATA_DIR + 'wide_deep/Electronics/wide_deep_amzn_e_20.csv', \n",
    "                       header=None, low_memory=False)\n",
    "all_data.columns = [USER_COL,ITEM_COL,RATING_COL,ITEM_FEAT_COL,\n",
    "                    'unixTimeStamp','title','price','main_cat','category']\n",
    "all_data.sort_values('unixTimeStamp', inplace=True)\n",
    "all_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of numerical and categorical columns\n",
    "with open(DATA_DIR + 'xdeepfm/lst_genres.pkl', 'rb') as f:\n",
    "    lst_genres = pickle.load(f)\n",
    "\n",
    "cols = [USER_COL, ITEM_COL] # numerical column\n",
    "for genre in lst_genres:\n",
    "    cols.append(genre) # categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot use column names with spaces in lightGBM\n",
    "cols_dict = {}\n",
    "cate_cols = []\n",
    "cols_dict[USER_COL] = USER_COL\n",
    "cols_dict[ITEM_COL] = ITEM_COL\n",
    "rev_cols_dict = {}\n",
    "for idx, col in enumerate(cols[2:], start=1):\n",
    "    cols_dict[col] = 'C' + str(idx)\n",
    "    rev_cols_dict['C'+str(idx)] = col\n",
    "    cate_cols.append('C' + str(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation, and test\n",
    "\n",
    "<b>Need to be done once.</b> [Jump to loading data](#read_csv) if you have already saved the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we cut three sets (train_data (first 80%), valid_data (middle 10%) and test_data (last 10%)), cut from the original all data. <br>\n",
    "Notably, considering the reviews data is a kind of time-series streaming data, which is also very common in recommendation scenario, we split the data by its order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.python_splitters import python_chrono_split\n",
    "train, valid, test = python_chrono_split(all_data, [0.8,0.1,0.1], col_timestamp='unixTimeStamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[RATING_COL,USER_COL,ITEM_COL,ITEM_FEAT_COL]].copy()\n",
    "valid = valid[[RATING_COL,USER_COL,ITEM_COL,ITEM_FEAT_COL]].copy()\n",
    "test  = test[[RATING_COL,USER_COL,ITEM_COL,ITEM_FEAT_COL]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand genre into categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols[2:]:\n",
    "    train[cols_dict[col]] = np.nan\n",
    "    valid[cols_dict[col]] = np.nan\n",
    "    test[cols_dict[col]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4519730/4519730 [02:36<00:00, 28909.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 562550/562550 [00:20<00:00, 27328.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 530903/530903 [00:19<00:00, 26927.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def expand_genres(row):\n",
    "    genres = row[ITEM_FEAT_COL].split('|')\n",
    "    for genre in genres:\n",
    "        row[cols_dict[genre]] = 1\n",
    "    return row\n",
    "\n",
    "train = train.progress_apply(expand_genres, axis=1)\n",
    "valid = valid.progress_apply(expand_genres, axis=1)\n",
    "test  = test.progress_apply(expand_genres, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=[ITEM_FEAT_COL], inplace=True)\n",
    "valid.drop(columns=[ITEM_FEAT_COL], inplace=True)\n",
    "test.drop(columns=[ITEM_FEAT_COL], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5613183, 9), (4519730, 39), (562550, 39), (530903, 39))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 numeric features + 36 categorical features + label\n",
    "all_data.shape, train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_DIR + 'lightgbm/train_e.csv', header=False, index=False)\n",
    "valid.to_csv(DATA_DIR + 'lightgbm/valid_e.csv', header=False, index=False)\n",
    "test.to_csv(DATA_DIR + 'lightgbm/test_e.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='read_csv'></a>\n",
    "### Read the saved train, valid, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + 'lightgbm/train_e.csv', header=None)\n",
    "train.columns = [RATING_COL, USER_COL, ITEM_COL] + cate_cols\n",
    "valid = pd.read_csv(DATA_DIR + 'lightgbm/valid_e.csv', header=None)\n",
    "valid.columns = [RATING_COL, USER_COL, ITEM_COL] + cate_cols\n",
    "test  = pd.read_csv(DATA_DIR + 'lightgbm/test_e.csv', header=None)\n",
    "test.columns = [RATING_COL, USER_COL, ITEM_COL] + cate_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "### Ordinal Encoding\n",
    "Considering LightGBM could handle the low-frequency features and missing value by itself, for basic usage, we only encode the string-like categorical features by an ordinal encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n",
    "# print(ord_encoder.get_params())\n",
    "\n",
    "def encode_csv(df, encoder, label_col, typ='fit'):\n",
    "    if typ == 'fit':\n",
    "        df = encoder.fit_transform(df)\n",
    "    else:\n",
    "        df = encoder.transform(df)\n",
    "    y = df[label_col].values\n",
    "    del df[label_col]\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = encode_csv(train, ord_encoder, RATING_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: X: (4519730, 38); Y: (4519730,).\n",
      "Valid Data Shape: X: (562550, 38); Y: (562550,).\n",
      "Test Data Shape: X: (530903, 38); Y: (530903,).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>...</th>\n",
       "      <th>C27</th>\n",
       "      <th>C28</th>\n",
       "      <th>C29</th>\n",
       "      <th>C30</th>\n",
       "      <th>C31</th>\n",
       "      <th>C32</th>\n",
       "      <th>C33</th>\n",
       "      <th>C34</th>\n",
       "      <th>C35</th>\n",
       "      <th>C36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>9662</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9662</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29877</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  C1  C2  C3  C4  C5  C6  C7  C8  ...  C27  C28  C29  C30  \\\n",
       "0       0   19203   1   1   1   1   1   1   1   1  ...    1    1    1    1   \n",
       "1       0       0   1   1   2   1   1   1   1   1  ...    1    1    1    2   \n",
       "2       0    9662   1   2   2   1   1   2   1   1  ...    1    1    1    1   \n",
       "3       0    9662   1   2   2   1   1   2   1   1  ...    1    1    1    1   \n",
       "4       0   29877   1   1   2   1   1   1   1   1  ...    1    1    1    1   \n",
       "\n",
       "   C31  C32  C33  C34  C35  C36  \n",
       "0    1    1    1    1    1    1  \n",
       "1    1    1    1    1    1    1  \n",
       "2    1    1    1    1    1    1  \n",
       "3    1    1    1    1    1    1  \n",
       "4    1    1    1    1    1    1  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_x, train_y = encode_csv(train_data, ord_encoder, label_col)\n",
    "valid_x, valid_y = encode_csv(valid, ord_encoder, RATING_COL, 'transform')\n",
    "test_x, test_y = encode_csv(test, ord_encoder, RATING_COL, 'transform')\n",
    "\n",
    "print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "      .format(trn_x_shape=train_x.shape,\n",
    "              trn_y_shape=train_y.shape,\n",
    "              vld_x_shape=valid_x.shape,\n",
    "              vld_y_shape=valid_y.shape,\n",
    "              tst_x_shape=test_x.shape,\n",
    "              tst_y_shape=test_y.shape,))\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Jump to Model Serving](#model_serving) if we already have a pre-built model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "When both hyper-parameters and data are ready, we can create a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/engine.py:260: UserWarning: 'evals_result' argument is deprecated and will be removed in a future release of LightGBM. Pass 'record_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'evals_result' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 606\n",
      "[LightGBM] [Info] Number of data points in the train set: 4519730, number of used features: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 4.351199\n",
      "[1]\tvalid_0's rmse: 1.15986\tvalid_0's l1: 0.887851\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's rmse: 1.15886\tvalid_0's l1: 0.886246\n",
      "[3]\tvalid_0's rmse: 1.15814\tvalid_0's l1: 0.884963\n",
      "[4]\tvalid_0's rmse: 1.15755\tvalid_0's l1: 0.883858\n",
      "[5]\tvalid_0's rmse: 1.15719\tvalid_0's l1: 0.883011\n",
      "[6]\tvalid_0's rmse: 1.15682\tvalid_0's l1: 0.882098\n",
      "[7]\tvalid_0's rmse: 1.15652\tvalid_0's l1: 0.881315\n",
      "[8]\tvalid_0's rmse: 1.15636\tvalid_0's l1: 0.880764\n",
      "[9]\tvalid_0's rmse: 1.15622\tvalid_0's l1: 0.880328\n",
      "[10]\tvalid_0's rmse: 1.1561\tvalid_0's l1: 0.880035\n",
      "[11]\tvalid_0's rmse: 1.15597\tvalid_0's l1: 0.879673\n",
      "[12]\tvalid_0's rmse: 1.15588\tvalid_0's l1: 0.879377\n",
      "[13]\tvalid_0's rmse: 1.15582\tvalid_0's l1: 0.879108\n",
      "[14]\tvalid_0's rmse: 1.15574\tvalid_0's l1: 0.878856\n",
      "[15]\tvalid_0's rmse: 1.15564\tvalid_0's l1: 0.878531\n",
      "[16]\tvalid_0's rmse: 1.15555\tvalid_0's l1: 0.878255\n",
      "[17]\tvalid_0's rmse: 1.15544\tvalid_0's l1: 0.877939\n",
      "[18]\tvalid_0's rmse: 1.15535\tvalid_0's l1: 0.87774\n",
      "[19]\tvalid_0's rmse: 1.15527\tvalid_0's l1: 0.87747\n",
      "[20]\tvalid_0's rmse: 1.1552\tvalid_0's l1: 0.877321\n",
      "[21]\tvalid_0's rmse: 1.15515\tvalid_0's l1: 0.877258\n",
      "[22]\tvalid_0's rmse: 1.1551\tvalid_0's l1: 0.877132\n",
      "[23]\tvalid_0's rmse: 1.155\tvalid_0's l1: 0.876844\n",
      "[24]\tvalid_0's rmse: 1.15496\tvalid_0's l1: 0.876715\n",
      "[25]\tvalid_0's rmse: 1.15489\tvalid_0's l1: 0.876567\n",
      "[26]\tvalid_0's rmse: 1.15482\tvalid_0's l1: 0.876472\n",
      "[27]\tvalid_0's rmse: 1.15476\tvalid_0's l1: 0.876314\n",
      "[28]\tvalid_0's rmse: 1.15473\tvalid_0's l1: 0.87624\n",
      "[29]\tvalid_0's rmse: 1.15472\tvalid_0's l1: 0.876184\n",
      "[30]\tvalid_0's rmse: 1.15467\tvalid_0's l1: 0.876068\n",
      "[31]\tvalid_0's rmse: 1.1546\tvalid_0's l1: 0.875923\n",
      "[32]\tvalid_0's rmse: 1.15454\tvalid_0's l1: 0.875816\n",
      "[33]\tvalid_0's rmse: 1.1545\tvalid_0's l1: 0.875746\n",
      "[34]\tvalid_0's rmse: 1.15447\tvalid_0's l1: 0.8757\n",
      "[35]\tvalid_0's rmse: 1.15442\tvalid_0's l1: 0.875582\n",
      "[36]\tvalid_0's rmse: 1.15436\tvalid_0's l1: 0.875474\n",
      "[37]\tvalid_0's rmse: 1.15433\tvalid_0's l1: 0.875397\n",
      "[38]\tvalid_0's rmse: 1.15431\tvalid_0's l1: 0.875371\n",
      "[39]\tvalid_0's rmse: 1.15427\tvalid_0's l1: 0.87528\n",
      "[40]\tvalid_0's rmse: 1.15426\tvalid_0's l1: 0.875236\n",
      "[41]\tvalid_0's rmse: 1.15424\tvalid_0's l1: 0.875187\n",
      "[42]\tvalid_0's rmse: 1.15422\tvalid_0's l1: 0.875134\n",
      "[43]\tvalid_0's rmse: 1.15418\tvalid_0's l1: 0.875056\n",
      "[44]\tvalid_0's rmse: 1.15413\tvalid_0's l1: 0.874952\n",
      "[45]\tvalid_0's rmse: 1.15411\tvalid_0's l1: 0.874919\n",
      "[46]\tvalid_0's rmse: 1.15406\tvalid_0's l1: 0.874852\n",
      "[47]\tvalid_0's rmse: 1.15404\tvalid_0's l1: 0.874771\n",
      "[48]\tvalid_0's rmse: 1.15397\tvalid_0's l1: 0.874615\n",
      "[49]\tvalid_0's rmse: 1.15395\tvalid_0's l1: 0.874576\n",
      "[50]\tvalid_0's rmse: 1.15394\tvalid_0's l1: 0.874557\n",
      "[51]\tvalid_0's rmse: 1.15391\tvalid_0's l1: 0.874482\n",
      "[52]\tvalid_0's rmse: 1.15389\tvalid_0's l1: 0.874453\n",
      "[53]\tvalid_0's rmse: 1.15386\tvalid_0's l1: 0.874393\n",
      "[54]\tvalid_0's rmse: 1.15385\tvalid_0's l1: 0.874381\n",
      "[55]\tvalid_0's rmse: 1.15383\tvalid_0's l1: 0.874339\n",
      "[56]\tvalid_0's rmse: 1.15381\tvalid_0's l1: 0.87431\n",
      "[57]\tvalid_0's rmse: 1.15378\tvalid_0's l1: 0.874245\n",
      "[58]\tvalid_0's rmse: 1.15373\tvalid_0's l1: 0.874166\n",
      "[59]\tvalid_0's rmse: 1.15372\tvalid_0's l1: 0.874155\n",
      "[60]\tvalid_0's rmse: 1.15371\tvalid_0's l1: 0.874123\n",
      "[61]\tvalid_0's rmse: 1.15369\tvalid_0's l1: 0.874062\n",
      "[62]\tvalid_0's rmse: 1.15368\tvalid_0's l1: 0.874033\n",
      "[63]\tvalid_0's rmse: 1.15368\tvalid_0's l1: 0.87403\n",
      "[64]\tvalid_0's rmse: 1.15367\tvalid_0's l1: 0.873986\n",
      "[65]\tvalid_0's rmse: 1.15363\tvalid_0's l1: 0.873918\n",
      "[66]\tvalid_0's rmse: 1.15363\tvalid_0's l1: 0.873903\n",
      "[67]\tvalid_0's rmse: 1.15359\tvalid_0's l1: 0.873844\n",
      "[68]\tvalid_0's rmse: 1.15359\tvalid_0's l1: 0.873822\n",
      "[69]\tvalid_0's rmse: 1.15358\tvalid_0's l1: 0.873787\n",
      "[70]\tvalid_0's rmse: 1.15356\tvalid_0's l1: 0.873748\n",
      "[71]\tvalid_0's rmse: 1.15355\tvalid_0's l1: 0.873737\n",
      "[72]\tvalid_0's rmse: 1.15354\tvalid_0's l1: 0.873722\n",
      "[73]\tvalid_0's rmse: 1.15351\tvalid_0's l1: 0.873642\n",
      "[74]\tvalid_0's rmse: 1.15348\tvalid_0's l1: 0.873589\n",
      "[75]\tvalid_0's rmse: 1.15347\tvalid_0's l1: 0.873585\n",
      "[76]\tvalid_0's rmse: 1.15343\tvalid_0's l1: 0.873518\n",
      "[77]\tvalid_0's rmse: 1.15342\tvalid_0's l1: 0.873512\n",
      "[78]\tvalid_0's rmse: 1.15341\tvalid_0's l1: 0.873504\n",
      "[79]\tvalid_0's rmse: 1.15338\tvalid_0's l1: 0.873459\n",
      "[80]\tvalid_0's rmse: 1.15336\tvalid_0's l1: 0.873406\n",
      "[81]\tvalid_0's rmse: 1.15334\tvalid_0's l1: 0.873379\n",
      "[82]\tvalid_0's rmse: 1.15329\tvalid_0's l1: 0.873267\n",
      "[83]\tvalid_0's rmse: 1.15327\tvalid_0's l1: 0.873207\n",
      "[84]\tvalid_0's rmse: 1.15323\tvalid_0's l1: 0.873165\n",
      "[85]\tvalid_0's rmse: 1.15322\tvalid_0's l1: 0.873147\n",
      "[86]\tvalid_0's rmse: 1.15321\tvalid_0's l1: 0.873108\n",
      "[87]\tvalid_0's rmse: 1.1532\tvalid_0's l1: 0.873092\n",
      "[88]\tvalid_0's rmse: 1.15316\tvalid_0's l1: 0.873021\n",
      "[89]\tvalid_0's rmse: 1.15313\tvalid_0's l1: 0.872961\n",
      "[90]\tvalid_0's rmse: 1.15313\tvalid_0's l1: 0.872942\n",
      "[91]\tvalid_0's rmse: 1.15313\tvalid_0's l1: 0.872934\n",
      "[92]\tvalid_0's rmse: 1.1531\tvalid_0's l1: 0.872913\n",
      "[93]\tvalid_0's rmse: 1.15306\tvalid_0's l1: 0.872835\n",
      "[94]\tvalid_0's rmse: 1.15305\tvalid_0's l1: 0.8728\n",
      "[95]\tvalid_0's rmse: 1.15303\tvalid_0's l1: 0.872747\n",
      "[96]\tvalid_0's rmse: 1.15302\tvalid_0's l1: 0.872742\n",
      "[97]\tvalid_0's rmse: 1.15299\tvalid_0's l1: 0.872676\n",
      "[98]\tvalid_0's rmse: 1.15299\tvalid_0's l1: 0.872674\n",
      "[99]\tvalid_0's rmse: 1.15299\tvalid_0's l1: 0.872655\n",
      "[100]\tvalid_0's rmse: 1.15298\tvalid_0's l1: 0.872656\n",
      "[101]\tvalid_0's rmse: 1.15298\tvalid_0's l1: 0.872646\n",
      "[102]\tvalid_0's rmse: 1.15297\tvalid_0's l1: 0.872616\n",
      "[103]\tvalid_0's rmse: 1.15296\tvalid_0's l1: 0.872607\n",
      "[104]\tvalid_0's rmse: 1.15293\tvalid_0's l1: 0.87256\n",
      "[105]\tvalid_0's rmse: 1.15293\tvalid_0's l1: 0.872551\n",
      "[106]\tvalid_0's rmse: 1.15293\tvalid_0's l1: 0.87252\n",
      "[107]\tvalid_0's rmse: 1.15289\tvalid_0's l1: 0.87247\n",
      "[108]\tvalid_0's rmse: 1.15286\tvalid_0's l1: 0.872403\n",
      "[109]\tvalid_0's rmse: 1.15286\tvalid_0's l1: 0.872394\n",
      "[110]\tvalid_0's rmse: 1.15285\tvalid_0's l1: 0.872381\n",
      "[111]\tvalid_0's rmse: 1.15285\tvalid_0's l1: 0.872374\n",
      "[112]\tvalid_0's rmse: 1.15284\tvalid_0's l1: 0.872349\n",
      "[113]\tvalid_0's rmse: 1.15283\tvalid_0's l1: 0.872325\n",
      "[114]\tvalid_0's rmse: 1.15281\tvalid_0's l1: 0.872284\n",
      "[115]\tvalid_0's rmse: 1.15279\tvalid_0's l1: 0.872248\n",
      "[116]\tvalid_0's rmse: 1.15279\tvalid_0's l1: 0.872245\n",
      "[117]\tvalid_0's rmse: 1.15276\tvalid_0's l1: 0.872206\n",
      "[118]\tvalid_0's rmse: 1.15276\tvalid_0's l1: 0.872191\n",
      "[119]\tvalid_0's rmse: 1.15276\tvalid_0's l1: 0.872192\n",
      "[120]\tvalid_0's rmse: 1.15275\tvalid_0's l1: 0.872173\n",
      "[121]\tvalid_0's rmse: 1.15273\tvalid_0's l1: 0.872134\n",
      "[122]\tvalid_0's rmse: 1.1527\tvalid_0's l1: 0.872082\n",
      "[123]\tvalid_0's rmse: 1.1527\tvalid_0's l1: 0.872075\n",
      "[124]\tvalid_0's rmse: 1.15269\tvalid_0's l1: 0.872045\n",
      "[125]\tvalid_0's rmse: 1.15267\tvalid_0's l1: 0.872022\n",
      "[126]\tvalid_0's rmse: 1.15266\tvalid_0's l1: 0.872009\n",
      "[127]\tvalid_0's rmse: 1.15266\tvalid_0's l1: 0.871982\n",
      "[128]\tvalid_0's rmse: 1.15265\tvalid_0's l1: 0.871964\n",
      "[129]\tvalid_0's rmse: 1.15264\tvalid_0's l1: 0.871942\n",
      "[130]\tvalid_0's rmse: 1.15264\tvalid_0's l1: 0.871936\n",
      "[131]\tvalid_0's rmse: 1.15264\tvalid_0's l1: 0.871926\n",
      "[132]\tvalid_0's rmse: 1.15263\tvalid_0's l1: 0.871921\n",
      "[133]\tvalid_0's rmse: 1.15263\tvalid_0's l1: 0.87192\n",
      "[134]\tvalid_0's rmse: 1.15262\tvalid_0's l1: 0.871885\n",
      "[135]\tvalid_0's rmse: 1.15261\tvalid_0's l1: 0.871858\n",
      "[136]\tvalid_0's rmse: 1.15261\tvalid_0's l1: 0.871851\n",
      "[137]\tvalid_0's rmse: 1.15259\tvalid_0's l1: 0.871804\n",
      "[138]\tvalid_0's rmse: 1.15258\tvalid_0's l1: 0.87178\n",
      "[139]\tvalid_0's rmse: 1.15256\tvalid_0's l1: 0.871736\n",
      "[140]\tvalid_0's rmse: 1.15254\tvalid_0's l1: 0.871703\n",
      "[141]\tvalid_0's rmse: 1.15254\tvalid_0's l1: 0.8717\n",
      "[142]\tvalid_0's rmse: 1.15253\tvalid_0's l1: 0.871688\n",
      "[143]\tvalid_0's rmse: 1.15251\tvalid_0's l1: 0.871664\n",
      "[144]\tvalid_0's rmse: 1.15251\tvalid_0's l1: 0.871665\n",
      "[145]\tvalid_0's rmse: 1.15249\tvalid_0's l1: 0.871621\n",
      "[146]\tvalid_0's rmse: 1.15248\tvalid_0's l1: 0.871583\n",
      "[147]\tvalid_0's rmse: 1.15248\tvalid_0's l1: 0.871583\n",
      "[148]\tvalid_0's rmse: 1.15247\tvalid_0's l1: 0.871567\n",
      "[149]\tvalid_0's rmse: 1.15246\tvalid_0's l1: 0.871553\n",
      "[150]\tvalid_0's rmse: 1.15246\tvalid_0's l1: 0.871545\n",
      "[151]\tvalid_0's rmse: 1.15245\tvalid_0's l1: 0.871536\n",
      "[152]\tvalid_0's rmse: 1.15244\tvalid_0's l1: 0.871533\n",
      "[153]\tvalid_0's rmse: 1.15244\tvalid_0's l1: 0.87152\n",
      "[154]\tvalid_0's rmse: 1.15244\tvalid_0's l1: 0.871504\n",
      "[155]\tvalid_0's rmse: 1.15243\tvalid_0's l1: 0.8715\n",
      "[156]\tvalid_0's rmse: 1.15242\tvalid_0's l1: 0.871477\n",
      "[157]\tvalid_0's rmse: 1.15242\tvalid_0's l1: 0.871465\n",
      "[158]\tvalid_0's rmse: 1.15242\tvalid_0's l1: 0.871465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159]\tvalid_0's rmse: 1.15239\tvalid_0's l1: 0.871405\n",
      "[160]\tvalid_0's rmse: 1.15238\tvalid_0's l1: 0.871372\n",
      "[161]\tvalid_0's rmse: 1.15237\tvalid_0's l1: 0.871355\n",
      "[162]\tvalid_0's rmse: 1.15237\tvalid_0's l1: 0.871341\n",
      "[163]\tvalid_0's rmse: 1.15236\tvalid_0's l1: 0.871331\n",
      "[164]\tvalid_0's rmse: 1.15235\tvalid_0's l1: 0.871316\n",
      "[165]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871312\n",
      "[166]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871307\n",
      "[167]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871307\n",
      "[168]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871301\n",
      "[169]\tvalid_0's rmse: 1.15233\tvalid_0's l1: 0.871304\n",
      "[170]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871274\n",
      "[171]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871266\n",
      "[172]\tvalid_0's rmse: 1.15231\tvalid_0's l1: 0.871242\n",
      "[173]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871242\n",
      "[174]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871243\n",
      "[175]\tvalid_0's rmse: 1.1523\tvalid_0's l1: 0.871225\n",
      "[176]\tvalid_0's rmse: 1.15229\tvalid_0's l1: 0.871199\n",
      "[177]\tvalid_0's rmse: 1.15229\tvalid_0's l1: 0.871191\n",
      "[178]\tvalid_0's rmse: 1.15228\tvalid_0's l1: 0.871167\n",
      "[179]\tvalid_0's rmse: 1.15227\tvalid_0's l1: 0.871159\n",
      "[180]\tvalid_0's rmse: 1.15227\tvalid_0's l1: 0.871155\n",
      "[181]\tvalid_0's rmse: 1.15226\tvalid_0's l1: 0.871135\n",
      "[182]\tvalid_0's rmse: 1.15226\tvalid_0's l1: 0.871087\n",
      "[183]\tvalid_0's rmse: 1.15225\tvalid_0's l1: 0.871074\n",
      "[184]\tvalid_0's rmse: 1.15223\tvalid_0's l1: 0.871047\n",
      "[185]\tvalid_0's rmse: 1.15223\tvalid_0's l1: 0.87103\n",
      "[186]\tvalid_0's rmse: 1.15223\tvalid_0's l1: 0.871029\n",
      "[187]\tvalid_0's rmse: 1.15222\tvalid_0's l1: 0.871017\n",
      "[188]\tvalid_0's rmse: 1.15222\tvalid_0's l1: 0.871017\n",
      "[189]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.871004\n",
      "[190]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.870996\n",
      "[191]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.870999\n",
      "[192]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.870995\n",
      "[193]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870947\n",
      "[194]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870948\n",
      "[195]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870941\n",
      "[196]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870939\n",
      "[197]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870934\n",
      "[198]\tvalid_0's rmse: 1.15218\tvalid_0's l1: 0.870916\n",
      "[199]\tvalid_0's rmse: 1.15218\tvalid_0's l1: 0.8709\n",
      "[200]\tvalid_0's rmse: 1.15216\tvalid_0's l1: 0.870867\n",
      "[201]\tvalid_0's rmse: 1.15215\tvalid_0's l1: 0.870824\n",
      "[202]\tvalid_0's rmse: 1.15215\tvalid_0's l1: 0.870811\n",
      "[203]\tvalid_0's rmse: 1.15214\tvalid_0's l1: 0.870795\n",
      "[204]\tvalid_0's rmse: 1.15213\tvalid_0's l1: 0.870773\n",
      "[205]\tvalid_0's rmse: 1.15213\tvalid_0's l1: 0.870765\n",
      "[206]\tvalid_0's rmse: 1.15212\tvalid_0's l1: 0.870752\n",
      "[207]\tvalid_0's rmse: 1.1521\tvalid_0's l1: 0.870725\n",
      "[208]\tvalid_0's rmse: 1.1521\tvalid_0's l1: 0.870713\n",
      "[209]\tvalid_0's rmse: 1.15209\tvalid_0's l1: 0.870695\n",
      "[210]\tvalid_0's rmse: 1.15209\tvalid_0's l1: 0.870687\n",
      "[211]\tvalid_0's rmse: 1.15208\tvalid_0's l1: 0.870676\n",
      "[212]\tvalid_0's rmse: 1.15208\tvalid_0's l1: 0.870673\n",
      "[213]\tvalid_0's rmse: 1.15208\tvalid_0's l1: 0.870675\n",
      "[214]\tvalid_0's rmse: 1.15206\tvalid_0's l1: 0.87062\n",
      "[215]\tvalid_0's rmse: 1.15206\tvalid_0's l1: 0.87061\n",
      "[216]\tvalid_0's rmse: 1.15204\tvalid_0's l1: 0.870574\n",
      "[217]\tvalid_0's rmse: 1.15203\tvalid_0's l1: 0.870552\n",
      "[218]\tvalid_0's rmse: 1.15201\tvalid_0's l1: 0.870513\n",
      "[219]\tvalid_0's rmse: 1.15199\tvalid_0's l1: 0.870489\n",
      "[220]\tvalid_0's rmse: 1.15199\tvalid_0's l1: 0.870487\n",
      "[221]\tvalid_0's rmse: 1.15198\tvalid_0's l1: 0.870465\n",
      "[222]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870452\n",
      "[223]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870434\n",
      "[224]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870414\n",
      "[225]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870414\n",
      "[226]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870412\n",
      "[227]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.8704\n",
      "[228]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.870392\n",
      "[229]\tvalid_0's rmse: 1.15195\tvalid_0's l1: 0.870383\n",
      "[230]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.87038\n",
      "[231]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.870373\n",
      "[232]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.870368\n",
      "[233]\tvalid_0's rmse: 1.15195\tvalid_0's l1: 0.870341\n",
      "[234]\tvalid_0's rmse: 1.15194\tvalid_0's l1: 0.870328\n",
      "[235]\tvalid_0's rmse: 1.15194\tvalid_0's l1: 0.870324\n",
      "[236]\tvalid_0's rmse: 1.15194\tvalid_0's l1: 0.870321\n",
      "[237]\tvalid_0's rmse: 1.15193\tvalid_0's l1: 0.870318\n",
      "[238]\tvalid_0's rmse: 1.15192\tvalid_0's l1: 0.870304\n",
      "[239]\tvalid_0's rmse: 1.15193\tvalid_0's l1: 0.870303\n",
      "[240]\tvalid_0's rmse: 1.15189\tvalid_0's l1: 0.870242\n",
      "[241]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870234\n",
      "[242]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870222\n",
      "[243]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.87022\n",
      "[244]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870218\n",
      "[245]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870218\n",
      "[246]\tvalid_0's rmse: 1.15187\tvalid_0's l1: 0.870211\n",
      "[247]\tvalid_0's rmse: 1.15187\tvalid_0's l1: 0.870194\n",
      "[248]\tvalid_0's rmse: 1.15186\tvalid_0's l1: 0.870192\n",
      "[249]\tvalid_0's rmse: 1.15187\tvalid_0's l1: 0.870196\n",
      "[250]\tvalid_0's rmse: 1.15186\tvalid_0's l1: 0.870187\n",
      "[251]\tvalid_0's rmse: 1.15186\tvalid_0's l1: 0.870188\n",
      "[252]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.870144\n",
      "[253]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.870141\n",
      "[254]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.870136\n",
      "[255]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.870128\n",
      "[256]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.87013\n",
      "[257]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.87012\n",
      "[258]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.870102\n",
      "[259]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.8701\n",
      "[260]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.870092\n",
      "[261]\tvalid_0's rmse: 1.15182\tvalid_0's l1: 0.87007\n",
      "[262]\tvalid_0's rmse: 1.15181\tvalid_0's l1: 0.87005\n",
      "[263]\tvalid_0's rmse: 1.15181\tvalid_0's l1: 0.870035\n",
      "[264]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870032\n",
      "[265]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870026\n",
      "[266]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870026\n",
      "[267]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870027\n",
      "[268]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.87002\n",
      "[269]\tvalid_0's rmse: 1.15179\tvalid_0's l1: 0.869991\n",
      "[270]\tvalid_0's rmse: 1.15179\tvalid_0's l1: 0.86999\n",
      "[271]\tvalid_0's rmse: 1.15179\tvalid_0's l1: 0.869984\n",
      "[272]\tvalid_0's rmse: 1.15178\tvalid_0's l1: 0.869972\n",
      "[273]\tvalid_0's rmse: 1.15178\tvalid_0's l1: 0.869965\n",
      "[274]\tvalid_0's rmse: 1.15177\tvalid_0's l1: 0.869925\n",
      "[275]\tvalid_0's rmse: 1.15176\tvalid_0's l1: 0.869915\n",
      "[276]\tvalid_0's rmse: 1.15175\tvalid_0's l1: 0.869908\n",
      "[277]\tvalid_0's rmse: 1.15174\tvalid_0's l1: 0.869891\n",
      "[278]\tvalid_0's rmse: 1.15174\tvalid_0's l1: 0.869884\n",
      "[279]\tvalid_0's rmse: 1.15173\tvalid_0's l1: 0.869882\n",
      "[280]\tvalid_0's rmse: 1.15173\tvalid_0's l1: 0.869879\n",
      "[281]\tvalid_0's rmse: 1.15173\tvalid_0's l1: 0.869875\n",
      "[282]\tvalid_0's rmse: 1.15172\tvalid_0's l1: 0.86986\n",
      "[283]\tvalid_0's rmse: 1.15172\tvalid_0's l1: 0.869851\n",
      "[284]\tvalid_0's rmse: 1.15171\tvalid_0's l1: 0.869845\n",
      "[285]\tvalid_0's rmse: 1.15171\tvalid_0's l1: 0.869841\n",
      "[286]\tvalid_0's rmse: 1.15171\tvalid_0's l1: 0.869829\n",
      "[287]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869807\n",
      "[288]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869788\n",
      "[289]\tvalid_0's rmse: 1.15169\tvalid_0's l1: 0.869783\n",
      "[290]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869785\n",
      "[291]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869787\n",
      "[292]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869781\n",
      "[293]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869777\n",
      "[294]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869774\n",
      "[295]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869774\n",
      "[296]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869764\n",
      "[297]\tvalid_0's rmse: 1.15169\tvalid_0's l1: 0.869754\n",
      "[298]\tvalid_0's rmse: 1.15169\tvalid_0's l1: 0.869749\n",
      "[299]\tvalid_0's rmse: 1.15168\tvalid_0's l1: 0.869737\n",
      "[300]\tvalid_0's rmse: 1.15168\tvalid_0's l1: 0.86973\n",
      "[301]\tvalid_0's rmse: 1.15167\tvalid_0's l1: 0.869719\n",
      "[302]\tvalid_0's rmse: 1.15167\tvalid_0's l1: 0.869723\n",
      "[303]\tvalid_0's rmse: 1.15167\tvalid_0's l1: 0.869716\n",
      "[304]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869688\n",
      "[305]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869687\n",
      "[306]\tvalid_0's rmse: 1.15166\tvalid_0's l1: 0.86969\n",
      "[307]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869683\n",
      "[308]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869675\n",
      "[309]\tvalid_0's rmse: 1.15166\tvalid_0's l1: 0.869674\n",
      "[310]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.86967\n",
      "[311]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869669\n",
      "[312]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.86966\n",
      "[313]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869644\n",
      "[314]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869642\n",
      "[315]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[316]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869636\n",
      "[317]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869629\n",
      "[318]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869629\n",
      "[319]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869627\n",
      "[320]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869625\n",
      "[321]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869626\n",
      "[322]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869618\n",
      "[323]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869617\n",
      "[324]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869614\n",
      "[325]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869598\n",
      "[326]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869587\n",
      "[327]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869574\n",
      "[328]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869554\n",
      "[329]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869545\n",
      "[330]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869543\n",
      "[331]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869536\n",
      "[332]\tvalid_0's rmse: 1.15162\tvalid_0's l1: 0.86951\n",
      "[333]\tvalid_0's rmse: 1.15162\tvalid_0's l1: 0.869506\n",
      "[334]\tvalid_0's rmse: 1.15162\tvalid_0's l1: 0.869505\n",
      "[335]\tvalid_0's rmse: 1.15159\tvalid_0's l1: 0.869469\n",
      "[336]\tvalid_0's rmse: 1.15159\tvalid_0's l1: 0.869469\n",
      "[337]\tvalid_0's rmse: 1.15158\tvalid_0's l1: 0.869451\n",
      "[338]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869417\n",
      "[339]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869415\n",
      "[340]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869415\n",
      "[341]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869414\n",
      "[342]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869414\n",
      "[343]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869402\n",
      "[344]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869391\n",
      "[345]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869387\n",
      "[346]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869377\n",
      "[347]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869369\n",
      "[348]\tvalid_0's rmse: 1.15155\tvalid_0's l1: 0.869366\n",
      "[349]\tvalid_0's rmse: 1.15155\tvalid_0's l1: 0.869365\n",
      "[350]\tvalid_0's rmse: 1.15155\tvalid_0's l1: 0.869358\n",
      "[351]\tvalid_0's rmse: 1.15154\tvalid_0's l1: 0.869344\n",
      "[352]\tvalid_0's rmse: 1.15154\tvalid_0's l1: 0.869344\n",
      "[353]\tvalid_0's rmse: 1.15153\tvalid_0's l1: 0.869331\n",
      "[354]\tvalid_0's rmse: 1.15153\tvalid_0's l1: 0.869328\n",
      "[355]\tvalid_0's rmse: 1.15153\tvalid_0's l1: 0.869316\n",
      "[356]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869309\n",
      "[357]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869308\n",
      "[358]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.86931\n",
      "[359]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869309\n",
      "[360]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869311\n",
      "[361]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869313\n",
      "[362]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869301\n",
      "[363]\tvalid_0's rmse: 1.15151\tvalid_0's l1: 0.869297\n",
      "[364]\tvalid_0's rmse: 1.15151\tvalid_0's l1: 0.869293\n",
      "[365]\tvalid_0's rmse: 1.15151\tvalid_0's l1: 0.86929\n",
      "[366]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869267\n",
      "[367]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869268\n",
      "[368]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869258\n",
      "[369]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869262\n",
      "[370]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869259\n",
      "[371]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869252\n",
      "[372]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869249\n",
      "[373]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869248\n",
      "[374]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869245\n",
      "[375]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869242\n",
      "[376]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.86924\n",
      "[377]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869232\n",
      "[378]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869202\n",
      "[379]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869199\n",
      "[380]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869195\n",
      "[381]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869192\n",
      "[382]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869184\n",
      "[383]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869176\n",
      "[384]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869177\n",
      "[385]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869176\n",
      "[386]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869172\n",
      "[387]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869167\n",
      "[388]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869165\n",
      "[389]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869163\n",
      "[390]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.86916\n",
      "[391]\tvalid_0's rmse: 1.15145\tvalid_0's l1: 0.86915\n",
      "[392]\tvalid_0's rmse: 1.15145\tvalid_0's l1: 0.869132\n",
      "[393]\tvalid_0's rmse: 1.15145\tvalid_0's l1: 0.869128\n",
      "[394]\tvalid_0's rmse: 1.15144\tvalid_0's l1: 0.869093\n",
      "[395]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869073\n",
      "[396]\tvalid_0's rmse: 1.15143\tvalid_0's l1: 0.869075\n",
      "[397]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869072\n",
      "[398]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869071\n",
      "[399]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869059\n",
      "[400]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869046\n",
      "[401]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869046\n",
      "[402]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869036\n",
      "[403]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869037\n",
      "[404]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869035\n",
      "[405]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869034\n",
      "[406]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869034\n",
      "[407]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869034\n",
      "[408]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.86903\n",
      "[409]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869032\n",
      "[410]\tvalid_0's rmse: 1.1514\tvalid_0's l1: 0.869019\n",
      "[411]\tvalid_0's rmse: 1.1514\tvalid_0's l1: 0.869014\n",
      "[412]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.868995\n",
      "[413]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.86898\n",
      "[414]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.868976\n",
      "[415]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.868976\n",
      "[416]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868957\n",
      "[417]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868953\n",
      "[418]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868949\n",
      "[419]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868949\n",
      "[420]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868944\n",
      "[421]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868916\n",
      "[422]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868913\n",
      "[423]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868911\n",
      "[424]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868893\n",
      "[425]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868876\n",
      "[426]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868875\n",
      "[427]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868874\n",
      "[428]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868872\n",
      "[429]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868875\n",
      "[430]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868866\n",
      "[431]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868864\n",
      "[432]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868859\n",
      "[433]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868859\n",
      "[434]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868847\n",
      "[435]\tvalid_0's rmse: 1.15134\tvalid_0's l1: 0.868821\n",
      "[436]\tvalid_0's rmse: 1.15132\tvalid_0's l1: 0.868794\n",
      "[437]\tvalid_0's rmse: 1.15131\tvalid_0's l1: 0.868777\n",
      "[438]\tvalid_0's rmse: 1.1513\tvalid_0's l1: 0.868752\n",
      "[439]\tvalid_0's rmse: 1.1513\tvalid_0's l1: 0.868751\n",
      "[440]\tvalid_0's rmse: 1.15131\tvalid_0's l1: 0.868754\n",
      "[441]\tvalid_0's rmse: 1.15131\tvalid_0's l1: 0.868752\n",
      "[442]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868728\n",
      "[443]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868726\n",
      "[444]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868727\n",
      "[445]\tvalid_0's rmse: 1.15128\tvalid_0's l1: 0.868712\n",
      "[446]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868707\n",
      "[447]\tvalid_0's rmse: 1.15128\tvalid_0's l1: 0.868704\n",
      "[448]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868686\n",
      "[449]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868677\n",
      "[450]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868669\n",
      "[451]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868661\n",
      "[452]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868659\n",
      "[453]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868657\n",
      "[454]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868654\n",
      "[455]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868656\n",
      "[456]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868641\n",
      "[457]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.86864\n",
      "[458]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868641\n",
      "[459]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868638\n",
      "[460]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868638\n",
      "[461]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868639\n",
      "[462]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868637\n",
      "[463]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868632\n",
      "[464]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868624\n",
      "[465]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868622\n",
      "[466]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868619\n",
      "[467]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868621\n",
      "[468]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.86862\n",
      "[469]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868614\n",
      "[470]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868609\n",
      "[471]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868607\n",
      "[472]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[473]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868606\n",
      "[474]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868592\n",
      "[475]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868586\n",
      "[476]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868582\n",
      "[477]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868574\n",
      "[478]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868574\n",
      "[479]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868573\n",
      "[480]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868573\n",
      "[481]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868574\n",
      "[482]\tvalid_0's rmse: 1.15123\tvalid_0's l1: 0.868562\n",
      "[483]\tvalid_0's rmse: 1.15123\tvalid_0's l1: 0.868551\n",
      "[484]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868543\n",
      "[485]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868542\n",
      "[486]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868535\n",
      "[487]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868535\n",
      "[488]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868535\n",
      "[489]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868517\n",
      "[490]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868517\n",
      "[491]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868492\n",
      "[492]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868491\n",
      "[493]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868492\n",
      "[494]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868491\n",
      "[495]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868481\n",
      "[496]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868474\n",
      "[497]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868474\n",
      "[498]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868476\n",
      "[499]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868467\n",
      "[500]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868469\n",
      "[501]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868468\n",
      "[502]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868468\n",
      "[503]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868467\n",
      "[504]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868464\n",
      "[505]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868464\n",
      "[506]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868469\n",
      "[507]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868446\n",
      "[508]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868447\n",
      "[509]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.86844\n",
      "[510]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868416\n",
      "[511]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868411\n",
      "[512]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868413\n",
      "[513]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868409\n",
      "[514]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868408\n",
      "[515]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868403\n",
      "[516]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868397\n",
      "[517]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868394\n",
      "[518]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868387\n",
      "[519]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868375\n",
      "[520]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.86837\n",
      "[521]\tvalid_0's rmse: 1.15118\tvalid_0's l1: 0.868371\n",
      "[522]\tvalid_0's rmse: 1.15118\tvalid_0's l1: 0.868353\n",
      "[523]\tvalid_0's rmse: 1.15118\tvalid_0's l1: 0.868353\n",
      "[524]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868351\n",
      "[525]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868344\n",
      "[526]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868342\n",
      "[527]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.86834\n",
      "[528]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868337\n",
      "[529]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868336\n",
      "[530]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868332\n",
      "[531]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868331\n",
      "[532]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868326\n",
      "[533]\tvalid_0's rmse: 1.15116\tvalid_0's l1: 0.868324\n",
      "[534]\tvalid_0's rmse: 1.15116\tvalid_0's l1: 0.868319\n",
      "[535]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868303\n",
      "[536]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868303\n",
      "[537]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868297\n",
      "[538]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868298\n",
      "[539]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868299\n",
      "[540]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868298\n",
      "[541]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.86829\n",
      "[542]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868286\n",
      "[543]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868285\n",
      "[544]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868276\n",
      "[545]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868274\n",
      "[546]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868268\n",
      "[547]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868264\n",
      "[548]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868265\n",
      "[549]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868259\n",
      "[550]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868256\n",
      "[551]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868255\n",
      "[552]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868254\n",
      "[553]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868253\n",
      "[554]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868253\n",
      "[555]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868252\n",
      "[556]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868249\n",
      "[557]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868249\n",
      "[558]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868246\n",
      "[559]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868244\n",
      "[560]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868243\n",
      "[561]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868246\n",
      "[562]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868248\n",
      "[563]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868242\n",
      "[564]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868238\n",
      "[565]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868234\n",
      "[566]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868233\n",
      "[567]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868204\n",
      "[568]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868204\n",
      "[569]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868203\n",
      "[570]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868202\n",
      "[571]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868185\n",
      "[572]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868183\n",
      "[573]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.86818\n",
      "[574]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868173\n",
      "[575]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868166\n",
      "[576]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868164\n",
      "[577]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868163\n",
      "[578]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868162\n",
      "[579]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.86816\n",
      "[580]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868159\n",
      "[581]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868159\n",
      "[582]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.86816\n",
      "[583]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868156\n",
      "[584]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868154\n",
      "[585]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868156\n",
      "[586]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868151\n",
      "[587]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868149\n",
      "[588]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868143\n",
      "[589]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868145\n",
      "[590]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868144\n",
      "[591]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868144\n",
      "[592]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868143\n",
      "[593]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868141\n",
      "[594]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868138\n",
      "[595]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868136\n",
      "[596]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868136\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868164\n"
     ]
    }
   ],
   "source": [
    "# https://lightgbm.readthedocs.io/en/v3.3.2/Python-API.html#training-api\n",
    "    \n",
    "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params, categorical_feature=cate_cols)\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "lgb_test = lgb.Dataset(test_x, test_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "eval_result = {}\n",
    "\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=NUM_OF_TREES,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      evals_result= eval_result,\n",
    "                      valid_sets=lgb_valid,\n",
    "                      categorical_feature=cate_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from recommenders.utils import plot\n",
    "v = eval_result['valid_0']['rmse']\n",
    "x = range(1, len(v)+1)\n",
    "plot.line_graph(\n",
    "    values=list(zip(v, x)),\n",
    "    labels='rmse',\n",
    "    x_name=\"steps\",\n",
    "    y_name='rmse',\n",
    "#     subplot=(math.ceil(len(logs)/2), 2, i),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_name, feat_imp = (lgb_model.feature_name(), lgb_model.feature_importance())\n",
    "feat_imp, feat_name = zip(*sorted(zip(feat_imp, feat_name), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['userID',\n",
       "  'itemID',\n",
       "  'All Electronics',\n",
       "  'Cell Phones & Accessories',\n",
       "  'Computers',\n",
       "  'Home Audio & Theater',\n",
       "  'Audio & Video Accessories',\n",
       "  'Accessories',\n",
       "  'Camera & Photo',\n",
       "  'Computer Accessories & Peripherals'],\n",
       " (11379, 9974, 1412, 1167, 1115, 1011, 914, 905, 806, 637))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_name = [n if not n.startswith('C') else rev_cols_dict[n] for n in feat_name]\n",
    "feat_name[:10], feat_imp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='Feature importance', ylabel='Features'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb.plot_importance(lgb_model, max_num_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.453011282813894, 'mae': 0.9014153288417129}\n"
     ]
    }
   ],
   "source": [
    "test_preds = lgb_model.predict(test_x)\n",
    "\n",
    "rmse = mean_squared_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "mae = mean_absolute_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "res_basic = {\"rmse\": rmse, \"mae\": mae}\n",
    "print(res_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fc6660b9d90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIR = '/home/shiv/Documents/DataScience/Capstone/Modeling/lightGBM/model/'\n",
    "save_file = os.path.join(MODEL_DIR, r'finished.model')\n",
    "lgb_model.save_model(save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model_serving\"></a>\n",
    "### Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '/home/shiv/Documents/DataScience/Capstone/Modeling/lightGBM/model/'\n",
    "save_file = os.path.join(MODEL_DIR, r'finished.model')\n",
    "\n",
    "loaded_model = lgb.Booster(model_file=save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating metrics (rmse, mae) on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.453011282813894, 'mae': 0.9014153288417129}\n"
     ]
    }
   ],
   "source": [
    "# eval the performance again\n",
    "test_preds = lgb_model.predict(test_x)\n",
    "\n",
    "rmse = mean_squared_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "mae = mean_absolute_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "res_basic = {\"rmse\": rmse, \"mae\": mae}\n",
    "print(res_basic) # Note that the results match from earlier cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for ranking metrics for users from the test file\n",
    "\n",
    "<b>Only need to run once</b>\n",
    "([Jump to calculating ndcg](#calc_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TST_DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/lightgbm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grp = train[[USER_COL,ITEM_COL]].groupby(USER_COL).agg(list)\n",
    "items_df = train.drop_duplicates([ITEM_COL]).copy()\n",
    "items_df.drop(columns=[USER_COL, RATING_COL], inplace=True)\n",
    "items_df.set_index(ITEM_COL, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!$@%#!$@%#!$@%#!$@%#!$@%#!$@%#!$@%#!$@%#!$@%#!$@%#!$@%"
     ]
    }
   ],
   "source": [
    "processors = []\n",
    "items_set = set(train[ITEM_COL].unique())\n",
    "\n",
    "def sample_function(train_grp, test, items_df, cate_cols, i, num_workers, seed, data_dir):\n",
    "    nrows = test.shape[0]\n",
    "    each = nrows // num_workers\n",
    "    start = i*each\n",
    "    end = nrows if i == num_workers - 1 else (i+1)*each\n",
    "    sym = ['!','@','#','$','%']\n",
    "\n",
    "    # print(i, start, end, test.iloc[start:end].shape)\n",
    "    random.seed(seed)\n",
    "    tst_w_neg_samples_path = data_dir + f'lightgbm/amzn_e_tst_w_neg{i}.txt'\n",
    "    \n",
    "    user_col = []\n",
    "    item_col = []\n",
    "    rating_col = []\n",
    "    feat_cols = []\n",
    "    for c in cate_cols:\n",
    "        feat_cols.append([])\n",
    "    \n",
    "    for j, row in test.iloc[start:end].iterrows():\n",
    "        u = row[USER_COL]\n",
    "        positive_item = row[ITEM_COL]\n",
    "        tmp_df = train_grp.loc[u]\n",
    "        assert(tmp_df.shape[0] != 0)\n",
    "        \n",
    "        items_seen_set = set(tmp_df[ITEM_COL])\n",
    "        items_not_seen = list(items_set - items_seen_set)\n",
    "        user_col.append(int(u))\n",
    "        item_col.append(int(positive_item))\n",
    "        rating_col.append(row[RATING_COL])\n",
    "        for k, c in enumerate(cate_cols):\n",
    "            feat_cols[k].append(float(row[c]))\n",
    "\n",
    "        cnt = 0\n",
    "        neg_items = set()\n",
    "        while cnt < NUM_NEG_SAMPLES:\n",
    "            neg_item = random.choice(items_not_seen)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "\n",
    "            cnt += 1\n",
    "            tmp_df = items_df.loc[neg_item]\n",
    "            assert(tmp_df.shape[0] != 0)\n",
    "            \n",
    "            user_col.append(int(u))\n",
    "            item_col.append(int(neg_item))\n",
    "            rating_col.append(5.0) # unused\n",
    "            for k, c in enumerate(cate_cols):\n",
    "                feat_cols[k].append(float(tmp_df[c]))\n",
    "        if j % 10_000 == 0:\n",
    "            print(sym[i], end='')\n",
    "            \n",
    "    test_dict = {USER_COL: user_col, ITEM_COL: item_col, RATING_COL: rating_col}\n",
    "    for k, c in enumerate(cate_cols):\n",
    "        test_dict[c] = feat_cols[k]\n",
    "    X_test = pd.DataFrame(test_dict)\n",
    "    X_test.to_csv(tst_w_neg_samples_path, header=False, index=False)\n",
    "\n",
    "for i in range(N_WORKERS):\n",
    "    processors.append(\n",
    "        Process(\n",
    "            target = sample_function,\n",
    "            args = (train_grp, test, items_df, cate_cols, i, N_WORKERS, RANDOM_SEED, DATA_DIR)\n",
    "        ))\n",
    "    # processors[-1].daemon = True\n",
    "    processors[-1].start()\n",
    "\n",
    "for i in range(N_WORKERS):\n",
    "    processors[i].join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"calc_metrics\"></a>\n",
    "# Ranking metrics (ndcg@k, hit@k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read amzn_e_tst_w_neg0.txt\n",
      "Predicting... 5415180\n",
      "Read amzn_e_tst_w_neg1.txt\n",
      "Predicting... 5415180\n",
      "Read amzn_e_tst_w_neg2.txt\n",
      "Predicting... 5415180\n",
      "Read amzn_e_tst_w_neg3.txt\n",
      "Predicting... 5415180\n",
      "Read amzn_e_tst_w_neg4.txt\n",
      "Predicting... 5415333\n"
     ]
    }
   ],
   "source": [
    "NUM_NEG_SAMPLES = 50\n",
    "N_WORKERS = 5\n",
    "\n",
    "group_preds = []\n",
    "group = NUM_NEG_SAMPLES + 1\n",
    "\n",
    "for i in range(N_WORKERS):\n",
    "    tst_w_neg_samples_path = DATA_DIR + f'lightgbm/amzn_e_tst_w_neg{i}.txt'\n",
    "    X_test = pd.read_csv(tst_w_neg_samples_path, header=None)\n",
    "    X_test.columns=[USER_COL,ITEM_COL,RATING_COL] + cate_cols\n",
    "    X_test.drop(columns=[RATING_COL], inplace=True)\n",
    "    print('Read', f'amzn_e_tst_w_neg{i}.txt')\n",
    "\n",
    "    print('Predicting...', X_test.shape[0])\n",
    "    # Rating prediction\n",
    "    step_pred = lgb_model.predict(X_test)\n",
    "    group_preds.extend(np.reshape(step_pred, (-1, group)))\n",
    "    \n",
    "    del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0724577113002917, 0.1631578649960539)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NDCG = 0.0\n",
    "HT = 0.0\n",
    "for each_preds in group_preds:\n",
    "    predictions = np.array(each_preds)\n",
    "    predictions = -1.0 * predictions\n",
    "\n",
    "    # print(predictions)\n",
    "    # print(predictions.argsort())    \n",
    "    rank = predictions.argsort().argsort()[0]\n",
    "    if rank < 10:\n",
    "        NDCG += 1 / np.log2(rank + 2)\n",
    "        HT += 1\n",
    "NDCG/len(group_preds),HT/len(group_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K for a user across all products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users 830668\n",
      "# items 63725\n"
     ]
    }
   ],
   "source": [
    "ratings_df = pd.read_csv(DATA_DIR + 'wide_deep/Electronics/wide_deep_amzn_e_20.csv',\n",
    "                         header=None, low_memory=False)\n",
    "ratings_df.columns=['userID','itemID', 'rating','genre','unixTimeStamp','title','price','main_cat', 'category']\n",
    "ratings_df = ratings_df[[USER_COL, ITEM_COL, ITEM_FEAT_COL, 'title']]\n",
    "\n",
    "users = ratings_df.drop_duplicates(USER_COL)[[USER_COL]].reset_index(drop=True)\n",
    "print(\"# users\", users.shape[0])\n",
    "\n",
    "items = ratings_df.drop_duplicates(ITEM_COL)\n",
    "print(\"# items\", items.shape[0])\n",
    "\n",
    "user = 8009\n",
    "\n",
    "all_items_set = set()\n",
    "rev_item_lookup = {} # dictionary for reverse lookup of title and genre\n",
    "for _, row in items.iterrows():\n",
    "    i = row[ITEM_COL]\n",
    "    rev_item_lookup[i] = {'title': row['title'], 'genre': row[ITEM_FEAT_COL]}\n",
    "    all_items_set.add(i)\n",
    "    \n",
    "\n",
    "tmp_df = train[train[USER_COL]==user]\n",
    "items_rated_set = set(tmp_df[ITEM_COL].values)\n",
    "\n",
    "# Ignore items already seen (makes sense for movies but Amazon products???\n",
    "items_not_seen = all_items_set - items_rated_set\n",
    "lst_items_not_seen = list(items_not_seen)\n",
    "\n",
    "items_df = train.drop_duplicates([ITEM_COL]).copy()\n",
    "items_df.drop(columns=[USER_COL, RATING_COL], inplace=True)\n",
    "items_df.set_index(ITEM_COL, inplace=True) # for fast lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************"
     ]
    }
   ],
   "source": [
    "it_cnt = 0\n",
    "user_col = []\n",
    "item_col = []\n",
    "feat_cols = []\n",
    "for c in cate_cols:\n",
    "    feat_cols.append([])\n",
    "\n",
    "for it in lst_items_not_seen:\n",
    "\n",
    "    tmp_df = items_df.loc[it]\n",
    "    assert(tmp_df.shape[0] != 0)\n",
    "    \n",
    "    user_col.append(int(user))\n",
    "    item_col.append(int(it))\n",
    "    for k, c in enumerate(cate_cols):\n",
    "        feat_cols[k].append(float(tmp_df[c]))\n",
    "    it_cnt += 1\n",
    "    if it_cnt % 1000 == 0:\n",
    "        print('*', end='')\n",
    "        \n",
    "test_dict = {USER_COL: user_col, ITEM_COL: item_col}\n",
    "for k, c in enumerate(cate_cols):\n",
    "    test_dict[c] = feat_cols[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting... 63478\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.DataFrame(test_dict)\n",
    "\n",
    "print('Predicting...', X_test.shape[0])\n",
    "# Rating prediction\n",
    "step_pred = lgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.316779</td>\n",
       "      <td>Amazon Kindle 2 (2nd Generation) USB Car Charg...</td>\n",
       "      <td>Amazon Devices|Cell Phones &amp; Accessories|Acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.294511</td>\n",
       "      <td>Zeikos ZE-SG26R 3 Pieces Ultra Clear Deluxe Sc...</td>\n",
       "      <td>Camera &amp; Photo|Cell Phones &amp; Accessories|Acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.285888</td>\n",
       "      <td>ANSMANN Individual cell battery charger Energy...</td>\n",
       "      <td>Home Audio &amp; Theater|Cell Phones &amp; Accessories...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.276212</td>\n",
       "      <td>CAVN 2-Pack Compatible Fitbit Surge Charger, R...</td>\n",
       "      <td>Cell Phones &amp; Accessories|Sports &amp; Outdoors|Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.276212</td>\n",
       "      <td>Getwow 10-Pack Silicon Fastener Ring for Fitbi...</td>\n",
       "      <td>Cell Phones &amp; Accessories|Sports &amp; Outdoors|Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.276212</td>\n",
       "      <td>[Upgrade] Bike Phone Mount, Tryone Bike Mount ...</td>\n",
       "      <td>Cell Phones &amp; Accessories|Sports &amp; Outdoors|Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.276212</td>\n",
       "      <td>HanTop Apple Watch Band 38mm Stainless Steel A...</td>\n",
       "      <td>Cell Phones &amp; Accessories|Sports &amp; Outdoors|Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.276212</td>\n",
       "      <td>bayite Accessory Silicone Watch Band for Fitbi...</td>\n",
       "      <td>Cell Phones &amp; Accessories|Sports &amp; Outdoors|Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.276212</td>\n",
       "      <td>TUSITA Fitbit One Charging Cable, Replacement ...</td>\n",
       "      <td>Cell Phones &amp; Accessories|Sports &amp; Outdoors|Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.276212</td>\n",
       "      <td>Garmin vívokeeper Wrist Band, 9-Pack</td>\n",
       "      <td>Cell Phones &amp; Accessories|Sports &amp; Outdoors|Sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction                                              title  \\\n",
       "0    5.316779  Amazon Kindle 2 (2nd Generation) USB Car Charg...   \n",
       "1    5.294511  Zeikos ZE-SG26R 3 Pieces Ultra Clear Deluxe Sc...   \n",
       "2    5.285888  ANSMANN Individual cell battery charger Energy...   \n",
       "3    5.276212  CAVN 2-Pack Compatible Fitbit Surge Charger, R...   \n",
       "4    5.276212  Getwow 10-Pack Silicon Fastener Ring for Fitbi...   \n",
       "5    5.276212  [Upgrade] Bike Phone Mount, Tryone Bike Mount ...   \n",
       "6    5.276212  HanTop Apple Watch Band 38mm Stainless Steel A...   \n",
       "7    5.276212  bayite Accessory Silicone Watch Band for Fitbi...   \n",
       "8    5.276212  TUSITA Fitbit One Charging Cable, Replacement ...   \n",
       "9    5.276212               Garmin vívokeeper Wrist Band, 9-Pack   \n",
       "\n",
       "                                               genre  \n",
       "0  Amazon Devices|Cell Phones & Accessories|Acces...  \n",
       "1  Camera & Photo|Cell Phones & Accessories|Acces...  \n",
       "2  Home Audio & Theater|Cell Phones & Accessories...  \n",
       "3  Cell Phones & Accessories|Sports & Outdoors|Sp...  \n",
       "4  Cell Phones & Accessories|Sports & Outdoors|Sp...  \n",
       "5  Cell Phones & Accessories|Sports & Outdoors|Sp...  \n",
       "6  Cell Phones & Accessories|Sports & Outdoors|Sp...  \n",
       "7  Cell Phones & Accessories|Sports & Outdoors|Sp...  \n",
       "8  Cell Phones & Accessories|Sports & Outdoors|Sp...  \n",
       "9  Cell Phones & Accessories|Sports & Outdoors|Sp...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['prediction'] = step_pred\n",
    "X_test['title'] = X_test[ITEM_COL].map(lambda x: rev_item_lookup[x]['title'])\n",
    "X_test[ITEM_FEAT_COL] = X_test[ITEM_COL].map(lambda x: rev_item_lookup[x][ITEM_FEAT_COL])\n",
    "X_test.drop(columns = [USER_COL, ITEM_COL] + cate_cols, inplace=True)\n",
    "X_test.sort_values('prediction', ascending=False, inplace=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "X_test.to_csv(DATA_DIR + f'lightgbm/output_{user}.csv', index=False) # header = True\n",
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "## Optimized Usage\n",
    "### Label-encoding and Binary-encoding\n",
    "Next, since LightGBM has a better capability in handling dense numerical features effectively, we try to convert all the categorical features in original data into numerical ones, by label-encoding [3] and binary-encoding [4].<br>\n",
    "Also due to the sequence property of review dataset, the label-encoding we adopted is executed one-by-one, which means we encode the samples in order, by the information of the previous samples before each sample (sequential label-encoding and sequential count-encoding). Besides, we also filter the low-frequency categorical features and fill the missing values by the mean of corresponding columns for the numerical features. (consulting `lgb_utils.NumEncoder`)\n",
    "\n",
    "Specifically, in `lgb_utils.NumEncoder`, the main steps are as follows.\n",
    "* <s>Firstly, we convert the low-frequency categorical features to `\"LESS\"` and the missing categorical features to `\"UNK\"`.</s>\n",
    "* <s>Secondly, we convert the missing numerical features into the mean of corresponding columns.</s>\n",
    "* Thirdly, the string-like categorical features are ordinal encoded like the example shown in basic usage. \n",
    "* And then, we target encode the categorical features in the samples order one-by-one. For each sample, we add the label and count information of its former samples into the data and produce new features. Formally, for $i=1,2,...,n$, we add $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c) \\cdot y}{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}$ as a new label feature for current sample $x_i$, where $c$ is a category to encode in current sample, so $(i-1)$ is the number of former samples, and $I(\\cdot)$ is the indicator function that check the former samples contain $c$ (whether $x_j=c$) or not. At the meantime, we also add the count frequency of $c$, which is $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}{i-1}$, as a new count feature. \n",
    "* Finally, based on the results of ordinal encoding, we add the binary encoding results as new columns into the data.<BR>\n",
    "\n",
    "<B>Note that the statistics used in the above process only updates when fitting the training set</B>, while maintaining static when transforming the testing set because the label of test data should be considered as unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nume_cols = [USER_COL, ITEM_COL]\n",
    "label_col = RATING_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in cate_cols:\n",
    "#     value_counts = train[item].value_counts()\n",
    "#     print(item, value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The string-like categorical features are ordinal encoded as shown in basic usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic target encoding\n",
    "\n",
    "* And then, we target encode the categorical features in the samples order one-by-one. For each sample, we add the label and count information of its former samples into the data and produce new features. Formally, for $i=1,2,...,n$, we add $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c) \\cdot y}{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}$ as a new label feature for current sample $x_i$, where $c$ is a category to encode in current sample, so $(i-1)$ is the number of former samples, and $I(\\cdot)$ is the indicator function that check the former samples contain $c$ (whether $x_j=c$) or not.<br>\n",
    "At the same time, we also add the count frequency of $c$, which is $\\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}{i-1}$, as a new count feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 1, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unpackbits(x, num_bits):\n",
    "    \"\"\"Convert a decimal value numpy.ndarray into multi-binary value numpy.ndarray ([1,2]->[[1,0],[0,1]])\n",
    "    \"\"\"\n",
    "    xshape = list(x.shape) # 1D array, xshape = rows\n",
    "    x = x.reshape([-1, 1]) # 2D array, rowsx1\n",
    "    to_and = 2 ** np.arange(num_bits).reshape([1, num_bits]) # [[1, 2, 4, 8]]\n",
    "    # print(xshape + [num_bits]) # [rows, 4]\n",
    "    return (x & to_and).astype(bool).astype(int).reshape(xshape + [num_bits])\n",
    "\n",
    "unpackbits(np.array([1,2,3]), 4) # example of one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "\n",
    "class NumEncoder(object):\n",
    "    \"\"\"Encode all the categorical features into numerical ones by \n",
    "    sequential label encoding, sequential count encoding, and binary encoding. \n",
    "    Additionally, it also filters the low-frequency categories and fills the missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cate_cols, nume_cols, label_col, threshold=10, thresrate=0.99):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "            cate_cols (list): The columns of categorical features.\n",
    "            nume_cols (list): The columns of numerical features.\n",
    "            label_col (object): The column of Label.\n",
    "            threshold (int): The categories whose frequency is lower than the threshold will be \n",
    "                filtered (be treated as \"<LESS>\").\n",
    "            thresrate (float): The (1.0 - thersrate, default 1%) lowest-frequency categories will \n",
    "                also be filtered.\n",
    "        \"\"\"\n",
    "        self.label_name = label_col\n",
    "        self.cate_cols = cate_cols\n",
    "        self.dtype_dict = {}\n",
    "        for item in cate_cols:\n",
    "            self.dtype_dict[item] = \"str\"\n",
    "        for item in nume_cols:\n",
    "            self.dtype_dict[item] = \"float\"\n",
    "        self.nume_cols = nume_cols\n",
    "        self.tgt_nume_cols = []\n",
    "        self.encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n",
    "        self.threshold = threshold\n",
    "        self.thresrate = thresrate\n",
    "\n",
    "        self.save_cate_avgs = {}\n",
    "        self.save_value_filter = {}\n",
    "        self.save_num_embs = {}\n",
    "        self.Max_len = {}\n",
    "        self.samples = 0\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        df = df.astype(dtype=self.dtype_dict)\n",
    "        self.samples = df.shape[0]\n",
    "#         print(\"Filtering and fillna features\", self.samples, df.shape)\n",
    "#         for item in tqdm(self.cate_cols):\n",
    "#             value_counts = df[item].value_counts()\n",
    "            \n",
    "#             print(item)\n",
    "#             print(value_counts.shape)\n",
    "#             num = value_counts.shape[0]\n",
    "            \n",
    "#             self.save_value_filter[item] = list(\n",
    "#                 value_counts[: int(num * self.thresrate)][value_counts > self.threshold].index\n",
    "#             )\n",
    "            \n",
    "#             rm_values = set(value_counts.index) - set(self.save_value_filter[item])\n",
    "#             df[item] = df[item].map(lambda x: \"<LESS>\" if x in rm_values else x)\n",
    "#             df[item] = df[item].fillna(\"<UNK>\")\n",
    "            \n",
    "#             del value_counts\n",
    "#             gc.collect()\n",
    "\n",
    "#         for item in tqdm(self.nume_cols):\n",
    "#             df[item] = df[item].fillna(df[item].mean())\n",
    "#             self.save_num_embs[item] = {\"sum\": df[item].sum(), \"cnt\": df[item].shape[0]}\n",
    "\n",
    "        print(\"Ordinal encoding cate features\")\n",
    "        # ordinal_encoding\n",
    "        df = self.encoder.fit_transform(df)\n",
    "\n",
    "        print(\"Target encoding cate features\")\n",
    "        # dynamic_targeting_encoding\n",
    "        for item in tqdm(self.cate_cols):\n",
    "            feats = df[item].values\n",
    "            labels = df[self.label_name].values\n",
    "            feat_encoding = {\"mean\": [], \"count\": []}\n",
    "            self.save_cate_avgs[item] = collections.defaultdict(lambda: [0, 0])\n",
    "            for idx in range(self.samples):\n",
    "                cur_feat = feats[idx]\n",
    "                if cur_feat in self.save_cate_avgs[item]:\n",
    "                    feat_encoding[\"mean\"].append(\n",
    "                        self.save_cate_avgs[item][cur_feat][0]\n",
    "                        / self.save_cate_avgs[item][cur_feat][1]\n",
    "                    )\n",
    "                    feat_encoding[\"count\"].append(\n",
    "                        self.save_cate_avgs[item][cur_feat][1] / idx\n",
    "                    )\n",
    "                else:\n",
    "                    feat_encoding[\"mean\"].append(0)\n",
    "                    feat_encoding[\"count\"].append(0)\n",
    "                self.save_cate_avgs[item][cur_feat][0] += labels[idx]\n",
    "                self.save_cate_avgs[item][cur_feat][1] += 1\n",
    "            df[item + \"_t_mean\"] = feat_encoding[\"mean\"]\n",
    "            df[item + \"_t_count\"] = feat_encoding[\"count\"]\n",
    "            self.tgt_nume_cols.append(item + \"_t_mean\")\n",
    "            self.tgt_nume_cols.append(item + \"_t_count\")\n",
    "\n",
    "        print(\"Start manual binary encoding\")\n",
    "        rows = None\n",
    "        for item in tqdm(self.nume_cols + self.tgt_nume_cols):\n",
    "            feats = df[item].values\n",
    "            if rows is None:\n",
    "                rows = feats.reshape((-1, 1))\n",
    "            else:\n",
    "                rows = np.concatenate([rows, feats.reshape((-1, 1))], axis=1)\n",
    "            del feats\n",
    "            gc.collect()\n",
    "        for item in tqdm(self.cate_cols):\n",
    "            feats = df[item].values\n",
    "            Max = df[item].max()\n",
    "            bit_len = len(bin(Max)) - 2\n",
    "            samples = self.samples\n",
    "            self.Max_len[item] = bit_len\n",
    "            res = unpackbits(feats, bit_len).reshape((samples, -1))\n",
    "            rows = np.concatenate([rows, res], axis=1)\n",
    "            del feats\n",
    "            gc.collect()\n",
    "        trn_y = np.array(df[self.label_name].values).reshape((-1, 1))\n",
    "        del df\n",
    "        gc.collect()\n",
    "        trn_x = np.array(rows)\n",
    "        return trn_x, trn_y\n",
    "\n",
    "    # for test dataset\n",
    "    def transform(self, df):\n",
    "        \"\"\"Input a testing / validation set (pandas.DataFrame) and return the converted 2 numpy.ndarray (x,y).\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): Input dataframe\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray, numpy.ndarray: New features and labels.\n",
    "        \"\"\"\n",
    "        df = df.astype(dtype=self.dtype_dict)\n",
    "        samples = df.shape[0]\n",
    "#         print(\"Filtering and fillna features\")\n",
    "#         for item in tqdm(self.cate_cols):\n",
    "#             value_counts = df[item].value_counts()\n",
    "#             rm_values = set(value_counts.index) - set(self.save_value_filter[item])\n",
    "#             df[item] = df[item].map(lambda x: \"<LESS>\" if x in rm_values else x)\n",
    "#             df[item] = df[item].fillna(\"<UNK>\")\n",
    "\n",
    "#         for item in tqdm(self.nume_cols):\n",
    "#             mean = self.save_num_embs[item][\"sum\"] / self.save_num_embs[item][\"cnt\"]\n",
    "#             df[item] = df[item].fillna(mean)\n",
    "\n",
    "        print(\"Ordinal encoding cate features\")\n",
    "        # ordinal_encoding\n",
    "        df = self.encoder.transform(df)\n",
    "\n",
    "        print(\"Target encoding cate features\")\n",
    "        # dynamic_targeting_encoding\n",
    "        for item in tqdm(self.cate_cols):\n",
    "            avgs = self.save_cate_avgs[item]\n",
    "            df[item + \"_t_mean\"] = df[item].map(\n",
    "                lambda x: avgs[x][0] / avgs[x][1] if x in avgs else 0\n",
    "            )\n",
    "            df[item + \"_t_count\"] = df[item].map(\n",
    "                lambda x: avgs[x][1] / self.samples if x in avgs else 0\n",
    "            )\n",
    "\n",
    "        print(\"Start manual binary encoding\")\n",
    "        rows = None\n",
    "        for item in tqdm(self.nume_cols + self.tgt_nume_cols):\n",
    "            feats = df[item].values\n",
    "            if rows is None:\n",
    "                rows = feats.reshape((-1, 1))\n",
    "            else:\n",
    "                rows = np.concatenate([rows, feats.reshape((-1, 1))], axis=1)\n",
    "            del feats\n",
    "            gc.collect()\n",
    "        for item in tqdm(self.cate_cols):\n",
    "            feats = df[item].values\n",
    "            bit_len = self.Max_len[item]\n",
    "            res = unpackbits(feats, bit_len).reshape((samples, -1))\n",
    "            rows = np.concatenate([rows, res], axis=1)\n",
    "            del feats\n",
    "            gc.collect()\n",
    "        vld_y = np.array(df[self.label_name].values).reshape((-1, 1))\n",
    "        del df\n",
    "        gc.collect()\n",
    "        vld_x = np.array(rows)\n",
    "        return vld_x, vld_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal encoding cate features\n",
      "Target encoding cate features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████████████████████████████████████▍             | 31/36 [01:36<00:15,  3.09s/it]/tmp/ipykernel_3889143/2399632350.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_count\"] = feat_encoding[\"count\"]\n",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████           | 32/36 [01:39<00:12,  3.11s/it]/tmp/ipykernel_3889143/2399632350.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_mean\"] = feat_encoding[\"mean\"]\n",
      "/tmp/ipykernel_3889143/2399632350.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_count\"] = feat_encoding[\"count\"]\n",
      " 92%|█████████████████████████████████████████████████████████████████████████████████████████▊        | 33/36 [01:43<00:09,  3.11s/it]/tmp/ipykernel_3889143/2399632350.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_mean\"] = feat_encoding[\"mean\"]\n",
      "/tmp/ipykernel_3889143/2399632350.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_count\"] = feat_encoding[\"count\"]\n",
      " 94%|████████████████████████████████████████████████████████████████████████████████████████████▌     | 34/36 [01:45<00:06,  3.04s/it]/tmp/ipykernel_3889143/2399632350.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_mean\"] = feat_encoding[\"mean\"]\n",
      "/tmp/ipykernel_3889143/2399632350.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_count\"] = feat_encoding[\"count\"]\n",
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████████▎  | 35/36 [01:49<00:03,  3.18s/it]/tmp/ipykernel_3889143/2399632350.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_mean\"] = feat_encoding[\"mean\"]\n",
      "/tmp/ipykernel_3889143/2399632350.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[item + \"_t_count\"] = feat_encoding[\"count\"]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [01:52<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start manual binary encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 74/74 [00:41<00:00,  1.79it/s]\n",
      "  0%|                                                                                                           | 0/36 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unpackbits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m num_encoder \u001b[38;5;241m=\u001b[39m NumEncoder(cate_cols, nume_cols, label_col)\n\u001b[0;32m----> 3\u001b[0m train_x, train_y \u001b[38;5;241m=\u001b[39m \u001b[43mnum_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m valid_x, valid_y \u001b[38;5;241m=\u001b[39m num_encoder\u001b[38;5;241m.\u001b[39mtransform(valid)\n\u001b[1;32m      5\u001b[0m test_x, test_y \u001b[38;5;241m=\u001b[39m num_encoder\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36mNumEncoder.fit_transform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    113\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMax_len[item] \u001b[38;5;241m=\u001b[39m bit_len\n\u001b[0;32m--> 115\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43munpackbits\u001b[49m(feats, bit_len)\u001b[38;5;241m.\u001b[39mreshape((samples, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    116\u001b[0m rows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([rows, res], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m feats\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unpackbits' is not defined"
     ]
    }
   ],
   "source": [
    "num_encoder = NumEncoder(cate_cols, nume_cols, label_col)\n",
    "\n",
    "train_x, train_y = num_encoder.fit_transform(train)\n",
    "valid_x, valid_y = num_encoder.transform(valid)\n",
    "test_x, test_y = num_encoder.transform(test)\n",
    "del num_encoder\n",
    "\n",
    "print('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n",
    "      .format(trn_x_shape=train_x.shape,\n",
    "              trn_y_shape=train_y.shape,\n",
    "              vld_x_shape=valid_x.shape,\n",
    "              vld_y_shape=valid_y.shape,\n",
    "              tst_x_shape=test_x.shape,\n",
    "              tst_y_shape=test_y.shape,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/shiv/anaconda3/envs/RecSys_39/lib/python3.9/site-packages/lightgbm/engine.py:260: UserWarning: 'evals_result' argument is deprecated and will be removed in a future release of LightGBM. Pass 'record_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'evals_result' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 618\n",
      "[LightGBM] [Info] Number of data points in the train set: 4519730, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 4.351199\n",
      "[1]\tvalid_0's rmse: 1.15986\tvalid_0's l1: 0.887851\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[2]\tvalid_0's rmse: 1.15886\tvalid_0's l1: 0.886246\n",
      "[3]\tvalid_0's rmse: 1.15814\tvalid_0's l1: 0.884963\n",
      "[4]\tvalid_0's rmse: 1.15755\tvalid_0's l1: 0.883858\n",
      "[5]\tvalid_0's rmse: 1.15719\tvalid_0's l1: 0.883011\n",
      "[6]\tvalid_0's rmse: 1.15682\tvalid_0's l1: 0.882098\n",
      "[7]\tvalid_0's rmse: 1.15652\tvalid_0's l1: 0.881315\n",
      "[8]\tvalid_0's rmse: 1.15636\tvalid_0's l1: 0.880764\n",
      "[9]\tvalid_0's rmse: 1.15622\tvalid_0's l1: 0.880328\n",
      "[10]\tvalid_0's rmse: 1.1561\tvalid_0's l1: 0.880035\n",
      "[11]\tvalid_0's rmse: 1.15597\tvalid_0's l1: 0.879673\n",
      "[12]\tvalid_0's rmse: 1.15588\tvalid_0's l1: 0.879377\n",
      "[13]\tvalid_0's rmse: 1.15582\tvalid_0's l1: 0.879108\n",
      "[14]\tvalid_0's rmse: 1.15574\tvalid_0's l1: 0.878856\n",
      "[15]\tvalid_0's rmse: 1.15564\tvalid_0's l1: 0.878531\n",
      "[16]\tvalid_0's rmse: 1.15555\tvalid_0's l1: 0.878255\n",
      "[17]\tvalid_0's rmse: 1.15544\tvalid_0's l1: 0.877939\n",
      "[18]\tvalid_0's rmse: 1.15535\tvalid_0's l1: 0.87774\n",
      "[19]\tvalid_0's rmse: 1.15527\tvalid_0's l1: 0.87747\n",
      "[20]\tvalid_0's rmse: 1.1552\tvalid_0's l1: 0.877321\n",
      "[21]\tvalid_0's rmse: 1.15515\tvalid_0's l1: 0.877258\n",
      "[22]\tvalid_0's rmse: 1.1551\tvalid_0's l1: 0.877132\n",
      "[23]\tvalid_0's rmse: 1.155\tvalid_0's l1: 0.876844\n",
      "[24]\tvalid_0's rmse: 1.15496\tvalid_0's l1: 0.876715\n",
      "[25]\tvalid_0's rmse: 1.15489\tvalid_0's l1: 0.876567\n",
      "[26]\tvalid_0's rmse: 1.15482\tvalid_0's l1: 0.876472\n",
      "[27]\tvalid_0's rmse: 1.15476\tvalid_0's l1: 0.876314\n",
      "[28]\tvalid_0's rmse: 1.15473\tvalid_0's l1: 0.87624\n",
      "[29]\tvalid_0's rmse: 1.15472\tvalid_0's l1: 0.876184\n",
      "[30]\tvalid_0's rmse: 1.15467\tvalid_0's l1: 0.876068\n",
      "[31]\tvalid_0's rmse: 1.1546\tvalid_0's l1: 0.875923\n",
      "[32]\tvalid_0's rmse: 1.15454\tvalid_0's l1: 0.875816\n",
      "[33]\tvalid_0's rmse: 1.1545\tvalid_0's l1: 0.875746\n",
      "[34]\tvalid_0's rmse: 1.15447\tvalid_0's l1: 0.8757\n",
      "[35]\tvalid_0's rmse: 1.15442\tvalid_0's l1: 0.875582\n",
      "[36]\tvalid_0's rmse: 1.15436\tvalid_0's l1: 0.875474\n",
      "[37]\tvalid_0's rmse: 1.15433\tvalid_0's l1: 0.875397\n",
      "[38]\tvalid_0's rmse: 1.15431\tvalid_0's l1: 0.875371\n",
      "[39]\tvalid_0's rmse: 1.15427\tvalid_0's l1: 0.87528\n",
      "[40]\tvalid_0's rmse: 1.15426\tvalid_0's l1: 0.875236\n",
      "[41]\tvalid_0's rmse: 1.15424\tvalid_0's l1: 0.875187\n",
      "[42]\tvalid_0's rmse: 1.15422\tvalid_0's l1: 0.875134\n",
      "[43]\tvalid_0's rmse: 1.15418\tvalid_0's l1: 0.875056\n",
      "[44]\tvalid_0's rmse: 1.15413\tvalid_0's l1: 0.874952\n",
      "[45]\tvalid_0's rmse: 1.15411\tvalid_0's l1: 0.874919\n",
      "[46]\tvalid_0's rmse: 1.15406\tvalid_0's l1: 0.874852\n",
      "[47]\tvalid_0's rmse: 1.15404\tvalid_0's l1: 0.874771\n",
      "[48]\tvalid_0's rmse: 1.15397\tvalid_0's l1: 0.874615\n",
      "[49]\tvalid_0's rmse: 1.15395\tvalid_0's l1: 0.874576\n",
      "[50]\tvalid_0's rmse: 1.15394\tvalid_0's l1: 0.874557\n",
      "[51]\tvalid_0's rmse: 1.15391\tvalid_0's l1: 0.874482\n",
      "[52]\tvalid_0's rmse: 1.15389\tvalid_0's l1: 0.874453\n",
      "[53]\tvalid_0's rmse: 1.15386\tvalid_0's l1: 0.874393\n",
      "[54]\tvalid_0's rmse: 1.15385\tvalid_0's l1: 0.874381\n",
      "[55]\tvalid_0's rmse: 1.15383\tvalid_0's l1: 0.874339\n",
      "[56]\tvalid_0's rmse: 1.15381\tvalid_0's l1: 0.87431\n",
      "[57]\tvalid_0's rmse: 1.15378\tvalid_0's l1: 0.874245\n",
      "[58]\tvalid_0's rmse: 1.15373\tvalid_0's l1: 0.874166\n",
      "[59]\tvalid_0's rmse: 1.15372\tvalid_0's l1: 0.874155\n",
      "[60]\tvalid_0's rmse: 1.15371\tvalid_0's l1: 0.874123\n",
      "[61]\tvalid_0's rmse: 1.15369\tvalid_0's l1: 0.874062\n",
      "[62]\tvalid_0's rmse: 1.15368\tvalid_0's l1: 0.874033\n",
      "[63]\tvalid_0's rmse: 1.15368\tvalid_0's l1: 0.87403\n",
      "[64]\tvalid_0's rmse: 1.15367\tvalid_0's l1: 0.873986\n",
      "[65]\tvalid_0's rmse: 1.15363\tvalid_0's l1: 0.873918\n",
      "[66]\tvalid_0's rmse: 1.15363\tvalid_0's l1: 0.873903\n",
      "[67]\tvalid_0's rmse: 1.15359\tvalid_0's l1: 0.873844\n",
      "[68]\tvalid_0's rmse: 1.15359\tvalid_0's l1: 0.873822\n",
      "[69]\tvalid_0's rmse: 1.15358\tvalid_0's l1: 0.873787\n",
      "[70]\tvalid_0's rmse: 1.15356\tvalid_0's l1: 0.873748\n",
      "[71]\tvalid_0's rmse: 1.15355\tvalid_0's l1: 0.873737\n",
      "[72]\tvalid_0's rmse: 1.15354\tvalid_0's l1: 0.873722\n",
      "[73]\tvalid_0's rmse: 1.15351\tvalid_0's l1: 0.873642\n",
      "[74]\tvalid_0's rmse: 1.15348\tvalid_0's l1: 0.873589\n",
      "[75]\tvalid_0's rmse: 1.15347\tvalid_0's l1: 0.873585\n",
      "[76]\tvalid_0's rmse: 1.15343\tvalid_0's l1: 0.873518\n",
      "[77]\tvalid_0's rmse: 1.15342\tvalid_0's l1: 0.873512\n",
      "[78]\tvalid_0's rmse: 1.15341\tvalid_0's l1: 0.873504\n",
      "[79]\tvalid_0's rmse: 1.15338\tvalid_0's l1: 0.873459\n",
      "[80]\tvalid_0's rmse: 1.15336\tvalid_0's l1: 0.873406\n",
      "[81]\tvalid_0's rmse: 1.15334\tvalid_0's l1: 0.873379\n",
      "[82]\tvalid_0's rmse: 1.15329\tvalid_0's l1: 0.873267\n",
      "[83]\tvalid_0's rmse: 1.15327\tvalid_0's l1: 0.873207\n",
      "[84]\tvalid_0's rmse: 1.15323\tvalid_0's l1: 0.873165\n",
      "[85]\tvalid_0's rmse: 1.15322\tvalid_0's l1: 0.873147\n",
      "[86]\tvalid_0's rmse: 1.15321\tvalid_0's l1: 0.873108\n",
      "[87]\tvalid_0's rmse: 1.1532\tvalid_0's l1: 0.873092\n",
      "[88]\tvalid_0's rmse: 1.15316\tvalid_0's l1: 0.873021\n",
      "[89]\tvalid_0's rmse: 1.15313\tvalid_0's l1: 0.872961\n",
      "[90]\tvalid_0's rmse: 1.15313\tvalid_0's l1: 0.872942\n",
      "[91]\tvalid_0's rmse: 1.15313\tvalid_0's l1: 0.872934\n",
      "[92]\tvalid_0's rmse: 1.1531\tvalid_0's l1: 0.872913\n",
      "[93]\tvalid_0's rmse: 1.15306\tvalid_0's l1: 0.872835\n",
      "[94]\tvalid_0's rmse: 1.15305\tvalid_0's l1: 0.8728\n",
      "[95]\tvalid_0's rmse: 1.15303\tvalid_0's l1: 0.872747\n",
      "[96]\tvalid_0's rmse: 1.15302\tvalid_0's l1: 0.872742\n",
      "[97]\tvalid_0's rmse: 1.15299\tvalid_0's l1: 0.872676\n",
      "[98]\tvalid_0's rmse: 1.15299\tvalid_0's l1: 0.872674\n",
      "[99]\tvalid_0's rmse: 1.15299\tvalid_0's l1: 0.872655\n",
      "[100]\tvalid_0's rmse: 1.15298\tvalid_0's l1: 0.872656\n",
      "[101]\tvalid_0's rmse: 1.15298\tvalid_0's l1: 0.872646\n",
      "[102]\tvalid_0's rmse: 1.15297\tvalid_0's l1: 0.872616\n",
      "[103]\tvalid_0's rmse: 1.15296\tvalid_0's l1: 0.872607\n",
      "[104]\tvalid_0's rmse: 1.15293\tvalid_0's l1: 0.87256\n",
      "[105]\tvalid_0's rmse: 1.15293\tvalid_0's l1: 0.872551\n",
      "[106]\tvalid_0's rmse: 1.15293\tvalid_0's l1: 0.87252\n",
      "[107]\tvalid_0's rmse: 1.15289\tvalid_0's l1: 0.87247\n",
      "[108]\tvalid_0's rmse: 1.15286\tvalid_0's l1: 0.872403\n",
      "[109]\tvalid_0's rmse: 1.15286\tvalid_0's l1: 0.872394\n",
      "[110]\tvalid_0's rmse: 1.15285\tvalid_0's l1: 0.872381\n",
      "[111]\tvalid_0's rmse: 1.15285\tvalid_0's l1: 0.872374\n",
      "[112]\tvalid_0's rmse: 1.15284\tvalid_0's l1: 0.872349\n",
      "[113]\tvalid_0's rmse: 1.15283\tvalid_0's l1: 0.872325\n",
      "[114]\tvalid_0's rmse: 1.15281\tvalid_0's l1: 0.872284\n",
      "[115]\tvalid_0's rmse: 1.15279\tvalid_0's l1: 0.872248\n",
      "[116]\tvalid_0's rmse: 1.15279\tvalid_0's l1: 0.872245\n",
      "[117]\tvalid_0's rmse: 1.15276\tvalid_0's l1: 0.872206\n",
      "[118]\tvalid_0's rmse: 1.15276\tvalid_0's l1: 0.872191\n",
      "[119]\tvalid_0's rmse: 1.15276\tvalid_0's l1: 0.872192\n",
      "[120]\tvalid_0's rmse: 1.15275\tvalid_0's l1: 0.872173\n",
      "[121]\tvalid_0's rmse: 1.15273\tvalid_0's l1: 0.872134\n",
      "[122]\tvalid_0's rmse: 1.1527\tvalid_0's l1: 0.872082\n",
      "[123]\tvalid_0's rmse: 1.1527\tvalid_0's l1: 0.872075\n",
      "[124]\tvalid_0's rmse: 1.15269\tvalid_0's l1: 0.872045\n",
      "[125]\tvalid_0's rmse: 1.15267\tvalid_0's l1: 0.872022\n",
      "[126]\tvalid_0's rmse: 1.15266\tvalid_0's l1: 0.872009\n",
      "[127]\tvalid_0's rmse: 1.15266\tvalid_0's l1: 0.871982\n",
      "[128]\tvalid_0's rmse: 1.15265\tvalid_0's l1: 0.871964\n",
      "[129]\tvalid_0's rmse: 1.15264\tvalid_0's l1: 0.871942\n",
      "[130]\tvalid_0's rmse: 1.15264\tvalid_0's l1: 0.871936\n",
      "[131]\tvalid_0's rmse: 1.15264\tvalid_0's l1: 0.871926\n",
      "[132]\tvalid_0's rmse: 1.15263\tvalid_0's l1: 0.871921\n",
      "[133]\tvalid_0's rmse: 1.15263\tvalid_0's l1: 0.87192\n",
      "[134]\tvalid_0's rmse: 1.15262\tvalid_0's l1: 0.871885\n",
      "[135]\tvalid_0's rmse: 1.15261\tvalid_0's l1: 0.871858\n",
      "[136]\tvalid_0's rmse: 1.15261\tvalid_0's l1: 0.871851\n",
      "[137]\tvalid_0's rmse: 1.15259\tvalid_0's l1: 0.871804\n",
      "[138]\tvalid_0's rmse: 1.15258\tvalid_0's l1: 0.87178\n",
      "[139]\tvalid_0's rmse: 1.15256\tvalid_0's l1: 0.871736\n",
      "[140]\tvalid_0's rmse: 1.15254\tvalid_0's l1: 0.871703\n",
      "[141]\tvalid_0's rmse: 1.15254\tvalid_0's l1: 0.8717\n",
      "[142]\tvalid_0's rmse: 1.15253\tvalid_0's l1: 0.871688\n",
      "[143]\tvalid_0's rmse: 1.15251\tvalid_0's l1: 0.871664\n",
      "[144]\tvalid_0's rmse: 1.15251\tvalid_0's l1: 0.871665\n",
      "[145]\tvalid_0's rmse: 1.15249\tvalid_0's l1: 0.871621\n",
      "[146]\tvalid_0's rmse: 1.15248\tvalid_0's l1: 0.871583\n",
      "[147]\tvalid_0's rmse: 1.15248\tvalid_0's l1: 0.871583\n",
      "[148]\tvalid_0's rmse: 1.15247\tvalid_0's l1: 0.871567\n",
      "[149]\tvalid_0's rmse: 1.15246\tvalid_0's l1: 0.871553\n",
      "[150]\tvalid_0's rmse: 1.15246\tvalid_0's l1: 0.871545\n",
      "[151]\tvalid_0's rmse: 1.15245\tvalid_0's l1: 0.871536\n",
      "[152]\tvalid_0's rmse: 1.15244\tvalid_0's l1: 0.871533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153]\tvalid_0's rmse: 1.15244\tvalid_0's l1: 0.87152\n",
      "[154]\tvalid_0's rmse: 1.15244\tvalid_0's l1: 0.871504\n",
      "[155]\tvalid_0's rmse: 1.15243\tvalid_0's l1: 0.8715\n",
      "[156]\tvalid_0's rmse: 1.15242\tvalid_0's l1: 0.871477\n",
      "[157]\tvalid_0's rmse: 1.15242\tvalid_0's l1: 0.871465\n",
      "[158]\tvalid_0's rmse: 1.15242\tvalid_0's l1: 0.871465\n",
      "[159]\tvalid_0's rmse: 1.15239\tvalid_0's l1: 0.871405\n",
      "[160]\tvalid_0's rmse: 1.15238\tvalid_0's l1: 0.871372\n",
      "[161]\tvalid_0's rmse: 1.15237\tvalid_0's l1: 0.871355\n",
      "[162]\tvalid_0's rmse: 1.15237\tvalid_0's l1: 0.871341\n",
      "[163]\tvalid_0's rmse: 1.15236\tvalid_0's l1: 0.871331\n",
      "[164]\tvalid_0's rmse: 1.15235\tvalid_0's l1: 0.871316\n",
      "[165]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871312\n",
      "[166]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871307\n",
      "[167]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871307\n",
      "[168]\tvalid_0's rmse: 1.15234\tvalid_0's l1: 0.871301\n",
      "[169]\tvalid_0's rmse: 1.15233\tvalid_0's l1: 0.871304\n",
      "[170]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871274\n",
      "[171]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871266\n",
      "[172]\tvalid_0's rmse: 1.15231\tvalid_0's l1: 0.871242\n",
      "[173]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871242\n",
      "[174]\tvalid_0's rmse: 1.15232\tvalid_0's l1: 0.871243\n",
      "[175]\tvalid_0's rmse: 1.1523\tvalid_0's l1: 0.871225\n",
      "[176]\tvalid_0's rmse: 1.15229\tvalid_0's l1: 0.871199\n",
      "[177]\tvalid_0's rmse: 1.15229\tvalid_0's l1: 0.871191\n",
      "[178]\tvalid_0's rmse: 1.15228\tvalid_0's l1: 0.871167\n",
      "[179]\tvalid_0's rmse: 1.15227\tvalid_0's l1: 0.871159\n",
      "[180]\tvalid_0's rmse: 1.15227\tvalid_0's l1: 0.871155\n",
      "[181]\tvalid_0's rmse: 1.15226\tvalid_0's l1: 0.871135\n",
      "[182]\tvalid_0's rmse: 1.15226\tvalid_0's l1: 0.871087\n",
      "[183]\tvalid_0's rmse: 1.15225\tvalid_0's l1: 0.871074\n",
      "[184]\tvalid_0's rmse: 1.15223\tvalid_0's l1: 0.871047\n",
      "[185]\tvalid_0's rmse: 1.15223\tvalid_0's l1: 0.87103\n",
      "[186]\tvalid_0's rmse: 1.15223\tvalid_0's l1: 0.871029\n",
      "[187]\tvalid_0's rmse: 1.15222\tvalid_0's l1: 0.871017\n",
      "[188]\tvalid_0's rmse: 1.15222\tvalid_0's l1: 0.871017\n",
      "[189]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.871004\n",
      "[190]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.870996\n",
      "[191]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.870999\n",
      "[192]\tvalid_0's rmse: 1.15221\tvalid_0's l1: 0.870995\n",
      "[193]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870947\n",
      "[194]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870948\n",
      "[195]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870941\n",
      "[196]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870939\n",
      "[197]\tvalid_0's rmse: 1.15219\tvalid_0's l1: 0.870934\n",
      "[198]\tvalid_0's rmse: 1.15218\tvalid_0's l1: 0.870916\n",
      "[199]\tvalid_0's rmse: 1.15218\tvalid_0's l1: 0.8709\n",
      "[200]\tvalid_0's rmse: 1.15216\tvalid_0's l1: 0.870867\n",
      "[201]\tvalid_0's rmse: 1.15215\tvalid_0's l1: 0.870824\n",
      "[202]\tvalid_0's rmse: 1.15215\tvalid_0's l1: 0.870811\n",
      "[203]\tvalid_0's rmse: 1.15214\tvalid_0's l1: 0.870795\n",
      "[204]\tvalid_0's rmse: 1.15213\tvalid_0's l1: 0.870773\n",
      "[205]\tvalid_0's rmse: 1.15213\tvalid_0's l1: 0.870765\n",
      "[206]\tvalid_0's rmse: 1.15212\tvalid_0's l1: 0.870752\n",
      "[207]\tvalid_0's rmse: 1.1521\tvalid_0's l1: 0.870725\n",
      "[208]\tvalid_0's rmse: 1.1521\tvalid_0's l1: 0.870713\n",
      "[209]\tvalid_0's rmse: 1.15209\tvalid_0's l1: 0.870695\n",
      "[210]\tvalid_0's rmse: 1.15209\tvalid_0's l1: 0.870687\n",
      "[211]\tvalid_0's rmse: 1.15208\tvalid_0's l1: 0.870676\n",
      "[212]\tvalid_0's rmse: 1.15208\tvalid_0's l1: 0.870673\n",
      "[213]\tvalid_0's rmse: 1.15208\tvalid_0's l1: 0.870675\n",
      "[214]\tvalid_0's rmse: 1.15206\tvalid_0's l1: 0.87062\n",
      "[215]\tvalid_0's rmse: 1.15206\tvalid_0's l1: 0.87061\n",
      "[216]\tvalid_0's rmse: 1.15204\tvalid_0's l1: 0.870574\n",
      "[217]\tvalid_0's rmse: 1.15203\tvalid_0's l1: 0.870552\n",
      "[218]\tvalid_0's rmse: 1.15201\tvalid_0's l1: 0.870513\n",
      "[219]\tvalid_0's rmse: 1.15199\tvalid_0's l1: 0.870489\n",
      "[220]\tvalid_0's rmse: 1.15199\tvalid_0's l1: 0.870487\n",
      "[221]\tvalid_0's rmse: 1.15198\tvalid_0's l1: 0.870465\n",
      "[222]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870452\n",
      "[223]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870434\n",
      "[224]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870414\n",
      "[225]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870414\n",
      "[226]\tvalid_0's rmse: 1.15197\tvalid_0's l1: 0.870412\n",
      "[227]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.8704\n",
      "[228]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.870392\n",
      "[229]\tvalid_0's rmse: 1.15195\tvalid_0's l1: 0.870383\n",
      "[230]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.87038\n",
      "[231]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.870373\n",
      "[232]\tvalid_0's rmse: 1.15196\tvalid_0's l1: 0.870368\n",
      "[233]\tvalid_0's rmse: 1.15195\tvalid_0's l1: 0.870341\n",
      "[234]\tvalid_0's rmse: 1.15194\tvalid_0's l1: 0.870328\n",
      "[235]\tvalid_0's rmse: 1.15194\tvalid_0's l1: 0.870324\n",
      "[236]\tvalid_0's rmse: 1.15194\tvalid_0's l1: 0.870321\n",
      "[237]\tvalid_0's rmse: 1.15193\tvalid_0's l1: 0.870318\n",
      "[238]\tvalid_0's rmse: 1.15192\tvalid_0's l1: 0.870304\n",
      "[239]\tvalid_0's rmse: 1.15193\tvalid_0's l1: 0.870303\n",
      "[240]\tvalid_0's rmse: 1.15189\tvalid_0's l1: 0.870242\n",
      "[241]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870234\n",
      "[242]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870222\n",
      "[243]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.87022\n",
      "[244]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870218\n",
      "[245]\tvalid_0's rmse: 1.15188\tvalid_0's l1: 0.870218\n",
      "[246]\tvalid_0's rmse: 1.15187\tvalid_0's l1: 0.870211\n",
      "[247]\tvalid_0's rmse: 1.15187\tvalid_0's l1: 0.870194\n",
      "[248]\tvalid_0's rmse: 1.15186\tvalid_0's l1: 0.870192\n",
      "[249]\tvalid_0's rmse: 1.15187\tvalid_0's l1: 0.870196\n",
      "[250]\tvalid_0's rmse: 1.15186\tvalid_0's l1: 0.870187\n",
      "[251]\tvalid_0's rmse: 1.15186\tvalid_0's l1: 0.870188\n",
      "[252]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.870144\n",
      "[253]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.870141\n",
      "[254]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.870136\n",
      "[255]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.870128\n",
      "[256]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.87013\n",
      "[257]\tvalid_0's rmse: 1.15184\tvalid_0's l1: 0.87012\n",
      "[258]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.870102\n",
      "[259]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.8701\n",
      "[260]\tvalid_0's rmse: 1.15183\tvalid_0's l1: 0.870092\n",
      "[261]\tvalid_0's rmse: 1.15182\tvalid_0's l1: 0.87007\n",
      "[262]\tvalid_0's rmse: 1.15181\tvalid_0's l1: 0.87005\n",
      "[263]\tvalid_0's rmse: 1.15181\tvalid_0's l1: 0.870035\n",
      "[264]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870032\n",
      "[265]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870026\n",
      "[266]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870026\n",
      "[267]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.870027\n",
      "[268]\tvalid_0's rmse: 1.1518\tvalid_0's l1: 0.87002\n",
      "[269]\tvalid_0's rmse: 1.15179\tvalid_0's l1: 0.869991\n",
      "[270]\tvalid_0's rmse: 1.15179\tvalid_0's l1: 0.86999\n",
      "[271]\tvalid_0's rmse: 1.15179\tvalid_0's l1: 0.869984\n",
      "[272]\tvalid_0's rmse: 1.15178\tvalid_0's l1: 0.869972\n",
      "[273]\tvalid_0's rmse: 1.15178\tvalid_0's l1: 0.869965\n",
      "[274]\tvalid_0's rmse: 1.15177\tvalid_0's l1: 0.869925\n",
      "[275]\tvalid_0's rmse: 1.15176\tvalid_0's l1: 0.869915\n",
      "[276]\tvalid_0's rmse: 1.15175\tvalid_0's l1: 0.869908\n",
      "[277]\tvalid_0's rmse: 1.15174\tvalid_0's l1: 0.869891\n",
      "[278]\tvalid_0's rmse: 1.15174\tvalid_0's l1: 0.869884\n",
      "[279]\tvalid_0's rmse: 1.15173\tvalid_0's l1: 0.869882\n",
      "[280]\tvalid_0's rmse: 1.15173\tvalid_0's l1: 0.869879\n",
      "[281]\tvalid_0's rmse: 1.15173\tvalid_0's l1: 0.869875\n",
      "[282]\tvalid_0's rmse: 1.15172\tvalid_0's l1: 0.86986\n",
      "[283]\tvalid_0's rmse: 1.15172\tvalid_0's l1: 0.869851\n",
      "[284]\tvalid_0's rmse: 1.15171\tvalid_0's l1: 0.869845\n",
      "[285]\tvalid_0's rmse: 1.15171\tvalid_0's l1: 0.869841\n",
      "[286]\tvalid_0's rmse: 1.15171\tvalid_0's l1: 0.869829\n",
      "[287]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869807\n",
      "[288]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869788\n",
      "[289]\tvalid_0's rmse: 1.15169\tvalid_0's l1: 0.869783\n",
      "[290]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869785\n",
      "[291]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869787\n",
      "[292]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869781\n",
      "[293]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869777\n",
      "[294]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869774\n",
      "[295]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869774\n",
      "[296]\tvalid_0's rmse: 1.1517\tvalid_0's l1: 0.869764\n",
      "[297]\tvalid_0's rmse: 1.15169\tvalid_0's l1: 0.869754\n",
      "[298]\tvalid_0's rmse: 1.15169\tvalid_0's l1: 0.869749\n",
      "[299]\tvalid_0's rmse: 1.15168\tvalid_0's l1: 0.869737\n",
      "[300]\tvalid_0's rmse: 1.15168\tvalid_0's l1: 0.86973\n",
      "[301]\tvalid_0's rmse: 1.15167\tvalid_0's l1: 0.869719\n",
      "[302]\tvalid_0's rmse: 1.15167\tvalid_0's l1: 0.869723\n",
      "[303]\tvalid_0's rmse: 1.15167\tvalid_0's l1: 0.869716\n",
      "[304]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869688\n",
      "[305]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869687\n",
      "[306]\tvalid_0's rmse: 1.15166\tvalid_0's l1: 0.86969\n",
      "[307]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869683\n",
      "[308]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869675\n",
      "[309]\tvalid_0's rmse: 1.15166\tvalid_0's l1: 0.869674\n",
      "[310]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.86967\n",
      "[311]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869669\n",
      "[312]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.86966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[313]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869644\n",
      "[314]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869642\n",
      "[315]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869643\n",
      "[316]\tvalid_0's rmse: 1.15165\tvalid_0's l1: 0.869636\n",
      "[317]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869629\n",
      "[318]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869629\n",
      "[319]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869627\n",
      "[320]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869625\n",
      "[321]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869626\n",
      "[322]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869618\n",
      "[323]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869617\n",
      "[324]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869614\n",
      "[325]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869598\n",
      "[326]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869587\n",
      "[327]\tvalid_0's rmse: 1.15164\tvalid_0's l1: 0.869574\n",
      "[328]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869554\n",
      "[329]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869545\n",
      "[330]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869543\n",
      "[331]\tvalid_0's rmse: 1.15163\tvalid_0's l1: 0.869536\n",
      "[332]\tvalid_0's rmse: 1.15162\tvalid_0's l1: 0.86951\n",
      "[333]\tvalid_0's rmse: 1.15162\tvalid_0's l1: 0.869506\n",
      "[334]\tvalid_0's rmse: 1.15162\tvalid_0's l1: 0.869505\n",
      "[335]\tvalid_0's rmse: 1.15159\tvalid_0's l1: 0.869469\n",
      "[336]\tvalid_0's rmse: 1.15159\tvalid_0's l1: 0.869469\n",
      "[337]\tvalid_0's rmse: 1.15158\tvalid_0's l1: 0.869451\n",
      "[338]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869417\n",
      "[339]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869415\n",
      "[340]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869415\n",
      "[341]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869414\n",
      "[342]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869414\n",
      "[343]\tvalid_0's rmse: 1.15157\tvalid_0's l1: 0.869402\n",
      "[344]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869391\n",
      "[345]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869387\n",
      "[346]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869377\n",
      "[347]\tvalid_0's rmse: 1.15156\tvalid_0's l1: 0.869369\n",
      "[348]\tvalid_0's rmse: 1.15155\tvalid_0's l1: 0.869366\n",
      "[349]\tvalid_0's rmse: 1.15155\tvalid_0's l1: 0.869365\n",
      "[350]\tvalid_0's rmse: 1.15155\tvalid_0's l1: 0.869358\n",
      "[351]\tvalid_0's rmse: 1.15154\tvalid_0's l1: 0.869344\n",
      "[352]\tvalid_0's rmse: 1.15154\tvalid_0's l1: 0.869344\n",
      "[353]\tvalid_0's rmse: 1.15153\tvalid_0's l1: 0.869331\n",
      "[354]\tvalid_0's rmse: 1.15153\tvalid_0's l1: 0.869328\n",
      "[355]\tvalid_0's rmse: 1.15153\tvalid_0's l1: 0.869316\n",
      "[356]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869309\n",
      "[357]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869308\n",
      "[358]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.86931\n",
      "[359]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869309\n",
      "[360]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869311\n",
      "[361]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869313\n",
      "[362]\tvalid_0's rmse: 1.15152\tvalid_0's l1: 0.869301\n",
      "[363]\tvalid_0's rmse: 1.15151\tvalid_0's l1: 0.869297\n",
      "[364]\tvalid_0's rmse: 1.15151\tvalid_0's l1: 0.869293\n",
      "[365]\tvalid_0's rmse: 1.15151\tvalid_0's l1: 0.86929\n",
      "[366]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869267\n",
      "[367]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869268\n",
      "[368]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869258\n",
      "[369]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869262\n",
      "[370]\tvalid_0's rmse: 1.1515\tvalid_0's l1: 0.869259\n",
      "[371]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869252\n",
      "[372]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869249\n",
      "[373]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869248\n",
      "[374]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869245\n",
      "[375]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869242\n",
      "[376]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.86924\n",
      "[377]\tvalid_0's rmse: 1.15149\tvalid_0's l1: 0.869232\n",
      "[378]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869202\n",
      "[379]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869199\n",
      "[380]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869195\n",
      "[381]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869192\n",
      "[382]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869184\n",
      "[383]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869176\n",
      "[384]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869177\n",
      "[385]\tvalid_0's rmse: 1.15147\tvalid_0's l1: 0.869176\n",
      "[386]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869172\n",
      "[387]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869167\n",
      "[388]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869165\n",
      "[389]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.869163\n",
      "[390]\tvalid_0's rmse: 1.15146\tvalid_0's l1: 0.86916\n",
      "[391]\tvalid_0's rmse: 1.15145\tvalid_0's l1: 0.86915\n",
      "[392]\tvalid_0's rmse: 1.15145\tvalid_0's l1: 0.869132\n",
      "[393]\tvalid_0's rmse: 1.15145\tvalid_0's l1: 0.869128\n",
      "[394]\tvalid_0's rmse: 1.15144\tvalid_0's l1: 0.869093\n",
      "[395]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869073\n",
      "[396]\tvalid_0's rmse: 1.15143\tvalid_0's l1: 0.869075\n",
      "[397]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869072\n",
      "[398]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869071\n",
      "[399]\tvalid_0's rmse: 1.15142\tvalid_0's l1: 0.869059\n",
      "[400]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869046\n",
      "[401]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869046\n",
      "[402]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869036\n",
      "[403]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869037\n",
      "[404]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869035\n",
      "[405]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869034\n",
      "[406]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869034\n",
      "[407]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869034\n",
      "[408]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.86903\n",
      "[409]\tvalid_0's rmse: 1.15141\tvalid_0's l1: 0.869032\n",
      "[410]\tvalid_0's rmse: 1.1514\tvalid_0's l1: 0.869019\n",
      "[411]\tvalid_0's rmse: 1.1514\tvalid_0's l1: 0.869014\n",
      "[412]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.868995\n",
      "[413]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.86898\n",
      "[414]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.868976\n",
      "[415]\tvalid_0's rmse: 1.15139\tvalid_0's l1: 0.868976\n",
      "[416]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868957\n",
      "[417]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868953\n",
      "[418]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868949\n",
      "[419]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868949\n",
      "[420]\tvalid_0's rmse: 1.15138\tvalid_0's l1: 0.868944\n",
      "[421]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868916\n",
      "[422]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868913\n",
      "[423]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868911\n",
      "[424]\tvalid_0's rmse: 1.15137\tvalid_0's l1: 0.868893\n",
      "[425]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868876\n",
      "[426]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868875\n",
      "[427]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868874\n",
      "[428]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868872\n",
      "[429]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868875\n",
      "[430]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868866\n",
      "[431]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868864\n",
      "[432]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868859\n",
      "[433]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868859\n",
      "[434]\tvalid_0's rmse: 1.15136\tvalid_0's l1: 0.868847\n",
      "[435]\tvalid_0's rmse: 1.15134\tvalid_0's l1: 0.868821\n",
      "[436]\tvalid_0's rmse: 1.15132\tvalid_0's l1: 0.868794\n",
      "[437]\tvalid_0's rmse: 1.15131\tvalid_0's l1: 0.868777\n",
      "[438]\tvalid_0's rmse: 1.1513\tvalid_0's l1: 0.868752\n",
      "[439]\tvalid_0's rmse: 1.1513\tvalid_0's l1: 0.868751\n",
      "[440]\tvalid_0's rmse: 1.15131\tvalid_0's l1: 0.868754\n",
      "[441]\tvalid_0's rmse: 1.15131\tvalid_0's l1: 0.868752\n",
      "[442]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868728\n",
      "[443]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868726\n",
      "[444]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868727\n",
      "[445]\tvalid_0's rmse: 1.15128\tvalid_0's l1: 0.868712\n",
      "[446]\tvalid_0's rmse: 1.15129\tvalid_0's l1: 0.868707\n",
      "[447]\tvalid_0's rmse: 1.15128\tvalid_0's l1: 0.868704\n",
      "[448]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868686\n",
      "[449]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868677\n",
      "[450]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868669\n",
      "[451]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868661\n",
      "[452]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868659\n",
      "[453]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868657\n",
      "[454]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868654\n",
      "[455]\tvalid_0's rmse: 1.15127\tvalid_0's l1: 0.868656\n",
      "[456]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868641\n",
      "[457]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.86864\n",
      "[458]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868641\n",
      "[459]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868638\n",
      "[460]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868638\n",
      "[461]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868639\n",
      "[462]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868637\n",
      "[463]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868632\n",
      "[464]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868624\n",
      "[465]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868622\n",
      "[466]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868619\n",
      "[467]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868621\n",
      "[468]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.86862\n",
      "[469]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[470]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868609\n",
      "[471]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868607\n",
      "[472]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868608\n",
      "[473]\tvalid_0's rmse: 1.15126\tvalid_0's l1: 0.868606\n",
      "[474]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868592\n",
      "[475]\tvalid_0's rmse: 1.15125\tvalid_0's l1: 0.868586\n",
      "[476]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868582\n",
      "[477]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868574\n",
      "[478]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868574\n",
      "[479]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868573\n",
      "[480]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868573\n",
      "[481]\tvalid_0's rmse: 1.15124\tvalid_0's l1: 0.868574\n",
      "[482]\tvalid_0's rmse: 1.15123\tvalid_0's l1: 0.868562\n",
      "[483]\tvalid_0's rmse: 1.15123\tvalid_0's l1: 0.868551\n",
      "[484]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868543\n",
      "[485]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868542\n",
      "[486]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868535\n",
      "[487]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868535\n",
      "[488]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868535\n",
      "[489]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868517\n",
      "[490]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868517\n",
      "[491]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868492\n",
      "[492]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868491\n",
      "[493]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868492\n",
      "[494]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868491\n",
      "[495]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868481\n",
      "[496]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868474\n",
      "[497]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868474\n",
      "[498]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868476\n",
      "[499]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868467\n",
      "[500]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868469\n",
      "[501]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868468\n",
      "[502]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868468\n",
      "[503]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868467\n",
      "[504]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868464\n",
      "[505]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868464\n",
      "[506]\tvalid_0's rmse: 1.15122\tvalid_0's l1: 0.868469\n",
      "[507]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868446\n",
      "[508]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.868447\n",
      "[509]\tvalid_0's rmse: 1.15121\tvalid_0's l1: 0.86844\n",
      "[510]\tvalid_0's rmse: 1.1512\tvalid_0's l1: 0.868416\n",
      "[511]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868411\n",
      "[512]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868413\n",
      "[513]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868409\n",
      "[514]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868408\n",
      "[515]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868403\n",
      "[516]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868397\n",
      "[517]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868394\n",
      "[518]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868387\n",
      "[519]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.868375\n",
      "[520]\tvalid_0's rmse: 1.15119\tvalid_0's l1: 0.86837\n",
      "[521]\tvalid_0's rmse: 1.15118\tvalid_0's l1: 0.868371\n",
      "[522]\tvalid_0's rmse: 1.15118\tvalid_0's l1: 0.868353\n",
      "[523]\tvalid_0's rmse: 1.15118\tvalid_0's l1: 0.868353\n",
      "[524]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868351\n",
      "[525]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868344\n",
      "[526]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868342\n",
      "[527]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.86834\n",
      "[528]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868337\n",
      "[529]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868336\n",
      "[530]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868332\n",
      "[531]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868331\n",
      "[532]\tvalid_0's rmse: 1.15117\tvalid_0's l1: 0.868326\n",
      "[533]\tvalid_0's rmse: 1.15116\tvalid_0's l1: 0.868324\n",
      "[534]\tvalid_0's rmse: 1.15116\tvalid_0's l1: 0.868319\n",
      "[535]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868303\n",
      "[536]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868303\n",
      "[537]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868297\n",
      "[538]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868298\n",
      "[539]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868299\n",
      "[540]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868298\n",
      "[541]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.86829\n",
      "[542]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868286\n",
      "[543]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868285\n",
      "[544]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868276\n",
      "[545]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868274\n",
      "[546]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868268\n",
      "[547]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868264\n",
      "[548]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868265\n",
      "[549]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868259\n",
      "[550]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868256\n",
      "[551]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868255\n",
      "[552]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868254\n",
      "[553]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868253\n",
      "[554]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868253\n",
      "[555]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868252\n",
      "[556]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868249\n",
      "[557]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868249\n",
      "[558]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868246\n",
      "[559]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868244\n",
      "[560]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868243\n",
      "[561]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868246\n",
      "[562]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868248\n",
      "[563]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868242\n",
      "[564]\tvalid_0's rmse: 1.15115\tvalid_0's l1: 0.868238\n",
      "[565]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868234\n",
      "[566]\tvalid_0's rmse: 1.15114\tvalid_0's l1: 0.868233\n",
      "[567]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868204\n",
      "[568]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868204\n",
      "[569]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868203\n",
      "[570]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868202\n",
      "[571]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868185\n",
      "[572]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868183\n",
      "[573]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.86818\n",
      "[574]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868173\n",
      "[575]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868166\n",
      "[576]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868164\n",
      "[577]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868163\n",
      "[578]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868162\n",
      "[579]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.86816\n",
      "[580]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868159\n",
      "[581]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868159\n",
      "[582]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.86816\n",
      "[583]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868156\n",
      "[584]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868154\n",
      "[585]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868156\n",
      "[586]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868151\n",
      "[587]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868149\n",
      "[588]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868143\n",
      "[589]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868145\n",
      "[590]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868144\n",
      "[591]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868144\n",
      "[592]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868143\n",
      "[593]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868141\n",
      "[594]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868138\n",
      "[595]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868136\n",
      "[596]\tvalid_0's rmse: 1.15113\tvalid_0's l1: 0.868136\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's rmse: 1.15112\tvalid_0's l1: 0.868164\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params)\n",
    "lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train)\n",
    "eval_result = {}\n",
    "lgb_model = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=NUM_OF_TREES*5,\n",
    "                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                      evals_result= eval_result,\n",
    "                      valid_sets=lgb_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.453011282813894, 'mae': 0.9014153288417129}\n"
     ]
    }
   ],
   "source": [
    "test_preds = lgb_model.predict(test_x)\n",
    "rmse = mean_squared_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "mae = mean_absolute_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "res_basic = {\"rmse\": rmse, \"mae\": mae}\n",
    "print(res_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving and loading\n",
    "Now we finish the basic training and testing for LightGBM, next let's try to save and reload the model, and then evaluate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rmse': 1.453011282813894, 'mae': 0.9014153288417129}\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as tmp:\n",
    "    save_file = os.path.join(tmp, r'finished.model')\n",
    "    lgb_model.save_model(save_file)\n",
    "    loaded_model = lgb.Booster(model_file=save_file)\n",
    "\n",
    "# eval the performance again\n",
    "test_preds = lgb_model.predict(test_x)\n",
    "\n",
    "rmse = mean_squared_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "mae = mean_absolute_error(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\n",
    "res_basic = {\"rmse\": rmse, \"mae\": mae}\n",
    "print(res_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "\\[1\\] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems. 3146–3154.<br>\n",
    "\\[2\\] The parameters of LightGBM: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst <br>\n",
    "\\[3\\] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363 (2018).<br>\n",
    "\\[4\\] Scikit-learn. 2018. categorical_encoding. https://github.com/scikit-learn-contrib/categorical-encoding<br>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
