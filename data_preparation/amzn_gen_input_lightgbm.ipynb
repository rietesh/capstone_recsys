{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c7c97f",
   "metadata": {},
   "source": [
    "# Prepare for LightGBM\n",
    "\n",
    "Prepare Electronics dataset for lightGBM model.\n",
    "\n",
    "#### Input\n",
    "- wide_deep/Electronics/wide_deep_amzn_e_20.csv, xdeepfm/lst_genres.pkl\n",
    "\n",
    "#### Output (includes positive & negative samples for NDCG, Hit Rate calculation)\n",
    "- train_e.csv, valid_e.csv, test_e.csv\n",
    "- amzn_e_tst_w_neg[0-5].txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5e7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../recommenders') # if needed, adjust the path to Microsoft Recommenders clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69466ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/shiv/Documents/DataScience/Capstone/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57a6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from recommenders.datasets.python_splitters import python_chrono_split\n",
    "from recommenders.utils.constants import (\n",
    "    DEFAULT_USER_COL as USER_COL,\n",
    "    DEFAULT_ITEM_COL as ITEM_COL,\n",
    "    DEFAULT_RATING_COL as RATING_COL,\n",
    "    DEFAULT_GENRE_COL as ITEM_FEAT_COL,\n",
    "    DEFAULT_PREDICTION_COL as PREDICT_COL,\n",
    "    DEFAULT_K,\n",
    "    DEFAULT_THRESHOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309c69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(DATA_DIR + 'wide_deep/Electronics/wide_deep_amzn_e_20.csv', \n",
    "                       header=None, low_memory=False)\n",
    "all_data.columns = [USER_COL,ITEM_COL,RATING_COL,ITEM_FEAT_COL,\n",
    "                    'unixTimeStamp','title','price','main_cat','category']\n",
    "all_data.sort_values('unixTimeStamp', inplace=True)\n",
    "all_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ceeb8",
   "metadata": {},
   "source": [
    "### Split data into train, validation, and test\n",
    "\n",
    "<b>Need to be done once.</b> And it takes a long time!\n",
    "\n",
    "First, we cut three sets (train_data (first 80%), valid_data (middle 10%) and test_data (last 10%)), cut from the original all data. <br>\n",
    "Notably, considering the reviews data is a kind of time-series streaming data, which is also very common in recommendation scenario, we split the data by its order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f5f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = python_chrono_split(all_data, [0.8,0.1,0.1], col_timestamp='unixTimeStamp')\n",
    "\n",
    "train = train[[RATING_COL,USER_COL,ITEM_COL,ITEM_FEAT_COL]].copy()\n",
    "valid = valid[[RATING_COL,USER_COL,ITEM_COL,ITEM_FEAT_COL]].copy()\n",
    "test  = test[[RATING_COL,USER_COL,ITEM_COL,ITEM_FEAT_COL]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9844afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of numerical and categorical columns\n",
    "with open(DATA_DIR + 'xdeepfm/lst_genres.pkl', 'rb') as f:\n",
    "    lst_genres = pickle.load(f)\n",
    "\n",
    "cols = [USER_COL, ITEM_COL] # numerical column\n",
    "for genre in lst_genres:\n",
    "    cols.append(genre) # categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d81e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cannot use column names with spaces in lightGBM\n",
    "cols_dict = {}\n",
    "cate_cols = []\n",
    "cols_dict[USER_COL] = USER_COL\n",
    "cols_dict[ITEM_COL] = ITEM_COL\n",
    "rev_cols_dict = {}\n",
    "for idx, col in enumerate(cols[2:], start=1):\n",
    "    cols_dict[col] = 'C' + str(idx)\n",
    "    rev_cols_dict['C'+str(idx)] = col\n",
    "    cate_cols.append('C' + str(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c423ca41",
   "metadata": {},
   "source": [
    "### Expand genre into categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a463f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols[2:]:\n",
    "    train[cols_dict[col]] = np.nan\n",
    "    valid[cols_dict[col]] = np.nan\n",
    "    test[cols_dict[col]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabf4efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 4519730/4519730 [02:37<00:00, 28719.04it/s]\n",
      "100%|████████████████████████████████| 562550/562550 [00:17<00:00, 31785.65it/s]\n",
      "100%|████████████████████████████████| 530903/530903 [00:19<00:00, 27531.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5613183, 9), (4519730, 39), (562550, 39), (530903, 39))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def expand_genres(row):\n",
    "    genres = row[ITEM_FEAT_COL].split('|')\n",
    "    for genre in genres:\n",
    "        row[cols_dict[genre]] = 1\n",
    "    return row\n",
    "\n",
    "train = train.progress_apply(expand_genres, axis=1)\n",
    "valid = valid.progress_apply(expand_genres, axis=1)\n",
    "test  = test.progress_apply(expand_genres, axis=1)\n",
    "\n",
    "train.drop(columns=[ITEM_FEAT_COL], inplace=True)\n",
    "valid.drop(columns=[ITEM_FEAT_COL], inplace=True)\n",
    "test.drop(columns=[ITEM_FEAT_COL], inplace=True)\n",
    "\n",
    "# 2 numeric features + 36 categorical features + label\n",
    "all_data.shape, train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08dc497",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(DATA_DIR + 'lightgbm/train_e.csv', header=False, index=False)\n",
    "valid.to_csv(DATA_DIR + 'lightgbm/valid_e.csv', header=False, index=False)\n",
    "test.to_csv(DATA_DIR + 'lightgbm/test_e.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d53879",
   "metadata": {},
   "source": [
    "### Prepare to get ndcg@10, hit@10 for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47326f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grp = train[[USER_COL,ITEM_COL]].groupby(USER_COL).agg(list)\n",
    "\n",
    "items_df = train.drop_duplicates([ITEM_COL]).copy()\n",
    "items_df.drop(columns=[USER_COL, RATING_COL], inplace=True)\n",
    "items_df.set_index(ITEM_COL, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ee01d",
   "metadata": {},
   "source": [
    "### Process for generating the test data positive and negative samples\n",
    "\n",
    "Note that the entire process takes time even when using multiprocessing module.\n",
    "\n",
    "- Each worker takes a part of the test dataframe, so if there are 5 workers, each get 1/5th of the dataframe; worker # 5 gets to work a bit more to handle the remaining rows in the end\n",
    "- Once a review is selected, that becomes the positive sample. NUM_NEG_SAMPLES are then found for this user.\n",
    "- Each of the negative sample is unique and not seen by the user that wrote the review. Both positive and negative samples are written using the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = []\n",
    "items_set = set(train[ITEM_COL].unique())\n",
    "\n",
    "def sample_function(train_grp, test, items_df, cate_cols, i, num_workers, seed, data_dir):\n",
    "    nrows = test.shape[0]\n",
    "    each = nrows // num_workers\n",
    "    start = i*each\n",
    "    end = nrows if i == num_workers - 1 else (i+1)*each\n",
    "    sym = ['!','@','#','$','%']\n",
    "\n",
    "    # print(i, start, end, test.iloc[start:end].shape)\n",
    "    random.seed(seed)\n",
    "    tst_w_neg_samples_path = data_dir + f'lightgbm/amzn_e_tst_w_neg{i}.txt'\n",
    "    \n",
    "    user_col = []\n",
    "    item_col = []\n",
    "    rating_col = []\n",
    "    feat_cols = []\n",
    "    for c in cate_cols:\n",
    "        feat_cols.append([])\n",
    "    \n",
    "    for j, row in test.iloc[start:end].iterrows():\n",
    "        u = row[USER_COL]\n",
    "        positive_item = row[ITEM_COL]\n",
    "        tmp_df = train_grp.loc[u]\n",
    "        assert(tmp_df.shape[0] != 0)\n",
    "        \n",
    "        items_seen_set = set(tmp_df[ITEM_COL])\n",
    "        items_not_seen = list(items_set - items_seen_set)\n",
    "        user_col.append(int(u))\n",
    "        item_col.append(int(positive_item))\n",
    "        rating_col.append(row[RATING_COL])\n",
    "        for k, c in enumerate(cate_cols):\n",
    "            feat_cols[k].append(float(row[c]))\n",
    "\n",
    "        cnt = 0\n",
    "        neg_items = set()\n",
    "        while cnt < NUM_NEG_SAMPLES:\n",
    "            neg_item = random.choice(items_not_seen)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "\n",
    "            cnt += 1\n",
    "            tmp_df = items_df.loc[neg_item]\n",
    "            assert(tmp_df.shape[0] != 0)\n",
    "            \n",
    "            user_col.append(int(u))\n",
    "            item_col.append(int(neg_item))\n",
    "            rating_col.append(5.0) # unused\n",
    "            for k, c in enumerate(cate_cols):\n",
    "                feat_cols[k].append(float(tmp_df[c]))\n",
    "        if j % 10_000 == 0:\n",
    "            print(sym[i], end='')\n",
    "            \n",
    "    test_dict = {USER_COL: user_col, ITEM_COL: item_col, RATING_COL: rating_col}\n",
    "    for k, c in enumerate(cate_cols):\n",
    "        test_dict[c] = feat_cols[k]\n",
    "    X_test = pd.DataFrame(test_dict)\n",
    "    X_test.to_csv(tst_w_neg_samples_path, header=False, index=False)\n",
    "\n",
    "for i in range(N_WORKERS):\n",
    "    processors.append(\n",
    "        Process(\n",
    "            target = sample_function,\n",
    "            args = (train_grp, test, items_df, cate_cols, i, N_WORKERS, RANDOM_SEED, DATA_DIR)\n",
    "        ))\n",
    "    # processors[-1].daemon = True\n",
    "    processors[-1].start()\n",
    "\n",
    "for i in range(N_WORKERS):\n",
    "    processors[i].join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
